



































THE MIT PRESS ESSENTIAL KNOWLEDGE SERIES



AI ETHICS





















10100101110101010111001000001000101010100100101001010010100101001010010
1010010010100101001010010100101001011101010101110010000010001010101001
00101001010010100101001010010101001001010010100101001010010100101110101
0101110010000010001010101001001010010100101001010010100101010010010100
1010010100101001010010111010101011100100000100010101010010010100101001
0100101001010010101001001010010100101001010010100101110101010111001000





























The MIT Press Essential Knowledge Series

AI ETHICS
MARK COECKELBERGH 













(c) 2020 The Massachusetts Institute of Technology

All rights reserved. No part of this book may be reproduced in any form
by any electronic or mechanical means (including photocopying, recording, or information storage and retrieval) without permission in writing from the publisher.

This book was set in Chaparral Pro by Toppan Best-set Premedia Limited. Library of Congress Cataloging-in-Publication Data
Names: Coeckelbergh, Mark, author. Title: AI ethics / Mark Coeckelbergh.
Description: Cambridge, MA : The MIT Press, [2020] | Series: The MIT Press essential knowledge series | Includes bibliographical references and index.
Identifiers: LCCN 2019018827 | ISBN 9780262538190 (pbk. :
alk. paper)
Subjects: LCSH: Artificial intelligence-Moral and ethical aspects.
Classification: LCC Q334.7 .C64 2020 | DDC 170-dc23 LC record available at https://lccn.loc.gov/2019018827

10  9  8  7  6  5  4  3  2  1












for Arno



CONTENTS 








Series Foreword	ix
Acknowledgments	xi


1 Mirror, Mirror, on the Wall	1
2 Superintelligence, Monsters, and the AI Apocalypse	11
3 All about the Human	31
4 Just Machines?	47
5 The Technology	63
6 Don't Forget the Data (Science)	83
7 Privacy and the Other Usual Suspects	97
8 A-responsible Machines and Unexplainable Decisions	109
9 Bias and the Meaning of Life	125
10 Policy Proposals	145
11 Challenges for Policymakers	167
12 It's the Climate, Stupid! On Priorities, the Anthropocene, and Elon Musk's Car in Space	183

Glossary	203
Notes	207
References	211
Further Reading	221
Index	225




























viii	CONTENTS 

SERIES FOREWORD 


The MIT Press Essential Knowledge series offers acces- sible, concise, beautifully produced pocket-size books on topics of current interest. Written by leading thinkers, the books in this series deliver expert overviews of subjects that range from the cultural and the historical to the sci- entific and the technical.
   In today's era of instant information gratification, we have ready access to opinions, rationalizations, and super- ficial descriptions. Much harder to come by is the founda- tional knowledge that informs a principled understanding of the world. Essential Knowledge books fill that need. Synthesizing specialized subject matter for nonspecialists and engaging critical topics through fundamentals, each of these compact volumes offers readers a point of access to complex ideas.

Bruce Tidor
Professor of Biological Engineering and Computer Science Massachusetts Institute of Technology



ACKNOWLEDGMENTS 


This book not only draws on my own work on this topic but reflects the knowledge and experience of the entire field of AI ethics. It would be impossible to list all the people I have discussed with and learned from over the past years, but the relevant and fast-growing communities I know in- clude AI researchers such as Joanna Bryson and Luc Steels, fellow philosophers of technology such as Shannon Vallor and Luciano Floridi, academics working on responsible in- novation in the Netherlands and the UK such as Bernd Stahl at De Montfort University, people I met in Vienna such as Robert Trappl, Sarah Spiekermann, and Wolfgang (Bill) Price, and my fellow members of the policy-oriented advisory bodies High-Level Expert Group on AI (European Commission) and Austrian Council on Robotics and Arti- ficial Intelligence, for example Raja Chatila, Virginia Dig- num, Jeroen van den Hoven, Sabine Köszegi, and Matthias Scheutz-to name just a few. I would also like to warmly thank Zachary Storms for helping with proofreading and formatting, and Lena Starkl and Isabel Walter for support with literature search.



1



MIRROR, MIRROR, ON THE WALL 




The AI Hype and Fears: Mirror, Mirror, on the Wall, Who Is the Smartest of Us All?

When the results are announced, Lee Sedol's eyes swell with tears. AlphaGo, an artificial intelligence (AI) devel- oped by Google's DeepMind, just secured a 4-1 victory in the game Go. It is March 2016. Two decades earlier, chess grandmaster Garry Kasparov lost to the machine Deep Blue, and now a computer program had won against eighteen-time world champion Lee Sedol in a complex game that was seen as one that only humans could play, using their intuition and strategic thinking. The computer won not by following rules given to it by programmers but by means of machine learning based on millions of past Go matches and by playing against itself. In such a case, programmers prepare the data sets and create the

algorithms, but cannot know which moves the program will come up with. The AI learns by itself. After a num- ber of unusual and surprising moves, Lee had to resign (Borowiec 2016).
   An impressive achievement by the AI. But it also raises concerns. There is admiration for the beauty of the moves, but also sadness, even fear. There is the hope that even smarter AIs could help us to revolutionize health care or find solutions for all kinds of societal problems, but also the worry that machines will take over. Could machines outsmart us and control us? Is AI still a mere tool, or is it slowly but surely becoming our master? These fears re- mind us of the words of the AI computer HAL in Stanley Kubrick's science fiction film 2001: A Space Odyssey, who in response to the human command to "Open the pod bay doors" answers: "I'm afraid I can't do that, Dave.". And if not fear, there may be a feeling of sadness or disappoint- ment. Darwin and Freud dethroned our beliefs of excep- tionalism, our feelings of superiority, and our fantasies of control; today, artificial intelligence seems to deal yet another blow to humanity's self-image. If a machine can do this, what is left for us? What are we? Are we just ma- chines? Are we inferior machines, with too many bugs? What is to become of us? Will we become the slaves of ma- chines? Or worse, a mere energy resource, as in the film The Matrix?



2	ChapTEr 1

The Real and Pervasive Impact of AI

But the breakthroughs of artificial intelligence are not limited to games or the realm of science fiction. AI is al- ready happening today and it is pervasive, often invisibly embedded in our day-to-day tools and as part of complex technological systems (Boddington 2017). Given the expo- nential growth of computer power, the availability of (big) data due to social media and the massive use of billons of smartphones, and fast mobile networks, AI, especially machine learning, has made significant progress. This has enabled algorithms to take over many of our activities, in- cluding planning, speech, face recognition, and decision making. AI has applications in many domains, including transport, marketing, health care, finance and insurance, security and the military, science, education, office work and personal assistance (e.g., Google Duplex1), entertain- ment, the arts (e.g., music retrieval and composition), ag- riculture, and of course manufacturing.
   AI is created and used by IT and internet companies. For example, Google has always used AI for its search en- gine. Facebook uses AI for targeted advertising and photo tagging. Microsoft and Apple use AI to power their digi- tal assistants. But the application of AI is wider than the IT sector defined in a narrow sense. For example, there are many concrete plans for, and experiments with, self- driving cars. This technology is also based on AI. Drones











AI is already happening today and it is pervasive, often invisibly embedded in our day-
to-day tools.

use AI, as do autonomous weapons that can kill without human intervention. And AI has already been used in de- cision making in courts. In the United States, for exam- ple, the COMPAS system has been used to predict who is likely to re-offend. AI also enters domains that we gener- ally consider to be more personal or intimate. For example, machines can now read our faces: not only to identify us, but also to read our emotions and retrieve all kinds of information.


The Need to Discuss Ethical and Societal Problems

AI can have many benefits. It can be used to improve public and commercial services. For example, image recognition is good news for medicine: it can help with the diagnosing of diseases such as cancer and Alzheimer. But such every- day applications of artificial intelligence also show how the new technologies raise ethical concerns. Let me give some examples of questions in AI ethics.
   Should self-driving cars have built-in ethical con- straints, and if so, what kind of constraints, and how should they be determined? For example, if a self-driving car gets into a situation where it must choose between driving into a child or into a wall to save the child's life but potentially killing its passenger, what should it choose? And should autonomous lethal weapons be allowed at

all? How many decisions and how much of those deci- sions do we want to delegate to AI? And who is respon- sible when something goes wrong? In one case, the judges put more faith in the COMPAS algorithm than in agree- ments reached by the defense and the prosecution.2 Will we rely too much on AI? The COMPAS algorithm is also highly controversial since research has shown that the algorithm's false positives (people who were predicted to re-offend but did not) were disproportionately black (Fry 2018). AI can thus reinforce bias and unjust discrimina- tion. Similar problems can arise with algorithms that rec- ommend decisions about mortgage applications and job applications. Or consider so-called predictive policing: al- gorithms are used to forecast where crimes are likely to occur (e.g., which area of a city) and who might commit them, but the result might be that specific socioeconomic or racial groups will be disproportionately targeted by po- lice surveillance. Predictive policing has already been used in the United States and, as a recent AlgorithmWatch (2019) report shows, also in Europe.3 And AI-based facial recognition technology is often used for surveillance and can violate people's privacy. It can also more or less pre- dict sexual preferences. No information from your phone and no biometric data are needed. The machine does its work from a distance. With cameras on the street and other public spaces, we can be identified and "read," in- cluding our mood. By means of analysis of our data, our

mental and bodily health can be predicted-without us knowing it. Employers can use the technology to moni- tor our performance. And algorithms that are active on social media can spread hate speech or false information; for example, political bots can appear as real people and post political content. A known case is the 2016 Microsoft chatbot named Tay that was designed to have playful con- versations on Twitter but, when it got smarter, started to tweet racist things. Some AI algorithms can even create false video speeches, such as the video that was composed to misleadingly resemble a speech by Barack Obama.4
   The intentions are often good. But these ethical prob- lems are usually unintended consequences of the technol- ogy: most of these effects, such as bias or hate speech, were not intended by the developers or users of the technology. Moreover, one critical question to be asked is always: Im- provement for whom? The government or the citizens? The police or those who are targeted by the police? The retailer or the customer? The judges or the accused? Ques- tions concerning power come into play, for instance when the technology is shaped by only a few mega corporations (Nemitz 2018). Who shapes the future of AI?
   This question points up the social and political signifi- cance of AI. AI ethics is about technological change and its impact on individual lives, but also about transforma- tions in society and in the economy. The issues of bias and discrimination already indicate that AI has societal

 relevance. But it is also changing the economy and there- fore perhaps the social structure of our societies. Accord- ing to Brynjolfsson and McAfee (2014), we have entered a Second Machine Age in which machines are not only complements to humans, as in the Industrial Revolution, but also substitutes. As professions and work of all kinds will be affected by AI, our society has been predicted to change dramatically as technologies once described in science fiction enter the real world (McAfee and Bryn- jolfsson 2017). What is the future of work? What kind of lives will we have when AIs take over jobs? And who is the "we"? Who will gain from this transformation, and who will lose?


This Book

Based on spectacular breakthroughs, a lot of hype sur- rounds AI. And AI is already used in a wide range of knowl- edge domains and human practices. The first has given rise to wild speculations about the technological future and interesting philosophical discussions about what it means to be human. The second has created a sense of ur- gency on the part of ethicists and policymakers to ensure that this technology benefits us instead of creating insur- mountable challenges for individuals and societies. These latter concerns are more practical and immediate.









AI ethics is about technological change and its impact on individual lives, but also about transformations in society and in the economy.

   This book, written by an academic philosopher who also has experience with advice for policymaking, deals with both aspects: it treats ethics as related to all these questions. It aims to give the reader a good overview of the ethical problems with AI understood broadly, rang- ing from influential narratives about the future of AI and philosophical questions about the nature and future of the human, to ethical concerns about responsibility and bias and how to deal with real-world practical issues raised by the technology by means of policy-preferably before it is too late.
   What happens when it is "too late"? Some scenarios are dystopian and utopian at the same time. Let me start with some dreams and nightmares about the technologi- cal future, influential narratives that, at least at first sight, seem relevant to evaluating the potential benefits and dangers of artificial intelligence.

2



SUPERINTELLIGENCE, MONSTERS, AND THE AI APOCALYPSE 



Superintelligence and Transhumanism

The hype surrounding AI has given rise to all kinds of speculations about the future of AI and indeed the future of what it is to be human. One popular idea, which is not only repeated often in the media and in the public dis- course about AI but is also entertained by influential tech people who develop AI technology such as Elon Musk and Ray Kurzweil, is that of superintelligence and, more gen- erally, the idea that machines will take over, will master us rather than the other way around. For some, this is a dream; for many, a nightmare. And for some, it is both at the same time.
   The idea of superintelligence is that machines will surpass human intelligence. It is often connected with the idea of an intelligence explosion and a technological

singularity. According to Nick Bostrom (2014), our predic- ament will be comparable to that of gorillas, whose fate is today entirely dependent on us. He sees at least two paths to superintelligence and what is sometimes called an in- telligence explosion. One is that AI will develop recursive self-improvement: an AI could design an improved ver- sion of itself, which in turn designs a smarter version of itself, and so on. Another path is whole brain emulation or uploading: a biological brain that could be scanned, mod- eled, and reproduced in and by intelligence software. This simulation of a biological brain would then be connected to a robot body. Such developments would then lead to an explosion of nonhuman intelligence. Max Tegmark (2017) imagines that a team could create an AI that will become all-powerful and run the planet. And Yuval Harari writes about a world in which humans no longer dominate but worship data and trust algorithms to make their decisions. After all humanist illusions and liberal institutions are de- stroyed, humans dream only of merging into the data flow. The AI follows its own path, "going where no human has gone before-and where no human can follow" (Harari 2015, 393).
   The idea of an intelligence explosion is closely related to that of the technological singularity: a moment in human history when exponential technological progress would bring such a dramatic change that we no longer compre- hend what happens and "human affairs as we understand

them today came to an end" (Shanahan 2015, xv). In 1965, the British mathematician Irving John Good speculated about an ultraintelligent machine that designs better ma- chines; in the 1990s, science fiction author and computer scientist Vernor Vinge argued that this would mean the end of the human era. Computer science pioneer John von Neumann already suggested the idea in the 1950s. Ray Kurzweil (2005) embraced the term "singularity" and predicted that AI, together with computers, genetics, nan- otechnology, and robotics, will lead to a point when ma- chine intelligence will be more powerful than all human intelligence combined, and when, ultimately, human and machine intelligence will merge. Humans will transcend the limitations of their biological bodies. And as the title of his book states: the singularity is near. He thinks it's going to happen around 2045.
   This story need not have a happy ending: for Bostrom, Tegmark, and others, "existential risks" are attached to superintelligence. The result of these developments may be that a superintelligent AI takes over and threatens hu- man intelligent life. Whether such an entity would be con- scious or not, and more generally whatever its status or how it comes into being, the worry here is about what the entity would do (or not do). The AI may not care about our human goals. Having no biological body, it would not even understand human suffering. Bostrom offers the thought experiment of an AI that is given the goal of maximizing

the manufacture of paperclips, which it does by converting the Earth and the humans who live on it into resources for producing paperclips. The challenge for us today, then, is to make sure that we build AI that somehow does not raise this control problem-that it does what we want and takes into consideration our rights. For example, should we somehow limit the AI's capabilities? How are we to contain AI?1
A related cluster of ideas is transhumanism. In light of
superintelligence and disappointment with human frailty and "errors," transhumanists such as Bostrom argue that we need to enhance the human being: make it smarter, less vulnerable to disease, live longer, and potentially even immortal-thus leading to what Harari calls the Homo deus: humans have been upgraded into gods. As Francis Bacon already said in "The Refutation of Philosophies": humans are "mortal gods" (Bacon 1964, 106). Why not try to achieve immortality? But even if that could not be achieved, the human machine needs an upgrade, accord- ing to transhumanists. If we don't do that, humans risk remaining "the slow and increasingly inefficient part of" AI (Armstrong 2014, 23). The human biology needs to be reengineered, and, some transhumanists argue, why not dispense with biological parts altogether and design non- organic intelligent beings?
   Although most philosophers and scientists who enter- tain these ideas take care to distinguish their views from

science fiction and religion, many researchers interpret their ideas in precisely these terms. For a start, it is not clear how relevant their ideas are to current technological developments and AI science, and whether there is a real chance that we will get to superintelligence in the foresee- able future-if we can get to it at all. Some straightfor- wardly reject its very possibility (see the next chapter), and those who are prepared to accept that it is possible in principle, such as, for example, scientist Margaret Boden, do not think it is likely to happen in practice. The idea of superintelligence assumes that we will develop so-called general artificial intelligence, or intelligence that matches or exceeds that of humans, and there are many hurdles to get over before we achieve this. Boden (2016) has ar- gued that AI is less promising than many people assume. And a White House report from 2016 endorses a consen- sus among private-sector experts that general AI will not be achieved for at least decades. Many researchers in AI also reject the dystopian views that Bostrom and others promote, and stress the positive use of AI as helper or teammate. But the question is not only what will actually happen in the future. Another concern is that this discus- sion about the (far-off) future impacts of AI distracts from the real and current risks of actually deployed systems (Crawford and Calo 2016). There seems to be a real risk that in the near future the systems will not be smart enough and that we will insufficiently understand their ethical and

societal implications and nevertheless use them widely. The overemphasis on intelligence as humanity's main feature and our only ultimate goal is also questionable (Boddington 2017).
   Nevertheless, ideas such as superintelligence con- tinue to influence the public discussion. They are also likely to have an impact on technology development. For example, Ray Kurzweil is not only a futurist. Since 2012 he has been director of engineering at Google. And Elon Musk, CEO of Tesla and SpaceX and a very well-known public figure, seems to endorse the superintelligence and existential risk scenarios (doom scenarios?) from Bostrom and Kurzweil. He has repeatedly warned of the dangers of artificial intelligence, seeing it as an existential threat and claiming that we cannot control the demon (Dowd 2017). He thinks humans will probably go extinct, unless human and machine intelligence merge or we manage to escape to Mars.
   Perhaps these ideas are so influential because they touch on deep concerns and hopes regarding humans and machines that are present in our collective consciousness. Whether or not one rejects these particular ideas, there are clear links to fictional narratives in human culture and his- tory that try to make sense of the human and our relation to machines. It's worth making these narratives explicit in order to contextualize and better understand some of the ideas. More generally, it is important to incorporate

narrative research into AI ethics-for example, to under- stand why certain narratives are prevalent, by whom they are created, and who benefits from them (Royal Society 2018). It can also help us to construct new narratives of the future of AI.


Frankenstein's New Monster

One way to get beyond the hype is to consider some rel- evant narratives from the history of human culture that shape the current public discussion about AI. This is not the first time that people have asked questions about the future of humanity and the future of technology. And, however exotic some ideas about AI may appear, we can explore connections with rather familiar ideas and narratives that are present in our collective conscious- ness, or more precisely, the collective consciousness of the West.
   First, there is a long history of thinking about humans and machines or artificial creatures, in both Western and non-Western cultures. The idea of creating living beings from inanimate matter can be found in creation stories in Sumerian, Chinese, and Jewish, Christian, and Muslim traditions. The ancient Greeks already had the idea of cre- ating artificial humans, in particular artificial women. For example, in The Iliad, Hephaestus is said to be assisted by

servants made from gold to look like women. In the fa- mous myth of Pygmalion, a sculptor falls in love with the ivory statue of a woman he's made. He wishes that she would come to life, and the goddess Aphrodite grants his wish: her lips become warm and her body soft. We can eas- ily see the link here to contemporary sex robots.
   These narratives come not only from myths: in his book Automata, the Greek mathematician and engineer Hero of Alexandria (ca. 10-ca. 70 CE) published descrip- tions of machines that made people in temples believe they were seeing acts of the gods; in 1901, an artifact was found in the sea, the Antikythera mechanism, which has been identified as an ancient Greek analog computer based on a complex clockwork mechanism. But fictional stories in which machines become human-like especially fascinate us. Consider, for example, the legend of the Golem: a monster made of clay created by a rabbi in the sixteenth century, which then gets out of control. Here we encounter an early version of the control problem. The myth of Prometheus is also often interpreted in this way: he steals fire from the gods and gives it to humans, but is then punished. His eternal torment is to be bound to a rock while every day an eagle eats his liver. The ancient lesson was to warn of hubris: such powers are not meant for mortals.
   However, in Mary Shelley's Frankenstein-which has the telling subtitle The Modern Prometheus-the creation

of intelligent life from lifeless matter becomes a modern scientific project. The scientist Victor Frankenstein cre- ates a human-like being from the parts of corpses, but loses control over his creation. Whereas the rabbi can still control the Golem in the end, that is not so in this case. Frankenstein can be seen as a Romantic novel that warns of modern technology, but it is informed by the science of its day. For example, the use of electricity-then a very new technology-plays an important role: it is used to animate the corpse. It also makes references to magnetism and anatomy. Thinkers and writers at the time debated about the nature and origin of life. What is the life force? Mary Shelley was influenced by the science of her day.2 The story shows how nineteenth-century Romantics were often fas- cinated by science, as much as they hoped for poetry and literature to liberate us from the darker sides of modernity (Coeckelbergh 2017). The novel should not necessarily be seen as against science and technology: the main message seems to be that scientists need to take responsibility for their creations. The monster runs away, but it does so because its creator rejects it. This lesson is important to keep in mind for the ethics of AI. Nevertheless, the novel clearly stresses the danger of technology that goes wild, in particular the danger of artificial humans running amok. This fear resurfaces in contemporary concerns about AI getting out of control.






In Mary Shelley's Frankenstein-which has the telling subtitle The Modern Prometheus-the creation of intelligent
life from lifeless matter becomes a modern scientific project.

   Moreover, as in Frankenstein and the Golem legend, a narrative of competition emerges: the artificial creation competes with the human. This narrative continues to shape our science fiction about AI, but also our con- temporary thinking about technologies such as AI and robotics. Consider the 1920 play R.U.R. which is about robot slaves that revolt against their masters, the already- mentioned 2001: A Space Odyssey from 1968 in which an AI starts killing the crew in order to fulfill its mission, or the 2015 film Ex Machina, in which AI robot Ava turns on its creator. The Terminator films also fit this narra- tive of machines turning against us. The science-fiction writer Isaac Asimov called this fear "the Frankenstein complex": fear of robots. This is also relevant to AI today. It is something scientists and investors have to deal with. Some argue against it; others help to create and sustain the fear. I've already mentioned Musk. Another example of an influential figure who has spread fear about AI is physicist Stephen Hawking, who said in 2017 that the cre- ation of AI could be the worst event in the history of our civilization (Kharpal 2017). The Frankenstein complex is widespread and deeply rooted in Western culture and civilization.

Transcendence and the AI Apocalypse

Ideas such as transhumanism and the technological singu- larity have precedents or at least parallels in the history of Western religious and philosophical thinking, especially in the Judeo-Christian tradition and Platonism. In con- trast to what many people think, religion and technology have always been connected in the history of Western culture. Let me limit my discussion to transcendence and apocalypse.
   In theistic religion, transcendence means that a god is "above" and independent of the material and physical world, as opposed to in the world and part of the world (immanence). In the Judeo-Christian monotheistic tradi- tion, God is seen as transcending his creation. God can also be seen at the same time as permeating all creation and beings (immanence), and, for example, in Catholic theol- ogy, God is understood as revealing himself immanently through his son (Christ) and the Holy Spirit. Frankenstei- nian narratives about AI seem to stress transcendence in the sense of a split or gap between creator and creation (between Homo deus and AI), without giving much hope that this split or gap can be bridged.
   Transcendence can also refer to going beyond limits, surpassing something. In Western religious and philo- sophical history, this idea often took the shape of going above and beyond the limits of the material and physical










In contrast to what many people think, religion and technology have always been connected in the history of Western culture.

world. For example, in the second-century CE Mediterra- nean world, Gnosticism saw all matter as evil and aimed at liberating the divine spark from the human body. Earlier, Plato saw the body as the prison of the soul. In contrast to the body, the soul is seen as immortal. In his metaphys- ics, he distinguished between the forms, which are eter- nal, and the things in the world, which are changing-the former thus transcend the latter. In transhumanism, we see some ideas that are reminiscent of this. Not only does it retain the goal of transcendence in the sense of over- coming human limitations, but also the specific ways this transcendence is supposed to happen evoke Plato and Gnosticism: to reach immortality, the biological body must be transcended by means of uploading and the de- velopment of artificial agents. More generally, when AI and related science and technology use mathematics to abstract more pure forms from the messy material world, this can be interpreted as a Platonic program realized by technological means. The AI algorithm turns out to be a Platonic machine that extracts form (a model) from the (data) world of appearances.
   Transcendence can also mean surpassing the human condition. In the Christian tradition, this can take the form of trying to bridge the gap between God and hu- mans by making humans into gods, perhaps by restoring their original God-likeness and perfection (Noble 1997). But the transhumanist quest for immortality is an ancient

one. It can be found already in Mesopotamian mythology: one of the oldest written tales of humanity, the Epic of Gil- gamesh tells the story of the king of Uruk (Gilgamesh) who seeks immortality after the death of his friend Enkidu. He does not find it: he manages to pluck a plant that is said to restore youth, but a serpent steals it, and in the end, he has to learn the lesson that he must face the reality of his own death; the quest for immortality is futile. Throughout the history of humanity, people have searched for the elixir of life. Today science looks for anti-aging therapies. In this sense, the transhumanist quest for immortality or longev- ity is not new or exotic; it is one of the oldest dreams of humanity and an exponent of some contemporary science. In transhumanist hands, AI becomes a transcendence ma- chine that promises immortality.
Other ancient concepts that help us to contextualize
transhumanist ideas, in particular the technological singu- larity, are apocalypse and eschatology. The ancient Greek term apocalypse, which also plays a role in the Jewish and Christian world, refers to revelation. Today it often refers to the content of a particular kind of revelation: the vision of an end time or end-of-the-world scenario. In religious contexts, we find the term eschatology: a part of theology concerned with the final events of history and the ulti- mate destiny of humanity. Most apocalyptic and eschato- logical ideas involve a radical and often violent disruption or destruction of the world, while heading toward a new,


SUp EriNTElliGENCE a ND ThE ai apOCalYp SE	25

higher reality, being, and level of consciousness. This also reminds us of so-called doomsday cults and sects, which were and are all about predicting disaster and the end of the world. While typically transhumanists have nothing to do with such religious cults and practices, clearly the idea of a technological singularity bears some resemblance to apocalyptic, eschatological, and doomsday narratives.
   Thus, while the development of AI is based on a sci- ence that is supposed to be nonfictional and secularized, and while transhumanists usually distance themselves from religion and reject any suggestion that their works are fictional, science fiction and ancient religious and phil- osophical ideas inevitably play a role when we discuss the future of AI in these terms.


How to Go beyond Competition Narratives and beyond the Hype

Now one may ask: is there a way out? Can we go beyond competition narratives and find more immanent ways of making sense of the future of AI and similar technologies? Or is Western thinking about AI doomed to remain in the prison of these modern fears and fascinations and their ancient roots? Can we get beyond the hype, or will the discussion remain focused on superintelligence? I think we have ways out.



While typically transhumanists have nothing to do with such religious cults and practices, clearly the idea of a technological singularity bears
some resemblance
to apocalyptic, eschatological, and doomsday narratives.

   First, we can look beyond Western culture to find different kinds of non-Frankensteinian narratives about technology and non-Platonic ways of thinking. For ex- ample, in Japan, where technological culture is still more influenced by nature religion than in the West, in particu- lar by the Shinto religion, and where popular culture has portrayed machines as helpers, we find a friendlier atti- tude toward robots and AI. Here, we find no Frankenstein complex. What is sometimes called an "animistic" way of thinking implies that AIs can also in principle have spirit or soul, can be experienced as sacred. This means that there is no narrative of competition-and no Platonic desire to transcend materiality and to constantly defend the human as being above and beyond the machine, or fundamentally different from the machine. To my knowl- edge, Eastern culture also has no ideas concerning an end time. In contrast to the monotheistic religions, nature re- ligions have a cyclical understanding of time. Thus, look- ing beyond Western culture (or indeed to the West's own ancient past, where we also find nature religions) can help us critically evaluate the dominant narratives about the future of AI.
   Second, to get beyond the hype and not limit the ethi- cal discussion about AI to dreams and nightmares about the distant future, we can (1) use philosophy and science to critically examine and discuss the assumptions about AI and the human that play a role in these scenarios and

discussions (e.g., Is general intelligence possible? What is the difference between humans and machines? What is the relation between humans and technology? What is the moral status of AI?); (2) look in more detail at what exist- ing AI is and what it does today in various applications;
(3) discuss more concrete and pressing ethical and societal problems raised by AI as it is applied today; (4) investigate AI policy for the near future; and (5) question whether the focus on AI in current public discourse is helpful in light of other problems we face, and whether intelligence should be our only focus. We will follow these paths in the next chapters.





















SUp EriNTElliGENCE a ND ThE ai apOCalYp SE	29



3


ALL ABOUT THE HUMAN 




Is General AI Possible? Are There Fundamental Differences between Humans and Machines?

The transhumanist vision of the technological future as- sumes that general artificial intelligence (or strong AI) is possible, but is it? That is, can we create machines with human-like cognitive capacities? If the answer is no, then the entire superintelligence vision is irrelevant to AI eth- ics. If human general intelligence is not possible in ma- chines, we don't have to worry about superintelligence. More generally, our evaluation of AI seems to depend on what we think AI is and can become, and on how we think about the differences between humans and machines. At least since the mid-twentieth century, philosophers and scientists have debated what computers are able to do and become, and what the differences are between humans

and intelligent machines. Let's have a look at some of these discussions, which are as much about what the hu- man is and should be as they are about what AI is and should be.
   Can computers have intelligence, consciousness, and creativity? Can they make sense of things and understand meaning? There is a history of criticism and skepticism about the possibility of human-like AI. In 1972, Hubert Dreyfus, a philosopher with a background in phenomenol- ogy, published a book called What Computers Can't Do.1 Since the 1960s, Dreyfus had been very critical about the philosophical basis of AI and had questioned its promises: he argued that the AI research program was doomed to fail. Before moving to Berkeley, he was working at MIT, an important place for the development of AI, which at the time was based mainly on symbolic manipulation. Drey- fus argued that the brain is not a computer and that the mind does not operate by means of symbolic manipula- tion. We have an unconscious background of common- sense knowledge based on experience and what Heidegger would call our "being-in-the-world," and this knowledge is tacit and cannot be formalized. Human expertise, Dreyfus argued, is based on know-how rather than know-that. AI cannot capture this background meaning and knowledge; if that's what AI aims at, it's basically alchemy and my- thology. Only human beings can see what is relevant be- cause, as embodied and existential beings, we are involved













There is a history of criticism and skepticism about the possibility of human-like AI.

in the world and are able to respond to the demands of the situation.
   At the time, Dreyfus met much opposition, but later, many AI researchers would no longer promise or predict general AI. AI research moved away from reliance on sym- bol manipulation toward new models, including statistics- based machine learning. And while at Dreyfus's time there was still a huge gap between phenomenology and AI, to- day many AI researchers embrace embodied and situated cognitive science approaches, which claim to be closer to phenomenology.
   That being said, Dreyfus's objections are still rele- vant and show how views of the human being, especially but not only in so-called continental philosophy, often clash with scientific worldviews. Continental philoso- phers usually stress that human beings and minds are fun- damentally different from machines, and focus on (self-) conscious human experience and human existence, which cannot and should not be reduced to formal descriptions and scientific explanations. Other philosophers, however, often from the analytic tradition of philosophy, endorse a view of the human being that supports AI researchers who think that the human brain and mind really are and work like their computer models. Philosophers such as Paul Churchland and Daniel Dennett are good examples of the latter. Churchland thinks that science, in particu- lar evolutionary biology, neuroscience, and AI, can fully

explain human consciousness. He thinks that the brain is a recurrent neutral network. His so-called eliminative materialism denies the existence of immaterial thoughts and experiences. What we call thoughts and experiences are just brain states. Dennett too denies the existence of anything above what happens in the body: he thinks that we are "a sort of robot ourselves" (Dennett 1997). And if the human is basically a conscious machine, then such machines are possible, and not just in principle but as a matter of fact. We can try to make them. Interestingly, both continental and analytic philosophers thus argue against a Cartesian dualism that splits mind and body, but for different reasons: the first because they think that hu- man existence is about being-in-the-world in which mind and body are not separated, the latter because for mate- rialist reasons they think that mind is nothing separate from body.
   But not all philosophers in the analytic tradition think that general or strong AI is possible. From a (later) Wittgensteinian point of view, one can argue that while a set of rules can describe a cognitive phenomenon, that doesn't imply that we actually have rules in our head (Ar- koudas and Bringsjord 2014). As with Dreyfus's criticism, this at least problematizes one kind of AI, symbolic AI, if it assumes that this is how humans think. Another famous philosophical criticism of AI comes from John Searle, who argues against the idea that computer programs could


all abOUT ThE hUMa N	35

have genuine cognitive states or understand meaning (Searle 1980). The thought experiment he offers, called the Chinese room argument, goes as follows: Searle is locked in a room and given Chinese writings but doesn't know Chinese. However, he can answer questions given to him by Chinese speakers outside the room because he uses a rulebook that enables him to produce the right answers (output) based on the documents (input) he is given. He can do that successfully without understanding Chinese. Similarly, Searle argues, computer programs can produce an output based on an input by means of rules that are given to them, but they don't understand anything. In more technical philosophical terms: computer programs don't have intentionality, and genuine understanding can- not be generated by formal computation. As Boden (2016) puts it, the idea is that meaning comes from humans.
   While today's AI computer programs are often dif- ferent from those Dreyfus and Searle criticized, the de- bate continues. Many philosophers think that there are crucial differences between how humans and computers think. For example, today one can still object that we are meaning-making, conscious, embodied, and living beings whose nature, mind, and knowledge cannot be explained away by comparisons to machines. Note again, however, that even those scientists and philosophers who believe that in principle there is much similarity between humans and machines, and that in theory general AI is possible,



We are meaning-making, conscious, embodied, and living beings
whose nature, mind, and knowledge cannot be explained away by comparisons to machines.

often reject Bostrom's vision of superintelligence and sim- ilar ideas that hold human-like AI to be around the corner. Both Boden and Dennett think that general AI is very dif- ficult to realize in practice and is hence not something to worry about today.
   In the background of the discussion about AI are thus deep disagreements about the nature of the human, hu- man intelligence, mind, understanding, consciousness, creativity, meaning, human knowledge, science, and so on. If it is a "battle" at all, it is one that is as much about the human as it is about AI.


Modernity, (Post)humanism, and Postphenomenology

From a broader humanities point of view, it is interesting to contextualize these debates about AI and the human further in order to show what is at stake. They are not only about technology and the human but reflect deep di- vides in modernity. Let me briefly touch on three divides that indirectly shape the ethical discussions about AI. The first is an early modern divide between the Enlightenment and Romanticism. The others are relatively recent devel- opments: one is between humanism and transhumanism, which stays within the tensions of modernity, and one is between humanism and posthumanism, which attempts to go beyond modernity.

   A first way of making sense of the debate about AI and the human is to consider the tension in modernity between the Enlightenment and Romanticism. In the eigh- teenth and nineteenth centuries, Enlightenment thinkers and scientists challenged traditional religious views and argued that reason, skepticism, and science would show us how humans and the world really are, as opposed to how it might seem given beliefs that are unjustified by arguments and unsupported by evidence. They were op- timistic about what science could do to benefit humanity. In response, Romantics argued that abstract reason and modern science had disenchanted the world and that we need to bring back the mystery and wonder that science wanted to eliminate. Looking at the debate about AI, it seems that we have not moved on much from there. Den- nett's work on consciousness and Boden's work on creativ- ity, for example, are aimed at explaining away, at "breaking the spell," as Dennett puts it. These thinkers are optimis- tic that science can unravel the mystery of consciousness, creativity, and so on. They react against those who resist such efforts to disenchant the human, such as continen- tal philosophers who work in the tradition of postmod- ernism and stress the mystery of being human-in other words: the new Romantics. "Break the spell, or hold on to the wonders of the human being?" seems, then, a pivotal question in discussions about general AI and its future.

   A second tension is between humanists and transhu- manists. What is "the human," and what should the human become? Is it important to defend the human as it is, or should we revise our concept of it? Humanists celebrate the human as it is. Ethically speaking, they emphasize the intrinsic and superior value of human beings. In the debate surrounding AI, traces of humanism can be found in arguments that defend human rights and human dig- nity as the basis of an ethics of AI, or in the argument for the centrality of humans and their values in the develop- ment and future of AI. Here humanism often teams up with Enlightenment thinking. But it can also take more conservative or Romantic forms. Humanism can also be found in the resistance against the transhumanist project. Whereas transhumanists think we should move on to a new type of human being that is enhanced by means of science and technology, humanists defend the human as it is and stress the value and dignity of the human, which is said to be threatened by transhumanist science and philosophy.
   Defensive reactions against new technologies have their own history. In the humanities and social sciences, technology has often been criticized as threatening hu- manity and society. Many twentieth-century philoso- phers, for example, were very pessimistic about science and warned against technology dominating society. But now the battle is not only about human lives and society,

it is about the human itself: to enhance or not to enhance, that is the question. On the one side, the human itself be- comes a scientific-technological project, open to improve- ment. Once the spell of the human is broken-by Darwin, neuroscience, and AI-we can get on with making it bet- ter. AI can help us to improve the human. On the other side, we should embrace the human as it is. And, some may say: what the human is always escapes us. It cannot com- pletely be understood by science.
   These tensions continue to divide the minds and hearts in this discussion. Can we get beyond them? Prac- tically, one could give up the goal of creating human-like AI. But even then disagreements remain about the status of AIs as models of humans used by AI science. Do they re- ally teach us something about how humans think? Or do they only teach us something about a particular kind of thinking, a thinking that can be formalized with mathe- matics, for example, or a thinking that aims at control and manipulation? How much can we really learn from these technologies about the human? Is humanity more than science can grasp? Even in more moderate discussions, the struggles about modernity surface.
   To find a way out of this impasse, one could follow scholars in the humanities and social sciences who dur- ing the past fifty years have explored nonmodern ways of thinking. Authors such as Bruno Latour and Tim In- gold have shown that we can find less dualist, more

nonmodern ways of relating to the world that go beyond the Enlightenment-Romanticism opposition. We can then try to cross the modern divide between humans and nonhumans not via modern science or transhumanism, which in their way also see humans and machines not as fundamentally opposed, but via posthumanist thinking from the (post)humanities. This brings us to the third ten- sion: between humanism and posthumanism. Against hu- manists, who are accused of having done violence toward nonhumans such as animals in the name of the supreme value of the human, posthumanists question the central- ity of the human in modern ontologies and ethics. Accord- ing to them, nonhumans matter too, and we should not be afraid of crossing borders between humans and nonhu- mans. This is an interesting direction to explore, since it takes us beyond the competition narrative about humans and machines.
   Posthumanists such as Donna Haraway offer a vision in which living together with machines, and even merging with machines, is seen no longer as a threat or a nightmare, as in humanism, or as a transhumanist dream come true, but as a way in which ontological and political borders be- tween humans and nonhumans can and should be crossed. AI can then be part of not a transhumanist but a critical posthumanist project, which enters from the side of hu- manities and the arts rather than science. Borders are crossed not in the name of science and universal progress,

as some Enlightenment transhumanists may want to say, but in the name of a posthumanist politics and ideology of crossing borders. And posthumanism can also offer some- thing else relevant to AI: it can urge us to acknowledge that nonhumans don't need to be similar to us and should not be made similar to us. Backed up by such a posthumanism, then, it seems that AI can free itself of the burden to imi- tate or rebuild the human and can explore different, non- human kinds of being, intelligence, creativity, and so on. AI need not be made in our image. Progress here means going beyond the human and opening ourselves up to the nonhuman to learn from it. Moreover, both transhuman- ists and posthumanists could agree that instead of compet- ing with an AI for a given task, we could also set a common goal, which then is reached by collaborating and mobilizing the best humans and artificial agents can offer in order to move closer to reaching that common goal.
   Another way of going beyond the competition narra- tive, a way that sometimes comes close to posthumanism, is an approach in philosophy of technology called postphe- nomenology. Dreyfus draws on phenomenology, in par- ticular the work of Heidegger. But postphenomenological thinking, initiated by philosopher Don Ihde, goes beyond phenomenology of technology à la Heidegger by focus- ing on how humans relate to specific technologies and in particular material artifacts. This approach, often col- laborating with science and technology studies, reminds






Backed up by posthumanism, AI can free itself of the burden to imitate or rebuild the human and can explore different, nonhuman kinds of being, intelligence, creativity, and so on.

us of the material dimension of AI. AI is sometimes seen as having a merely abstract or formal nature, unrelated to specific material artifacts and infrastructures. But all the formalizations, abstractions, and symbolic manipulations mentioned earlier rely on material instruments and mate- rial infrastructures. For example, as we will see in the next chapter, contemporary AI relies heavily on networks and the production of large amounts of data with electronic devices. Those networks and devices are not merely "vir- tual" but have to be materially produced and sustained. Moreover, against the modern subject-object divide, post- phenomenologists such as Peter-Paul Verbeek talk about the mutual constitution of humans and technology, sub- ject and object. Instead of seeing technology as a threat, they emphasize that humans are technological (that is, we have always used technology; it is part of our existence rather than something external that threatens that exis- tence) and that technology naturally mediates our engage- ment with the world. For AI, this view seems to imply that the humanist battle to defend the human against technol- ogy is misdirected. Instead, according to this approach, the human has always been technological and therefore we should rather ask how AI mediates humans' relation to the world and try to actively shape these mediations while we still can: we can and should discuss ethics at the stage of AI development rather than complain afterward about the problems it causes.

   However, one may worry that posthumanist and post- phenomenological visions are not critical enough because they are too optimistic and too remote from scientific and engineering practice, and so insufficiently sensitive to the real dangers and ethical and societal consequences of AI. Crossing never-before-crossed borders is not neces- sarily unproblematic, and in practice such posthumanist and postphenomenological ideas might be of little help against the domination and exploitation we may face from technologies such as AI. One may also defend a more traditional view of the human or call for a new kind of humanism, rather than posthumanism. Thus the debate continues.

4



JUST MACHINES?




Questioning the Moral Status of AI: Moral Agency and Moral Patiency

One of the issues that came up in the previous chapter was whether nonhumans matter, too. Today many people think that animals matter, morally speaking. But this was not always the case. Apparently, we were wrong about ani- mals in the past. If today many people think that AIs are just machines, are they making a similar mistake? Would superintelligent AIs, for example, deserve moral status? Would they have to be given rights? Or is it a dangerous idea to even consider the question of whether machines can have moral status?
   One way of discussing what AI is and can become is to ask about the moral status of AI. Here we approach philosophical questions regarding AI, not via metaphysics,

epistemology, or the history of ideas, but rather via moral philosophy. The term moral status (also sometimes called moral standing) can refer to two kinds of questions. The first concerns what the AI is capable of doing morally speaking-in other words, whether it can have what phi- losophers call moral agency, and, if so, whether it can be a full moral agent. What does this mean? It seems that the actions of AIs today already have moral consequences. Most people will agree that AI has a "weak" form of moral agency in this sense, which is similar to, say, most cars to- day: the latter can also have moral consequences. But given that AI is becoming more intelligent and autonomous, can an AI have a stronger form of moral agency? Should it be given or will it develop some capacity for moral reason- ing, judgment, and decision making? For example: can and should self-driving cars that use AI be considered moral agents? These questions are about the ethics of AI, in the sense of what kind of moral capacities does or should an AI have? But questions about "moral status" can also refer to how we should treat an AI. Is an AI "just a machine," or does it deserve some form of moral consideration? Should we treat it differently than, say, a toaster or a washing machine? Would we have to confer rights upon a highly intelligent artificial entity, if such an entity were someday developed, even if it were not human? This is what phi- losophers call the question regarding moral patiency. This question is not about the ethics by or in AI but about our











Is an AI "just a machine"? Should we treat it differently than, say, a toaster or a washing machine?

ethics toward AI. Here the AI is object of ethical concern, rather than a potential ethical agent itself.


Moral Agency

Let's start with the question of moral agency. If an AI were to be more intelligent than is possible today, we can suppose that it could develop moral reasoning and that it could learn how humans make decisions about ethical problems. But would this suffice for full moral agency, that is, for human-like moral agency? The question is not en- tirely science fiction. If we already today hand over some of our decisions to algorithms, for example in cars or courtrooms, then it seems it would be a good thing if those decisions were morally sound. But it is not clear whether machines can have the same moral capacities as humans. They are given agency in the sense that they do things in the world, and these actions have moral consequences. For example, a self-driving car may cause an accident, or an AI may recommend sending a particular person to jail. These behaviors and choices are not morally neutral: there are clearly moral consequences for the people involved. But to deal with this problem, should AIs be given moral agency? Can they have full moral agency?
   There are various philosophical positions on these questions. Some say that machines can never be moral

agents at all. Machines, they argue, do not have the re- quired capacities for moral agency such as mental states, emotions, or free will. Hence it is dangerous to suppose that they can make sound moral decisions and to totally hand over these moral decisions to them. For example, Deborah Johnson (2006) has argued that computer sys- tems have no moral agency of their own: they are pro- duced and used by humans, and only these humans have freedom and are able to act and decide morally. Similarly, one could say that AIs are made by humans and that hence moral decision making in technological practices should be performed by humans. On the other side of the spec- trum are those who think that machines can be full moral agents in the same way that humans are. Researchers such as Michael and Susan Anderson, for example, claim that in principle it is possible and desirable to give machines a human kind of morality (Anderson and Anderson 2011). We can give AIs principles, and machines might even be better than human beings at moral reasoning since they are more rational and do not get carried away by their emotions. Against this position, some have argued that moral rules often conflict (consider, for example, Asimov's robot stories, in which moral laws for robots always get robots and humans in trouble) and that the entire proj- ect of building "moral machines" by giving them rules is based on mistaken assumptions regarding the nature of morality. Morality cannot be reduced to following

rules and is not entirely a matter of human emotions- but the latter may well be indispensable for moral judg- ment. If general AI is possible at all, then we don't want a kind of "psychopath AI" that is perfectly rational but insensitive to human concerns because it lacks emotions (Coeckelbergh 2010).
   For these reasons, we could reject the very idea of full moral agency altogether, or we could take a middle posi- tion: we have to give AIs some kind of morality, but not full morality. Wendell Wallach and Colin Allen use the term "functional morality" (2009, 39). AI systems need some capacity to evaluate the ethical consequences of their actions. The rationale for this decision is clear in the case of self-driving cars: the car will likely get into situ- ations where a moral choice has to be made but there is no time for human decision making or human interven- tion. Sometimes these choices take the form of dilemmas. Philosophers talk about trolley dilemmas, named after a thought experiment in which a trolley barrels down a rail- way track and you have to choose between doing nothing, which will kill five people tied to the track, or pulling a lever and sending the trolley to another track, where only one person is tied down but is someone you know. What is the morally right thing to do? Similarly, proponents of this approach argue, a self-driving car may have to make a moral choice between, for example, killing pedestrians

crossing the road and driving into a wall, thereby killing the driver. What should the car choose? It seems that we will have to make these moral decisions (beforehand) and make sure developers implement them in the cars. Or perhaps we need to build AI cars that learn from humans' choices. However, one may question whether giving AIs rules is a good way to represent human morality, if moral- ity can be "represented" and reproduced at all, and if trol- ley dilemmas capture something that is central to moral life and experience. Or, from an entirely different per- spective, one may ask whether humans are in fact good in making moral choices. Why imitate human morality at all? Transhumanists, for example, may argue that AIs will have a superior morality because they will be more intelligent than us.
   This questioning the focus on the human leads us to another position, which does not require full moral agency and tries to leave the anthropocentric ethical position. Lu- ciano Floridi and J. W. Sanders (2004) have argued for a mindless morality not based on properties that humans have. We could make moral agency dependent on having a sufficient level of interactivity, autonomy, and adaptivity, and on being capable of morally qualifiable action. Accord- ing to these criteria, a search-and-rescue dog is a moral agent, but so is an AI web bot that filters out unwanted emails. Similarly, one could apply nonanthropocentric

criteria for moral agency of robots, as proposed by John Sullins (2006): if an AI is autonomous from programmers and we can explain its behavior by ascribing moral inten- tions to it (like the intention to do good or harm), and if it behaves in a way that shows an understanding of its re- sponsibility to other moral agents, then that AI is a moral agent. Thus, these views do not require full moral agency if that means human moral agency, but rather define moral agency in a way that is in principle independent of human full moral agency and the human capacities required for that. However, would such artificial moral agency be suf- ficient if judged by human moral standards? The practical worry is that, for example, self-driving cars may not be moral enough. The principled worry is that we stray too far from human morality here. Many people think that moral agency is and should be connected to humanness and personhood. They are not willing to endorse posthu- manist or transhumanist notions.


Moral Patiency

Another controversy concerns the moral patiency of AI. Imagine that we have a superintelligent AI. Is it morally acceptable to switch it off, to "kill" it? And closer to to- day's AI: is it ok to kick an AI robot dog?1 If AIs are to be part of everyday life, as many researchers predict, then

such cases will inevitably come up and raise the question of how we humans should behave toward these artificial entities. But again, we do not have to look to the far-off future or to science fiction. Research has shown that al- ready today people empathize with robots and hesitate to "kill" or "torture" them (Suzuki et al. 2015; Darling, Nandy, and Breazeal 2015), even if these robots do not have AI. Humans seem to require very little of artificial agents in order to project personhood or humanness onto them and to empathize with them. If these agents now be- come AI, which potentially make them more human-like (or animal-like), this seems to make the question regard- ing moral patiency only more urgent. For example, how should we respond to people who empathize with an AI? Are they wrong?
   To say that AIs are just machines and that people who empathize with them are simply mistaken in their judg- ment, emotions, and moral experience is perhaps the most intuitive position. At first sight, it seems that we do not owe anything to machines. They are things, not people. Many AI researchers think along these lines. For exam- ple, Joanna Bryson has argued that robots are tools and property and that we have no obligations to them (Bryson 2010). Those who hold this position might well agree that if AIs were to be conscious, have mental states, and so on, we would have to give them moral status. But they will say that this condition is not fulfilled today. As we have

seen in the previous chapters, some will argue that it can never be fulfilled; others think that it could be fulfilled in principle, but that this will not happen any time soon. But the upshot for the question regarding moral status is that today and in the near future AIs are to be treated as things, unless proven otherwise.
   One problem with this position, however, is that it neither explains nor justifies our moral intuitions and moral experiences that tell us there is something wrong with "mistreating" an AI, even if that AI does not have human-like or animal-like properties such as conscious- ness or sentience. To find such justifications, one could turn to Kant, who argued that it is wrong to shoot a dog, not because shooting a dog breaches any duties to the dog, but because such a person "damages the kindly and hu- mane qualities in himself, which he ought to exercise in virtue of his duties to mankind" (Kant 1997). Today we tend to think differently of dogs (although not everyone and everywhere). But it seems that the argument could be applied to AIs: we could say that we owe nothing to an AI, but still should not kick or "torture" the AI because it makes us unkind to humans. One could also use a virtue ethics argument, which is also an indirect argument since it is about humans, not about the AI: "mistreating" an AI is wrong not because any harm is done to the AI, but be- cause our moral character is damaged if we do so. It does not make us into better persons. Against this approach









Some argue that "mistreating" an AI is wrong not because any harm is done to the AI, but because our moral character is damaged
if we do so.

 we could argue that in the future some AIs may have in- trinsic value and deserve our moral concern, provided they have properties such as sentience. An indirect duty or virtue approach does not seem to take seriously this "other" side of the moral relation. It cares only about hu- mans. What about the AIs? But can AIs or robots be others at all, as David Gunkel (2018) has asked? Again, com- mon sense seems to say: no, AIs do not have the required properties.
   An entirely different approach argues that the way we question moral status is problematic. The usual moral reasoning about moral status is based on what morally rel- evant properties entities have-for example, conscious- ness or sentience. But how do we know that the AI really has particular morally relevant properties or not? Are we sure in the case of humans? The skeptic says we are not sure. Yet even without this epistemological certainty we still ascribe moral status to humans on the basis of ap- pearance. This would also be likely to happen if AIs were to have a human-like appearance and behavior in the fu- ture. It seems that whatever is deemed to be morally right by philosophers, humans will anyway ascribe moral sta- tus to such machines and, for example, give them rights. Moreover, if we look more closely at how humans actually ascribe moral status, it turns out that, for example, exist- ing social relations and language play a role. For example,

if we treat our cat kindly, this is not because we engage in moral reasoning about our cat, but because we already have a kind of social relation with it. It is already a pet and companion before we do the philosophical work of ascribing moral status-if we ever felt the need for such an exercise at all. And if we give our dog a personal name, then-in contrast to the nameless animals we eat-we have already conferred a particular moral status on it in- dependent of its objective properties. Using such a rela- tional and critical, nondogmatic approach (Coeckelbergh 2012), we could argue that, similarly, the status of AIs will be ascribed by human beings and will depend on how they will be embedded in our social life, in language, and in human culture.
   Furthermore, since such conditions are historically variable-think again about how we used to treat and think about animals-perhaps some moral caution is needed before we "fix" the moral status of AI in general or any particular AI. And why even talk about AI in gen- eral or in the abstract? It seems that there is something wrong with the moral procedure of ascribing status: in order to judge it, we take the entity out of its relational context, and before we have the result of our moral pro- cedure we already treat it, rather hierarchically, patroniz- ingly, and hegemonically, as an entity we superior human judges will make decisions about. It seems that before we

do our actual reasoning about its moral status, we have already positioned it and perhaps even done violence to it by treating it as the object of our decision making, set- ting up ourselves as central, powerful, and all-knowing gods on Earth who reserve the right to confer moral sta- tus upon other entities. We have also made all situational and social contexts and conditions invisible. As in the trolley dilemma case, we have reduced ethics to a carica- ture. With such reasoning, moral philosophers seem to do what Dreyfusian philosophers accused symbolic AI re- searchers of doing: formalizing and abstracting a wealth of moral experience and knowledge at the cost of leav- ing out what makes us human and-in addition-at the risk of begging the very question of the moral status of nonhumans. Regardless of what the actual moral status of AIs "is," as if this could be defined entirely independent from human subjectivity, it is worth critically examining our own moral attitude and the project of abstract moral reasoning itself.


Toward More Practical Ethical Issues

As the discussions in this and the previous chapter show, thinking about AI not only teaches us something about AI. It also teaches us something about ourselves: about how we think and how we actually do and should relate

to nonhumans. If we look into the philosophical founda- tions of AI ethics, we see deep disagreements about the nature and future of humanity, science, and modernity. Questioning AI opens up an abyss of critical questions about human knowledge, human society, and the nature of human morality.
   These philosophical discussions are less far-fetched and less "academic" than one may think. They will keep resurfacing when, later in this book, we consider more concrete ethical, legal, and policy questions raised by AI. If we try to tackle topics such as responsibility and self- driving cars, the transparency of machine learning, biased AI, or the ethics of sex robots, we soon find ourselves con- fronted with them again. If AI ethics wants to be more than a checklist of issues, it should also have something to say about such questions.
   That being said, it is time now to turn to more practi- cal issues. These concern neither the philosophical prob- lems raised by hypothetical general artificial intelligence, nor the risks connected to superintelligence in the far future, nor other spectacular monsters of science fiction. They are about the less visible and arguably less sexy, but still very important, realities of AIs that are already in ef- fect. AI as it already functions today does not take the role of Frankenstein's monster or the spectacular AI robots that threaten civilization, and is more than a philosophical thought experiment. AI is about the less visible, backstage

but pervasive, powerful, and increasingly smarter tech- nologies that already shape our lives today. AI ethics, then, is about the ethical challenges posed by current and near- future AI and its impact on our societies and vulnerable democracies. AI ethics is about the lives of people and it is about policy. It is about the need for us, as persons and as societies, to deal with the ethical issues now.

5



THE TECHNOLOGY




Before discussing more detailed and concrete ethical prob- lems with AI, we have one more task to do to clear the ground: beyond the hype, we need some understanding of the technology and its applications. Leaving aside trans- humanist science fiction and philosophical speculation about general AI, let's take a look at what AI technology is and does today. As the definitions of AI and other terms are themselves contested, I will not delve too deeply into philosophical discussions or historical contextualization. Here my main purpose is to give the reader an idea of the technology in question and how it is used. Let me start by saying something about AI in general; the next chapter focuses on machine learning and data science and their applications.

What Is Artificial Intelligence?

AI can be defined as intelligence displayed or simulated by code (algorithms) or machines. This definition of AI raises the question of how to define intelligence. Philo- sophically speaking, it is a vague concept. An obvious com- parison is human-like intelligence. For example, Philip Jansen et al. define AI as "the science and engineering of machines with capabilities that are considered intelligent by the standard of human intelligence" (2018, 5). On this view, AI is about creating intelligent machines that think or (re)act like humans. However, many researchers in AI think that intelligence need not be human-like and prefer a more neutral definition that is formulated in terms in- dependent of human intelligence and the related goals of general or strong AI. They enumerate all kinds of cognitive functions and tasks such as learning, perception, planning, natural language processing, reasoning, decision making, and problem solving-the last is also often equated with intelligence per se. For example, Margaret Boden claims that AI "seeks to make computers do the sort of things that minds can do." At first, this makes it sound like hu- mans are the only model. However, she then enumerates all kinds of psychological skills such as perception, pre- diction, and planning, which are part of the "richly struc- tured space of diverse information-processing capacities"

(2016, 1). And this information processing need not be an exclusively human affair. General intelligence, according to Boden, need not be human. Some animals can also be considered intelligent. And transhumanists dream about future minds that are no longer biologically embedded. That being said, the goal of achieving human-like capabili- ties and possibly human-like general intelligence has been part of AI from the beginning.
   The history of AI is closely connected to that of com- puter science and related disciplines such as mathemat- ics and philosophy, and hence reaches back at least to early modern times (Gottfried Wilhelm Leibniz and René Descartes, for example) if not to ancient times, with its stories about craftsmen making artificial beings and inge- nious mechanical artifacts that could trick people (think of animated figures in ancient Greece or human-shaped mechanical figures in ancient China). But as a discipline on its own, AI is generally seen as having started in the 1950s, after the invention of the programmable digital computer in the 1940s and the birth of the discipline of cybernetics, defined by Norbert Wiener in 1948 as the sci- entific study of "control and communication in the animal and the machine" (Wiener 1948). An important moment for the history of AI was the publication of Alan Turing's 1950 paper "Computing Machinery and Intelligence" in Mind, which introduced the famous Turing test but was

more broadly about the question whether machines can think and already speculated about machines that could learn and do abstract tasks. But the Dartmouth workshop that took place in the summer of 1956 in Hanover, New Hampshire, is generally regarded as the birthplace of con- temporary AI. Its organizer John McCarthy coined the term AI, and participants included names such as Marvin Minsky, Claude Shannon, Allen Newell, and Herbert Si- mon. Whereas cybernetics was perceived as being too busy with analog machines, Dartmouth's AI embraced digital machines. The idea was to simulate human intelligence (which is not to re-create: the process is not the same as in humans). Many participants thought that a machine as intelligent as a human being would be around the corner: they expected that it would take no more than a generation.
   This is the goal of strong AI. Strong or general AI is ca- pable of carrying out any cognitive tasks that humans can do, whereas weak or narrow AI can perform only in specific domains such as chess, the classification of images, and so on. As of today, we have not achieved general AI and, as we have seen in the previous chapters, it is doubtful whether we ever will. Although some researchers and companies are trying to develop it, especially those who believe in the computational theory of mind, general AI is not on the horizon. Hence the ethics and policy questions in the next chapter focus on weak or narrow AI, which we already have

today and which is likely to get more powerful and perva- sive in the near future.
AI can be defined both as a science and as a technology.
Its purpose can be to achieve a better scientific explana- tion of intelligence and the mentioned cognitive func- tions. It can help us to better understand human beings and other beings that have natural intelligence. In this way it is a science and a discipline that systematically studies the phenomenon of intelligence (Jansen et al. 2018) and sometimes the mind or brain. As such, AI is linked to other sciences such as cognitive science, psychology, data sci- ence (see below), and sometimes also neuroscience, which makes its own claims about understanding natural intel- ligence. But AI can also aim to develop technologies for various practical purposes, "to get useful things done," as Boden puts it: it can take the form of tools, designed by humans, that create the appearance of intelligence and in- telligent behavior for practical purposes. AIs can do this by analyzing the environment (in the form of data) and acting with a significant degree of autonomy. Sometimes scientific-theoretical interests and technological pur- poses meet, for example in computational neuroscience, which uses tools from computer science to understand the nervous system, or in particular projects such as the Eu- ropean "Human Brain Project,"1 which involves neurosci- ence but also robotics and AI; some of its projects combine

neuroscience and machine learning in so-called big data neuroscience (e.g., Vu et al. 2018).
   More generally, AI relies on and is linked to many disciplines, including mathematics (e.g., statistics), engi- neering, linguistics, cognitive science, computer science, psychology, and even philosophy. As we have seen, both philosophers and AI researchers are interested in under- standing the mind and phenomena such as intelligence, consciousness, perception, action, and creativity. AI has influenced philosophy and vice versa. Keith Frankish and William Ramsey acknowledge this link with philosophy, stress AI's cross-disciplinarity, and combine the scientific and technological aspects in their definition of AI as "a cross-disciplinary approach to understanding, modeling, and replicating intelligence and cognitive processes by in- voking various computational, mathematical, logical, me- chanical, and even biological principles and devices" (2014, 1). AI is thus both theoretical and pragmatic, both science and technology. This book focuses on AI as a technology, on the more pragmatic aspect: not only because within AI the focus has shifted in this direction, but especially be- cause it is mainly in this form that AI has ethical and so- cietal consequences-although scientific research is also not entirely ethically neutral.
   As a technology, AI can take various forms and is usu- ally part of larger technological systems: algorithms, ma- chines, robots, and so on. Thus, while AI may be about

"machines," this term refers not only to robots, let alone only to humanoid robots. AI can be embedded in many other kinds of technological systems and devices. AI sys- tems can take the form of software running on the web (e.g., chatbots, search engines, image analysis), but AI can also be embedded in hardware devices such as robots, cars, or "internet of things" applications.2 For the internet of things, the term "cyber-physical systems" is sometimes used: devices that work in, and interact with, the physical world. Robots are one kind of cyber-physical system, one that directly exerts influence on the world (Lin, Abney, and Bekey 2011).
   If AI is embedded in a robot, it is also sometimes called embodied AI. In exerting direct influence on the physical world, robotics is very dependent on physical components. But every AI, including software active on the web, "does" something and also has material aspects such as the computer on which it runs, the material as- pects of the network and infrastructure it relies on, and so on. This renders problematic the distinction between, on the one hand, "virtual" web-based and "software" ap- plications and, on the other hand, physical or "hardware" applications. AI software needs hardware and physical infrastructure to run, and cyber-physical systems are only "AI" if they are connected to the relevant software. Moreover, phenomenologically speaking, hardware and software sometimes merge in our experience and use of

devices: we do not experience an interactive humanoid robot powered by AI or an AI conversational device such as Alexa as either software or hardware, but as one tech- nological device (and sometimes as a quasi-person, e.g., Hello Barbie).
   AI is likely to have a significant influence on robotics, for example, through progress in natural language proc- essing and more human-like communication. Often these robots are called "social robots" because they are meant to participate in the daily social life of human beings, for example, as companions or assistants, by interacting with humans in a natural way. AI can thus foster further devel- opments in social robotics.
   However, regardless of the appearance and behavior of the system as a whole and its influence on its environ- ment, which is very important phenomenologically and ethically speaking, the basis of the "intelligence" of an AI is software: an algorithm or a combination of algorithms. An algorithm is a set and sequence of instructions, like a recipe, that tells the computer, smartphone, machine, ro- bot, or whatever it is embedded in, what to do. It leads to a particular output based on the information available (input). It is applied to solve a problem. To understand AI ethics, we need to understand how AI algorithms work and what they do. I will say more about this here and in the next chapter.

Different Approaches and Subfields

There are different kinds of AI. One may also say there are different approaches or research paradigms. As we saw in Dreyfus's criticism, AI was historically often symbolic AI. This was the dominant paradigm until the late 1980s. Symbolic AI relies on symbolic representations of higher cognitive tasks such as abstract reasoning and decision making. For example, it may decide based on a decision tree-a model of decisions and their possible conse- quences, often graphically represented as a flowchart. An algorithm that does this contains conditional statements: decision rules in the form of if ... (conditions) ... then ... (outcome). The process is a deterministic one. Drawing on a database that represents human expert knowledge, such an AI can reason through a lot of information and act as an expert system. It can make expert decisions or recommendations based on an extensive body of knowl- edge, which may be difficult or impossible for humans to read through. Expert systems are used, for example, in the medical sector for diagnosis and treatment plan- ning. For a long time they were the most successful AI software.
   Today symbolic AI is still useful, but new kinds of AI have also emerged, which may or may not be combined with symbolic AI, and which in contrast to expert systems are able to learn autonomously from data. This is done

by means of an entirely different approach. The research paradigm connectionism, which was developed in the 1980s as an alternative to what came to be called Good Old-Fashioned Artificial Intelligence (GOFAI), and the technology of neural networks is based on the idea that instead of representing higher cognitive functions, we need to build interconnected networks based on simple units. Proponents claim that this is similar to how the human brain works: cognition emerges from interactions between simple processing units, called "neurons" (which, however, are not like biological neurons). Many intercon- nected neurons are used. This approach and technology are often used by and for machine learning (see the next chapter), which then is called deep learning if the neural networks have several layers of neurons. Some systems are hybrid; for example, DeepMind's AlphaGo is a hybrid system. Deep learning has enabled progress in fields such as machine vision and natural language processing. Ma- chine learning that uses a neutral network can be a "black box" in the sense that while the programmers know the architecture of the network, it is not clear to others what precisely happens in its intermediate layers (between in- put and output) and thus how it comes to a decision. This contrasts with decision trees, which are transparent and interpretable, and can hence be checked and evaluated by human beings.

   Another important paradigm in AI is one that uses more embodied and situational approaches, focusing on motor tasks and interaction rather than so-called higher cognitive tasks. The robots built by AI researchers such as Rodney Brooks of MIT do not solve problems by us- ing symbolic representations but by interacting with the surrounding environment. For example, Brooks's hu- manoid robot Cog, developed in the 1990s, was built to learn by interacting with the world-as infants do. Fur- thermore, some people believe that mind can only arise from life; thus to create AI, we need to try to create ar- tificial life. And some engineers take a less metaphysical and more practical approach: they take biology as a model from which to develop practical technology applications. There are also evolutionary AIs that can evolve. Some pro- grams, using so-called genetic algorithms, can even change themselves.
   This diversity in approaches to and functions of AI also implies that today AI has various subfields: machine learning, computer vision, natural language processing, expert systems, evolutionary computation, and so on. Today the focus is often on machine learning, but this is only one area of AI, even if these other areas are often connected to machine learning. Recently much progress has been made in computer vision, natural language proc- essing, and the analysis of big data by means of machine learning. For example, machine learning can be used for

natural language processing based on analysis of speech and written sources such as texts found on the internet. This work created the conversational agents of today. An- other example is face recognition based on computer vi- sion and deep learning, which can be used, for example, for surveillance.


Applications and Impact

AI technology can be applied in various domains (it has various applications), ranging from industrial manufac- turing, agriculture, and transportation to health care, fi- nance, marketing, sex and entertainment, education, and social media. In retail and marketing, recommender or recommendation systems are used to influence purchase decisions and to offer targeted advertising. In social me- dia, AI can power bots: user accounts that appear to be real people but are in fact software. Such bots can post political content or chat with human users. In health care, AI is used to analyze data from millions of patients. Ex- pert systems are also still used in this area. In finance, AI is used to analyze big data sets for market analysis and automate trading. Robot companions often include some AI. Autopilots and self-driving cars use AI. Employers can use AI to monitor employees. Video games have characters

powered by AI. AIs can compose music or write news ar- ticles. It can also mimic voices of people and even create fake videos of speeches.
   Given its numerous applications, AI is likely to have a pervasive impact, now and in the near future. Consider predictive policing and speech recognition, which create new possibilities for security and surveillance, peer-to- peer transportation and self-driving cars that can trans- form entire cities, high-frequency algorithmic trading that already shapes financial markets, or diagnostic applica- tions in the medical sector that influence expert decision making. We should also not forget science as one of the major fields impacted by AI: by means of analysis of big data sets, AI can help scientists discover connections they would otherwise overlook. This is applicable to the natural sciences such as physics, but also to the social sciences and the humanities. AI is sure to affect the emerging field of digital humanities, for example, teaching us more about humans and about human societies.
   AI also has an impact on social relations and wider so- cietal, economic, and environmental influence (Jansen et al. 2018). AI is likely to shape human interactions and im- pact privacy. It is said to potentially increase bias and dis- crimination. It is predicted that it will lead to job losses and perhaps transform the entire economy. It could increase the gap between rich and poor and between powerful and

powerless, thus accelerating injustice and inequality. Mili- tary applications may change the way wars are conducted, for example, when automated lethal weapons are used. We should also bear in mind the environmental impact, which includes the increase of energy consumption and pollu- tion. Later I will discuss some of the ethical and societal implications in more detail, focusing on the problems and the risks of AI. But AI is also likely to have positive effects; for example, it can create new communities by means of social media, reduce repetitive and dangerous tasks by having robots take them over, improve supply chains, re- duce water use, and so on.
   With regard to impact-positive or negative-we should not only question the nature and extent of the impact; it is also important to ask who is affected and in what way by the impact. A particular impact may be more positive for some than for others. There are many stake- holders, ranging from workers, patients, and consumers, to governments, investors, and enterprises, all of whom may be affected differently. And these differences in gains and vulnerability to the impacts of AI arise not only within countries but also between countries and parts of the world. Will AI mainly benefit highly advanced and de- veloped countries? Could it also benefit less educated and low-income people, for instance? Who will have access to the technology and be able to reap its benefits? Who will







Who will have access to the technology and be able to reap its benefits? Who will be able to empower themselves
by using AI? Who
will be excluded from these rewards?

be able to empower themselves by using AI? Who will be excluded from these rewards?
   AI is not the only digital technology that raises such questions. Other digital information and communication technologies also have a huge impact on our lives and soci- eties. As we will see, some ethical problems with AI are not specific to AI. For example, there are parallels with other automation technologies. Consider industrial robots that are programmed and are not considered AI, but neverthe- less have societal consequences when they lead to unem- ployment. And some of AI's problems are related to the technologies AI is connected with, such as social media and the internet, which when combined with AI present us with new challenges. For instance, when social media platforms such as Facebook use AI to learn more about their users, this raises privacy concerns.
   This link to other technologies also means that of- ten AI is not visible. This is so in the first place because it has already become an ingrained part of our everyday life. AI is often used for new and spectacular applications such as AlphaGo. But we should not forget the AI that al- ready powers social media platforms, search engines, and other media and technologies that have become part of our everyday experience. AI is all over the place. The line between AI proper and other forms of technology can be blurred, rendering AI invisible: if AI systems are embed- ded within technology we tend not to notice them. And






We should not forget the AI that already powers social media platforms, search engines, and other media and technologies that have become part of our everyday lives. AI is all over the place.

if we do know that AI is involved, then it is difficult to say if it is AI that creates the problem or impact, or if it is the other technology connected to the AI. In a sense, there is no "AI" in itself: AI always relies on other tech- nologies and is embedded in broader scientific and tech- nological practices and procedures. While AI also raises its own specific ethical problems, any "AI ethics" thus needs to be connected to more general ethics of digital informa- tion and communication technologies, computer ethics, and so on.
   Another sense in which there is no such thing as AI in itself is that the technology is always also social and hu- man: AI is not only about technology but also about what humans do with it, how they use it, how they perceive and experience it, and how they embed it in wider social- technical environments. This is important for ethics- which is also about human decisions-and also means that it needs to include a historical and social-cultural per- spective. The current media hype about AI is not the first- ever hype about advanced technologies. Before AI, "robots" or "machines" were the key words. And other advanced technologies such as nuclear technology, nanotechnology, the internet, and biotechnology have also seen a lot of de- bate. It is worth keeping this in mind for our discussions of AI ethics, since perhaps we can learn something from these controversies. The use and development of technol- ogy takes place in a social context. As people in technology

assessment know, when technology is new it tends to be highly controversial, but once the technology becomes embedded in everyday life, the hype and the controversy deflate significantly. This is also likely to happen with AI. While such a prediction is not a good reason for abandon- ing the task of evaluating the ethical aspects and social consequences of AI, it helps us to see AI in context and hence to better understand it.


























Th E TEChNOlOGY	81



6


DON'T FORGET THE DATA
( SCIENCE)




Machine Learning

Since many ethical questions about AI concern technolo- gies that are entirely or partly based on machine learning and related data science, it is worth zooming in on this technology and science.
Machine learning refers to software that can "learn."
The term is controversial: some say that what it does is not true learning because it does not have real cognition; only humans can learn. In any case, modern machine learning bears "little or no similarity to what might plausibly be go- ing on in human heads" (Boden 2016, 46). Machine learn- ing is based on statistics; it is a statistical process. It can be used for various tasks, but the underlying task is often pattern recognition. Algorithms can identify patterns or rules in data and use those patterns or rules to explain the data and make predictions for future data.

   This is done autonomously in the sense that it hap- pens without direct instruction and rules given by the programmer. In contrast to expert systems, which rely on human domain experts who explain the rules to program- mers who then code these rules, the machine learning al- gorithm finds rules or patterns that the programmer has not specified. Only the objective or task is given. The soft- ware can adapt its behavior to better match the require- ments of the task. For example, machine learning can help distinguish spam from significant email by going through a large number of messages and learning what counts as spam. Another example: to build an algorithm that recog- nizes images of cats, the programmers do not give a set of rules to the computer that define what cats are, but rather have the algorithm make its own model of cat images. It will optimize for reaching the highest prediction accuracy on a set of images of cats and non-cats. It thus aims to learn what cat images are. Humans give feedback, but they do not feed it specific instructions or rules.
   Scientists used to create theories to explain data and make predictions; in machine learning, the computer cre- ates its own models that fit the data. The starting point is the data, not the theories. In this sense, data is no lon- ger "passive" but "active": it is "the data itself that defines what to do next" (Alpaydin 2016, 11). Researchers train the algorithm using existing data sets (e.g., old emails) and then the algorithm can predict results from new data

(e.g., new emails that come in) (CDT 2018). Identifying patterns in large amounts of information (big data) is sometimes also called "data mining," in analogy to extract- ing valuable minerals from the earth. However, the term is misleading because the goal is the extraction of patterns from the data, the analysis of data, not the extraction of data itself.
Machine learning can be supervised, which means
that the algorithm focuses on a particular variable that is designated as the target for prediction. For example, if the goal is to divide up people into categories (e.g., high or low security risk), the variables that predict these cat- egories are already known, and the algorithm then learns to predict category membership (high security risk/low security risk). The programmer trains the system by pro- viding examples and non-examples, for example, images of people that pose a high security risk and examples of people that don't. The goal is then that the system learns to predict who belongs to which category, who poses a high security risk and who not, based on new data. If the system is given enough examples, it will be able to generalize from these examples and know how to catego- rize new data, such as a new image of a passenger pass- ing through airport security. Unsupervised means that this kind of training is not done and that the categories are not known: algorithms make their own clusters. For example, the AI makes its own security categories based

 on variables it selects; the programmer does not provide them. The AI may find patterns that domain experts (here: security people) have not identified yet. The catego- ries created by the AI can look quite arbitrary to humans. Maybe they do not make sense. But statistically the cat- egories can be identified. Sometimes they do make sense, and then this method can give us new knowledge about the categories in the real world. Reinforcement learning, finally, requires an indication of whether the output is good or bad. It is analogous to reward and punishment. The program is not told which actions to take but "learns" through an iterative process which actions yield reward. To take the security example: the system receives feed- back from (data provided by) security people so it "knows" whether it has done a good job when it makes a particular prediction. If a person who was predicted to pose a low security risk did not cause any security problems, the system gets the feedback that its output was good and "learns" from it. Note that there is always a percentage of error: the system is never 100 percent accurate. Note also that the technical terms "supervised" and "unsupervised" have little to do with how much humans are involved in the use of the technology: while the algorithm is given some autonomy, in all three cases humans are involved in various ways.
   This is also true for the data aspect of AI, including so-called big data. Machine learning based on big data has

gained a lot of interest because of the availability of large amounts of data and an increase in (cheaper) computer power. Some researchers speak of a "dataquake" (Alpaydin 2016, x). We all produce data by means of our digital activi- ties, for example when we use social media or when we buy products online. These data are of interest to commercial actors but also to governments and scientists. It has never been easier for organizations to gather, store, and process data (Kelleher and Tierney 2018). This is not only because of machine learning: the wider digital environment and other digital technologies play a role here. Online applica- tions and social media make it easy to collect data from people. It is also less expensive to store data and comput- ers have gotten more powerful. All this has been impor- tant for the development of AI in general, but also for data science.


Data Science

Machine learning is thus connected to data science. Data science aims to extract meaningful and useful patterns from data sets, and today these data sets are large. Ma- chine learning is able to automatically analyze these large data sets. Machine learning and data science are based on statistics, which is all about going from particular observa- tions to general descriptions. Statisticians are interested










We all produce data by means of our digital activities, for example when we use social media or when we buy products online.

in finding correlations in the data through statistical analysis. Statistical modeling looks for mathematical rela- tionships between input and output. This is what machine learning algorithms help with.
   But data science involves more than just the analy- sis of data by means of machine learning. The data have to be collected and prepared before they can be analyzed, and afterward the results of the analysis have to be inter- preted. Data science includes challenges such as how to capture and clean data (for example, from social media and the web), how to get sufficient data, how to draw data sets together, how to restructure the data sets, how to select the relevant data sets, and what sorts of data are used. Humans thus still play an important role at all stages and with regard to all these aspects, including framing the problem, data capture, preparation of the data (the data set the algorithm trains on and the data set it will be ap- plied to), creating or selecting the learning algorithm, in- terpreting the results, and deciding what action to take (Kelleher and Tierney 2018).
   Scientific challenges present themselves at every stage of this process, and while the software may be easy to use, human expert knowledge is needed to deal with these challenges. Usually collaboration between humans is also needed, for example, between data scientists and en- gineers. Mistakes are always possible, and human choice, knowledge, and interpretation is crucial. Humans are

needed here to meaningfully interpret and to direct the technology toward finding different factors and relations. As Boden (2016) remarks, AI lacks our understanding of relevance. One should add that it also lacks understanding, experience, sensitivity, and wisdom. This is a good argu- ment why in theory and in principle humans need to be involved. But there is also an empirical argument for not leaving humans out of the picture: in practice, humans are involved. Without programmers and data scientists, the technology simply doesn't work. Moreover, human exper- tise and AI are often combined, for example, when a medi- cal doctor uses a cancer therapy recommendation from an AI but also draws on her own experience and intuition as an expert. If human intervention is left out, things can go wrong, make no sense, or simply get ridiculous.
   Take, for example, the following well-known problem from statistics, which therefore also affects the use of ma- chine learning AI: correlations are not necessarily causal relations. Tyler Vigen's book Spurious Correlations (2015) gives some good examples of this. In statistics, a spuri- ous correlation is one in which variables are not causally related but may appear to be; the correlations are due to the presence of a third, invisible factor. Examples include the correlation between the divorce rate in Maine and the per capita consumption of margarine, or the correlation between the per capita consumption of mozzarella cheese and civil engineering doctorates awarded.1 An AI may find

such correlations, but humans are needed to decide which correlations deserve further study in order to find causal relations.
   Moreover, already at the stage of gathering the data and designing or creating the data set, we are making choices about how to abstract from reality (Kelleher and Tierney 2018). Abstraction from reality is never neutral, and the abstraction itself is not reality; it is a representa- tion. This means we can discuss how good and appropriate the representation is, given a particular purpose. Compare this with a map: the map itself is not the territory, and humans have made choices in designing the map for a particular purpose (e.g., a map for car navigation versus a topographical map for hiking). In machine learning, ab- straction by means of statistical methods creates a model of reality; it is not reality. It also includes choices: choices concerning the algorithm itself which provides the statis- tical operation that takes us from the data to the pattern/ rule, but also choices involved in designing the data set the learning algorithm trains on. This choice aspect, and hence human aspect, of machine learning, means that we can and should ask critical questions about the choices be- ing made. For example, is the training data set representa- tive of the population? Are any biases embedded in the data? As we will see in the next chapter, these choices and issues are never mere technical questions but also have a crucial ethical component.

Applications

Machine learning and data science have numerous ap- plications, some of which I already mentioned under the more general heading of AI. These technologies can be used to recognize faces (and even recognize emotions based on analysis of the faces), make search sugges- tions, drive a car, make personality predictions, predict who is going to re-offend, or recommend music to listen to. In sales and marketing, they are used to recommend products and services. For example, when you buy some- thing on Amazon, the site will collect data about you and make recommendations on the basis of a statistical model drawing on data from all customers. Walmart has trialed face recognition technology to tackle theft in its stores; in the future it might use the same technology to determine if shoppers are happy or frustrated. The tech- nologies also have various applications in finance. Credit reference agency Experian works with machine learning AI to analyze data about transactions and court cases in order to recommend whether or not to lend to a mort- gage applicant. American Express uses machine learning to predict fraudulent transactions. In transportation, AI and big data are used to create autonomous cars. For ex- ample, BMW uses a kind of image recognition technology to analyze data that come in from the car's sensors and cameras. In health care, machine learning AI can help in

the diagnosis of cancer (e.g., in analyzing radiology scans to diagnose cancer) or the detection of infectious disease. For example, DeepMind's AI analyzed one million images from eye scans and patient data, training itself to diag- nose indications of degenerative eye conditions. IBM's Watson has moved beyond playing Jeopardy and is used to give recommendations for treating cancer. Wearable mo- bile sport and health devices also deliver data for machine learning applications. In the field of journalism, machine learning can write news stories. For example, in the UK the news agency Press Association has bots write local news pieces. AI also enters the home and private sphere, for example, in the form of robots that gather data and as- sistive interactive devices connected to natural language processing. Hello Barbie talks to children on the basis of natural language processing that analyzes recorded dia- logues. Everything the children say is recorded, stored, and analyzed at the servers of ToyTalk. Then a response is sent to the device: Hello Barbie answers on the basis of what it has "learned" about its user. Facebook uses deep learning technologies and neural networks to structure and analyze data from the nearly two billion users of the platform who produce unstructured data. This helps the company offer targeted advertisements. Instagram ana- lyzes the images of 800 million users in order to sell ad- vertising to companies. Using recommendation engines that analyze customer data, Netflix is transforming itself

from a distributor into a content creator: if you can pre- dict what people want to watch, you can produce it your- self and make money with it. Data science has even been used in cooking. For example, based on analysis of nearly 10,000 recipes, IBM's Chef Watson creates its own recipes that suggest new ingredient combinations.2 AI machine learning can also be used in education, recruiting, criminal justice, security (e.g., predictive policing), music retrieval, office work, agriculture, military weapons, and so on.
   Statistics used to be seen as a not very sexy field. To- day, as part of data science and in the form of AI work- ing with big data, it is hot. It is the new magic. It is the stuff the media like to talk about. And it is big business. Some speak of a new kind of gold rush; expectations are high. Furthermore, this kind of AI is not science fiction or speculation; as the examples show, so-called narrow or weak AI is already here and it is pervasive. When it comes to its potential impact, there is nothing narrow or weak about it. It is therefore urgent to analyze and discuss the many ethical issues that are raised by machine learning and other AI technologies and their applications. This is the topic of the next chapters.









Statistics used to be seen as a not very sexy field. Today, as part of data science and in the form of AI working with big data, it is hot. It is the new magic.



7



PRIVACY AND THE OTHER USUAL SUSPECTS 




Many ethical problems with AI are known from the area of the ethics of robotics and automation or, more gener- ally, from the area of the ethics of digital information and communication technologies. But this by itself does not render them any less important. Moreover, because of the technology and the way it is connected to other technolo- gies, these issues take on a new dimension and become even more urgent.


Privacy and Data Protection

Consider, for example, privacy and data protection. AI, and in particular machine learning applications working with big data, often involves the collection and use of per- sonal information. AI can also be used for surveillance, on

the street but also in the workplace and-through smart- phones and social media-everywhere. Often people do not even know that data are being gathered, or that the data they provided in one context are then used by third parties in another context. Big data also often means that data (sets) acquired by different organizations are being combined.
   An ethical use of AI requires that data are collected, processed, and shared in a way that respects the privacy of individuals and their right to know what happens to their data, to access their data, to object to the collection or processing of their data, and to know that their data are being collected and processed and (if applicable) that they are then subject to a decision made by an AI. Many of these issues also arise with other information and com- munication technologies and, as we will see, transparency is also an important requirement in those instances as well (see later in this chapter). And data protection issues also arise in research ethics, for example, the ethics of col- lecting data for social science research.
   If one considers the contexts in which AI is used today, however, these privacy and data protection issues become increasingly problematic. It is relatively easy to respect these values and rights when doing a survey as a social scientist: one can inform one's respondents and explicitly ask their consent, and it is relatively clear what will hap- pen to the data. But the environment in which AI and data

science are used today is usually very different. Consider social media: in spite of privacy information and applica- tions that ask users for consent, it is unclear for users what happens to their data or even which data are collected; and if they want to use the application and enjoy its benefits, they have to consent. Often, users also don't even know that AI is powering the application they use. And often data given in one context are then moved to another do- main and used for a different purpose (data repurposing), for example, when companies sell their data to other com- panies or move the data between different parts of the same company without users knowing this.


Manipulation, Exploitation, and Vulnerable Users

This last phenomenon also points to the risk of users being manipulated and exploited. AI is used to manipulate what we buy, which news we follow, whose opinions we trust, and so on. Researchers in critical theory have pointed to the capitalist context in which social media use happens. For example, it could be said that users of social media do free "digital labor" (Fuchs 2014) by producing data for companies. This form of exploitation can also involve AI. As social media users, we risk becoming the unpaid, ex- ploited workforce that produces data for the AI that then analyzes our data-and in the end for the companies that


priVaCY	99

use the data, which usually also include third parties. It also reminds us of Herbert Marcuse's warning in the 1960s that even so called "free," "non-totalitarian" societies have their own forms of domination, in particular the exploita- tion of consumers (Marcuse 1991). The danger here is that even in today's democracies, AI may lead to new forms of manipulation, surveillance, and totalitarianism, not nec- essarily in the form of authoritarian politics but in a more hidden and highly effective way: by changing the economy in a way that turns us all into smartphone cattle milked for our data. But AI can also be used to manipulate politics more directly, for example, by analyzing social media data to help political campaigns (as in the famous case of Cam- bridge Analytica, a company that used data from Facebook users without their consent for political purposes in the 2016 US presidential election), or by having bots posting political messages on social media based on analysis of people's data in terms of their political preferences in or- der to influence voting. Some also worry that AI, by taking over cognitive tasks from humans, infantilizes its users by "rendering them less capable of thinking for them- selves or deciding for themselves what to do" (Shanahan 2015, 170). Furthermore, the risk of exploitation lies not just on the user side: AI relies on hardware that is made somewhere by people, and this production may involve the exploitation of those people. Exploitation may also be involved in the training of algorithms and the production



AI may lead to new forms of manipulation, surveillance, and totalitarianism, not necessarily in the form of authoritarian politics but in a more hidden and highly effective way.

of data that are used for and by AI. AI may make life easier for its users, but not necessarily for those who mine the minerals, deal with the e-waste, and train the AI. For ex- ample, Amazon Echo's Alexa not only creates a user who does free labor, becomes a resource for data, and is sold as a product; a world of human labor is also hidden behind the scenes: miners, workers on ships, click workers who label data sets, all in the service of capital accumulation by very few people (Schwab 2018).
   Some users of AI are also more vulnerable than oth- ers. Theories of privacy and exploitation often assume that the user is an autonomous and relatively young and healthy adult human being with full mental capacities. The real world is one populated with children, elderly people, people who do not have "normal" or "full" mental capabilities, and so on. These vulnerable users are more at risk. Their privacy can often be easily violated or they can be easily manipulated; and AI provides new oppor- tunities for such violations and manipulations. Consider young children who chat with a doll that is connected to a technological system that includes AI: most likely, the child does not know that AI is being used or that data are collected, let alone what is being done with her or his per- sonal information. An AI-powered chatbot or doll not only can collect a lot of personal information about the child and its parents in this way; it can also manipulate the child by using language and voice interface. As AI becomes part

of the "internet of toys" (Druga and Williams 2017) and the internet of (other) things, this is an ethical and a po- litical problem. The ghost of totalitarianism returns once more: not in dystopian science fiction stories or seemingly outdated postwar nightmares, but in consumer technol- ogy that is already on the market.


Fake News, the Danger of Totalitarianism, and the Impact on Personal Relationships

AI may also be used to produce hate speech and false infor- mation, or to create bots that appear to be people but in fact are AI software. I already mentioned the chatbot Tay and the fake speech of Obama. This may lead to a world in which it is no longer clear what is true and what is false, where facts and fiction mix. Whether or not this should be called "post-truth" (McIntyre 2018), these applications of AI clearly contribute to the problem. Of course, false information and manipulation existed before AI. Film, for example, has always created illusions, and newspapers have spread propaganda. But with AI, combined with the possibilities and environment of the internet and digital social media, the problem seems to increase in intensity. There seem to be more opportunities for manipulation, putting critical thinking at risk. All this reminds us once more about the dangers of totalitarianism, which benefits

from confusion about the truth and in which fake news is created for ideological purposes.
   However, even in a libertarian utopia things may not look so bright. False information erodes trust and thereby damages the social fabric. Overuse of technology can lead to less contact, or at least less meaningful contact, be- tween people. Sherry Turkle (2011) has made this claim with regard to technologies such as computers and robots: we end up expecting more from technology but less from each other. This argument could also be made with regard to AI: the worry is that AI, in the form of social media or in the form of digital "companions," gives us the illusion of companionship but unsettles true relationships with friends, lovers, and families. Although this concern was already there before AI and tends to surge with every new medium (reading the newspaper or watching TV instead of talking), the argument could be that now, with AI, the technology is much better in creating the illusion of com- panionship and that this increases the risk of loneliness or deteriorating personal relationships.


Safety and Security

There are also more visible dangers. AIs, especially when embedded in hardware systems that operate in the phys- ical world, also need to be safe. Consider, for example,

industrial robots: they are supposed not to harm workers. Yet sometimes accidents happen in factories. Robots can kill, even if this is relatively rare. However, with AI robots, the safety problem becomes more challenging: such ro- bots may be able to work more closely with humans, and may be able to "intelligently" avoid harming humans. But what exactly does this mean? Should they move more slowly when near a human, which slows down the pro- cess, or is it OK to move at high speed in order to do the work efficiently? There is always the risk that something could go wrong. Should the ethics of safety be a matter of trade-offs? AI robots in a home environment or in pub- lic spaces also cause safety issues. For example, should a robot always avoid bumping into humans or is it OK if it sometimes obstructs a person in order to reach its goal? These are not mere technical issues but have an ethical component: it is an issue of human lives and values such as freedom and efficiency. They also raise responsibility problems (more on this below).
   Another problem that was already there before AI entered the stage, but which deserves renewed attention, is security. In a networked world, every electronic device or software can be hacked, invaded, and manipulated by people with malicious intentions. We all know about computer viruses, for example, which can mess up your computer. But when equipped with AI, our devices and software can do more, and when they gain more agency

and this has real-world physical consequences, the secu- rity problem becomes much larger. For example, if your AI- powered self-driving car is hacked, you have more than just a "computer problem" or "software problem"; you may die. And if the software of a critical infrastructure (internet, water, energy, etc.) or a military device with lethal capac- ity is hacked, an entire society will likely be disrupted and many people will be harmed. In military applications, the use of autonomous lethal weapons poses an obvious se- curity risk, especially of course to those who are targeted by them (usually not people in the West) but also to those who deploy them: they can always be hacked and turned against you. Moreover, an arms race involving these weap- ons could lead to a new world war. And one does not need to look far into the future: if today (non-AI) drones can already handicap a big London airport, it is not difficult to imagine how vulnerable our daily infrastructures are and how easily the maleficent use or hacking of AI could cause massive disruptions and destructions. Note also that, in contrast to, say, nuclear technology, using exist- ing AI technology does not require expensive equipment or a long training; the hurdle for using AI for malicious purposes is thus rather low.
More mundane security problems with cars and infra-
structures such as airports also remind us that while some people are more vulnerable than others, we are all vulner- able in the light of technologies such as AI because, as their









In a networked world, every electronic device or software
can be hacked, invaded, and manipulated by people with malicious intentions.

agency increases and we delegate more tasks to them, we all become more dependent on them. Things can always go wrong. The new technological vulnerabilities, then, are never merely technological; they also become our human, existential vulnerabilities (Coeckelbergh 2013). The ethi- cal problems discussed here can thus be seen as human vulnerabilities: technological vulnerabilities ultimately transform our existence as humans. To the extent that we become dependent on AI, AI is more than a tool we use; it becomes part of how we are, and how we are at risk, in the world.
Increased agency of AI, especially when it replaces
human agency, also raises another ever more urgent ethi- cal problem: responsibility. This is the topic of the next chapter.

8



A-RESPONSIBLE MACHINES AND UNEXPLAINABLE DECISIONS 



How Can and Should We Attribute Moral Responsibility?

When AI is used to make decisions for us and to do things for us, we encounter a problem that is shared with all automation technologies but which becomes even more important when AI enables us to delegate far more to ma- chines than we used to: responsibility attribution.1 If AI is given more agency and takes over what humans used to do, how do we then attribute moral responsibility? Who is responsible for the harms and benefits of the technol- ogy when humans delegate agency and decisions to AI? To put it in terms of risk: who is responsible when something goes wrong?
   When humans are doing things and making decisions, we normally connect agency with moral responsibility. You are responsible for what you do and for what you decide.










If AI is given more agency and takes over what humans used
to do, how do we then attribute moral responsibility?

If you have effect on the world and on others, you are re- sponsible for those consequences. According to Aristotle this is the first condition for moral responsibility, the so-called control condition: in the Nicomachean Ethics he argues that the action must have its origin in the agent. This view also has a normative side: if you have agency and if you can decide, you should take responsibility for your actions. What we want to avoid, morally speaking, is someone who has agency and power but no responsibility. Aristotle also added another condition for moral respon- sibility: you are responsible if you know what you're do- ing. This is an epistemic condition: you need to be aware of what you are doing and know what the consequences could be. What we need to avoid here is someone who does things without knowing what she is doing, potentially re- sulting in harmful consequences.
   Now let's see how these conditions fare when we del- egate decisions and actions to AI. The first problem is that an AI can take actions and make decisions that have ethi- cal consequences, but is not aware of what it does and not capable of moral thought and hence cannot be held mor- ally responsible for what it does. Machines can be agents but not moral agents since they lack consciousness, free will, emotions, the capability to form intentions, and the like. For example, on an Aristotelian view, only humans can perform voluntary actions and deliberate about their actions. If this is true, the only solution is to make humans responsible for what the machine does. Humans then

delegate agency to the machine, but retain the responsi- bility. In our legal systems, we already do this: we do not hold dogs or small children responsible for their actions, but put the legal responsibility on the shoulders of their caretakers. And in an organization, we may delegate a par- ticular task to an individual but ascribe the responsibility to the manager in charge of the overall project-although in this case there is still some responsibility on the part of the delegate.2 So why not let the machine perform the actions and keep the responsibility on the side of the hu- man? This seems the best way forward, since algorithms and machines are a-responsible.
   However, this solution faces several problems in the case of AI. First, an AI system may make its decisions and actions very quickly, for example, in high-frequency trad- ing or in a self-driving car, which gives the human too little time to make the final decision or to intervene. How can humans take responsibility for such actions and deci- sions? Second, AIs have histories. When the AI does things in a particular context of application, it may no longer be clear who created it, who used it first, and how the respon- sibility should be distributed among these different par- ties involved. For example, an AI algorithm made in the context of a scientific project at university may find its first application in the lab at university, then in the health care sector, and later in a military context. Who is respon- sible? It may be difficult to track all the humans involved

in the history of the particular AI and indeed in the causal history that led to a particular ethically problematic out- come. We do not always know all the people involved at the moment when a responsibility problem arises. An AI algorithm often has a long history, involving many people. This leads us to a typical problem with responsibility at- tribution for technological actions: there are usually many hands and, I will add, many things.
There are many hands in the sense that many people
are involved in technological action. In the case of AI, it be- gins with the programmer, but we also have the end user and others. Consider the example of the self-driving car: there is the programmer, the user of the car, the owners of the car company, the other users of the road, and so on. In March 2018 an Uber self-driving car caused an accident in Arizona that resulted in the death of a pedestrian. Who is responsible for this tragic outcome? It could be those who programmed the car, those at the car company responsible for the product development, Uber, the car user, the pe- destrian, the regulator (e.g., the State of Arizona), and so on. It is not clear who is responsible. It may be that respon- sibility cannot and should not be attributed to one person; more than one person may be responsible. But then it is not clear how to distribute the responsibility. Some people may be more responsible than others.
There are also many things, in the sense that the
technological system consists of many interconnected

elements; there are usually many components of the sys- tem involved. There is the AI algorithm, but this algorithm interacts with sensors, uses all kinds of data, and interacts with all kinds of hardware and software. All these things have histories and are connected to the people who pro- grammed or produced them. When something goes wrong, it is not always clear if it is "the AI" that caused the problem or some other component of the system-or even where the AI ends and the rest of the technology begins. This also makes it difficult to ascribe and distribute responsi- bility. Consider also machine learning and data science: as we have seen, there is not only the algorithm, but also a process that includes various stages such as the collec- tion and treatment of data, the training of the algorithm, and so on-all involving various technical elements and requiring human decisions. Again, there is a causal his- tory that involves many humans and parts; this renders responsibility attribution difficult.
   To try to deal with these issues, one could learn from legal systems or look at how insurance works; I will say something about legal notions in the policy chapters. But behind these legal and insurance systems loom more gen- eral questions about the agency of AI and responsibility for AI: how dependent do we want to be on automation technology, can we take responsibility for something the AI does, and how can we attribute and distribute respon- sibilities? For example, the legal notion of negligence is

about whether one exercised a duty of care. But what does this duty mean in the case of AI, given that it is so hard to predict all potential ethically relevant consequences?
   This brings us to the next issue. Even if the control problem could be solved, there is also the second condition for responsibility, which concerns a problem of knowledge. To be responsible, you need to know what you are doing and bringing about, and, in retrospect, know what you have done. Moreover, this issue has a relational aspect: in the case of humans, we expect that someone can explain what she has done or decided. Responsibility then means answerability and explainability. If something goes wrong, we want an answer and an explanation. For example, we ask a judge to explain her decision, or we ask a criminal why she did what she did. These conditions become very problematic in the case of an AI. First, in principle the AI of today does not "know" what it is doing, in the sense that it is not conscious and hence not aware of what it is doing and what it is bringing about. It can log and record what it does, but it does not "know what it is doing" in the same way as humans, who as conscious beings are aware of what they do and can (following Aristotle again) deliberate and reflect on what their actions and the consequences of those actions. When these conditions are not met in the case of humans, in the case of very young children, for ex- ample, we don't hold them responsible. Usually we also do not hold animals responsible.3 If AI does not meet these

conditions, then, we cannot hold an AI responsible. The solution is again to hold humans responsible for what the AI does, assuming that they know what the AI is doing and what they are doing with the AI, and-keeping in mind the relational aspect-that they are answerable for its ac- tions and can explain what the AI did.
   However, whether this assumption holds is not as straightforward as it might seem at first sight. Usually programmers and users know what they want to do with the AI, or more precisely: they know what they want the AI to do for them. They know the goal, the end; this is why they delegate the task to the AI. They might also know how the technology works in general. But, as we will see, they do not always know exactly what the AI is doing (at any moment in time) and cannot always explain what it did or how it came to its decision.


Transparency and Explainability

Here we encounter the problem of transparency and ex- plainability. With some AI systems, the way the AI comes to its decision is clear. If the AI uses a decision tree, for ex- ample, the way it reaches its decision is transparent. It has been programmed in a way that determines the decision, given a particular input. Humans can thus explain how the AI came to its decision and the AI can be "asked" to

"explain" its decision. Humans can then take responsibility for the decision or, perhaps more accurately, can make a decision based on the decision recommendation made by the AI. However, with some other AI systems, notably AIs that use machine learning and in particular deep learning that uses neural networks, this explanation and this kind of decision making is no longer possible. It is no longer transparent how the AI comes to its decision, and humans cannot fully explain the decision. They know how their system works, in general, but cannot explain a particular decision. Think about chess with deep learning: the pro- grammers know how the AI works, but the precise way the machine arrives at a particular move (i.e., what happens in the layers of the neutral net) is not transparent and cannot be explained. This is a problem for responsibility, since the humans who create or use the AI cannot explain a particular decision and hence fail to know what the AI is doing and cannot answer for its actions. In one sense, the humans know what the AI is doing (for example, they know the code of the AI and know how it works in general), but in another sense they don't know (they cannot explain a particular decision), with the result that people affected by the AI cannot be given precise information about what made the machine arrive at its prediction. Thus, while all automation technology raises problems of responsibility, here we encounter a problem specific to some kinds of AI: the so-called black box problem.

   Furthermore, even the assumption that in such cases humans have knowledge about the AI in general and of the code is not always true. Probably the initial program- mers know the code and how everything works (or at least know the part they programmed), but that does not mean that the subsequent programmers and users who change or use the algorithm for specific applications fully know what the AI is doing. For example, someone using a trading algorithm may not fully understand AI, or users of social media may not even know that AI is being used, let alone understand it. And from their side, the (initial) programmers may not know the precise future use of the algorithm they develop or the different domains of applica- tion in which the algorithm may be used, let alone all the unintended effects of the future use of their algorithm. So even regardless of the particular problem with (deep) ma- chine learning, there is a knowledge problem with AI to the extent that many people who use it don't know what they are doing, since they don't know what the AI is do- ing, what its effects are, or even that it is used. This, too, is a problem for responsibility and hence a serious ethical problem.
   Sometimes these problems are put in terms of trust: a lack of transparency leads to less trust in the technology and in the people who use the technology. Some research- ers therefore ask how we can increase trust in AI, and iden- tify transparency and explainability as one of the factors

that can increase trust, as well as, for example, avoiding bias (Winikoff 2018) or "Terminator" images of AI (Siau and Wang 2018). And as we will see in the next chapter, AI policy also often aims at building trust. However, terms such as "trustworthy" AI are controversial: should we re- serve the term "trust" for talking about human-human relations, or is it fine to use it for machines as well? AI re- searcher Joanna Bryson (2018) has argued that AI is not a thing to be trusted but a set of software development tech- niques; she thinks that the term "trust" should be reserved for people and their social institutions. Moreover, the is- sue of transparency and explainability makes us wonder again about what kind of society we want. Here the danger is not only manipulation and domination by capitalists or technocratic elites, creating a highly divided society. The further and perhaps deeper danger that looms here is a high-tech society in which even those elites no longer know what they are doing, and in which nobody is answer- able for what is happening.
   As we will see, policymakers sometimes propose "ex- plainable AI" and a "right to explanation." Yet it is ques- tionable if it is possible to always have transparent AI. It seems easy to reach with classic systems. But if with con- temporary machine learning applications it seems im- possible in principle to explain every step of the decision process and explain decisions relating to specific individu- als, we have a problem. Is it possible to "open the black

box"? This would probably be a good thing, not only for ethics but also for improving the system (i.e., the model) and learning from it. For example, if the system is more explainable, and if the AI uses what we consider to be inadequate features, then humans can spot these issues and help to eliminate the spurious correlations. And if an AI identifies new strategies of playing a game and makes this more transparent to humans, then humans can learn from the machine in order to get better at playing the game. This is useful not only for gaming, but also in do- mains such as health care, criminal justice, and science. Some researchers therefore try to develop techniques for opening the black box (Samek, Wiegand, and Müller 2017). But if this is not yet possible or only possible to a limited extent, how do we proceed? Is the ethical issue then about trade-offs between performance and explainability (Seseri 2018)? If the cost of a well-performing system is a lack of transparency, should we still use it, or not? Or should we try to avoid this problem and find different techni- cal solutions, so that even very advanced AIs are able to explain themselves to humans? Can we train machines to do that?
Moreover, even if transparency were desirable and
possible, it may be difficult to realize it in practice. For ex- ample, private companies may not be willing to reveal their algorithms because they want to protect their commer- cial interests. Intellectual property legislation protecting

those interests may also hinder this. And, as we will see in later chapters, if AI is in the hands of powerful corpo- rations, this raises the question of who makes and who should make the AI rules.
   Note, however, that ethically speaking transpar- ency and explainability are not necessarily and certainly not only about disclosing the software code. The issue is mainly about explaining decisions to people. It is not pri- marily about explaining "how it works" but about how I, as a human being who is expected to be accountable and act responsibly, can explain my decision. How the AI works and came to its recommendation can be part of that expla- nation. Furthermore, disclosing a code does not by itself necessarily give knowledge about how the AI works. This depends on people's educational background and skills. If they lack the relevant technical expertise, a different kind of explanation is needed. This not only reminds us of the problem of education but also leads to the question of what kind of explanation is needed and, ultimately, what an explanation is.
   Thus the issue about transparency and explainability also raises interesting philosophical and scientific ques- tions, such as questions about the nature of explanation (Weld and Bansal 2018). What constitutes a good expla- nation? What is the difference between explanations and reasons, and can machines provide any of these? And how do humans in fact make decisions? How do they explain

their decisions? There is research on this in cognitive psy- chology and cognitive science, which could be used for thinking about explainable AI. For example, people gener- ally do not provide complete causal chains; instead, they select explanations, and they respond to what they believe are the explainee's beliefs: explanations are social (Miller 2018). And perhaps we also expect different explanations from machines than from humans, who, for example, of- ten make excuses for their actions because of emotions. But if we do so, does this mean that we hold machine de- cision making to a higher standard than human decision making (Dignum et al. 2018), and if so, should we do so? Some researchers speak of reasoning rather than expla- nation. Winikoff (2018) even demands "value-based rea- soning" from AIs and other autonomous systems, which should be able to represent human values and reason us- ing human values. But can a machine "reason," and in what sense can a technological system "use" or "represent" val- ues at all? What kind of knowledge does it have? Does it have knowledge at all? Does it have understanding at all? And, as Boddington (2017) asks, can humans necessarily fully articulate their most fundamental values?
   Such problems are interesting for philosophers, but they also have direct ethical relevance and are very real and practical. As Castelvecchi (2016) puts it: opening the black box is a problem in the real world. For example, banks should explain why they deny a loan; judges should

explain why they send someone (back) to prison. Explain- ing decisions is not only a part of what humans naturally do when they communicate (Goebel et al. 2018), it is also a moral requirement. Explainability is a necessary condi- tion for responsible and accountable behavior and deci- sions. It seems required for any society that wants to take human beings seriously as autonomous and social individ- uals who try to act and decide responsibly and who rightly demand reasons and explanations for decisions that affect them. Whether or not AI can directly provide those rea- sons and explanations, humans should be able to answer the question: "Why?" The challenge for AI researchers is to ensure that if an AI is used for decision making at all, the technology is built in such a way that humans will be able as much as possible to answer that question.



9



BIAS AND THE MEANING OF LIFE 




Bias

Another problem that is both ethical and societal, and also specific to data science-based AI as opposed to other au- tomation technologies, is the issue of bias. When an AI makes-or more precisely recommends-decisions, bias may arise: the decisions may be unjust or unfair to par- ticular individuals or groups. Although bias may also arise with classic AI-say, an expert system using a decision tree or database that contains bias-the issue of bias is often connected to machine learning applications. And while problems of bias and discrimination have always been present in society, the worry is that AI may perpetu- ate these problems and enlarge their impact.
   Bias is often unintentional: the developers, users, and other people involved such as the management of the









While problems of bias and discrimination have always been present in society, the worry is that AI may perpetuate these problems and enlarge their impact.

company often do not foresee the discriminatory effects against certain groups or individuals. This can be because they don't understand the AI system well enough, are not sufficiently aware of the problem of bias or indeed of their own biases, or more generally do not sufficiently imagine and reflect on the potential unintended consequences of the technology and are out of touch with some relevant stakeholders. This is problematic since biased decisions can have severe consequences, for example, in terms of ac- cess to resources and freedoms (CDT 2018): individuals might not get a job, might not get credit, might end up in prison, or might even experience violence against them. And not only individuals may suffer; entire communities might be affected by the biased decisions, for example, when an entire area of the city or all people with a par- ticular ethnic background are profiled by the AI as posing a high security risk.
   Consider again the example of the COMPAS algo- rithm mentioned in the first chapter, which predicts if de- fendants are likely to re-offend and was used by judges in Florida in sentencing decisions, for example, about when someone will be granted parole. According to a study by online newsroom ProPublica, the algorithm's false posi- tives (defendants predicted to re-offend but who actu- ally did not) were disproportionately black, and the false negatives (defendants predicted not to re-offend but who actually re-offended) were disproportionately white (Fry

2018). Critics thus argued that there was a bias against black defendants. Another example is PredPol, a so-called predictive policing tool that has been used in the United States to predict the probability of crime in particular ar- eas of cities and to recommend the allocation of police re- sources (e.g., where police officers should patrol) on the basis of these predictions. Here the worry was that the system would be biased against poor and colored neigh- borhoods or that disproportionate police surveillance would break down trust between people in those areas, turning the prediction of crime into a self-fulfilling proph- ecy (Kelleher and Tierney 2018). But bias is not only about criminal justice or policing; it can also mean, for example, that users of internet services are discriminated against if the AI profiles them unfavorably.
   Bias may arise in a number of ways at all stages of design, testing, and application. To focus on design: bias can arise in the selection of the training data set; in the training data set itself, which may be unrepresentative or incomplete; in the algorithm; in the data set the algorithm is given once it is trained; in decisions based on spurious correlations (see the previous chapter); in the group that creates the algorithm; and in wider society. For example, a data set may not be representative of the population (e.g., it may be based on American white males) but still used to predict for the entire population (males and females of various ethnic backgrounds). Bias can also concern

differences between countries. Many deep neural net- works for image recognition are trained on the annotated data set ImageNet, which contains a disproportionate amount of data from the United States, whereas countries such as China and India, which represent a much larger part of the world's population, contribute only a small fraction (Zou and Schiebinger 2018). This can lead to cul- tural bias embedded in the data set. More generally, data sets may be incomplete or of poor quality, which can lead to bias. The prediction might also be based on too little data, for example, in the case of murder prediction: there are not that many murders, which renders generalization problematic. Another example: some researchers worry about the lack diversity in the AI developers and data science teams: most computer scientists and engineers are white men from Western countries age 20 to 40, and their personal experience, opinions, and indeed prejudices might feed into the process, potentially negatively affect- ing people who do not fit this description, like women, dis- abled people, elderly people, people of color, and people from developing countries.
   Data might also be biased against particular groups because the bias is embedded in the specific practice or in the wider society. Consider claims that medicine uses mainly data from male patients and is therefore biased, or biases against people of color that are prevalent in wider society. If an algorithm feeds on such data, the results will

also be biased. Bias in, bias out, as a 2016 Nature editorial put it. It has also been shown that machine learning can acquire bias from feeding on textual data from the World Wide Web since these language data reflect everyday hu- man culture, including its biases (Caliskan, Bryson, and Narayanan 2017). Language corpora, for example, them- selves contain gender biases. The concern is then that AI may perpetuate these biases, further disadvantaging his- torically marginalized groups. Bias may also arise if there is a correlation but no causation. To take again a criminal justice example: an algorithm may infer that if one of a de- fendant's parents went to prison, that defendant is more likely to be sent to prison. Even if this correlation may ex- ist and even if the inference is predictive, it seems unfair that such a defendant would get a harsher sentence since there is no causal relation (House of Commons 2018). Fi- nally, bias may also arise because human decision makers trust more in the accuracy of the recommendations of the algorithms than they should (CDT 2018) and disregard other information or do not sufficiently exercise their own judgment. For example, a judge may rely entirely on the algorithm and not take into account other elements. As al- ways with AI and other automation technologies, human decisions and human interpretation play an important role, and there is always a risk of relying too much on the technology.

   However, it is not clear if bias is avoidable at all or even if it should be avoided, and, if so, at what cost it should be avoided. For example, if changing the machine learn- ing algorithm in order to decrease the risk of bias makes its predictions less accurate, should we change it? There may be a trade-off between effectiveness of the algorithm and the countering of bias. There is also the problem that if certain characteristics like race are ignored or removed, machine learning systems may identify so-called proxies for such characteristics, which also leads to bias. For exam- ple, in the case of race it could be that other variables that are correlated with race such as, for example, postcode, are selected by the algorithm. And is a perfectly unbiased algorithm possible? There is no consensus among phi- losophers or indeed in society about what perfect justice or fairness is. Furthermore, as remarked in the previous chapter, the data sets used by the algorithms are abstrac- tions from reality and the result of human choices, and are hence never neutral (Kelleher and Tierney 2018). Bias per- meates our world and societies; thus, although a lot can and should be done to minimize bias, AI models will never be entirely free from bias (Digital Europe 2018).
   Moreover, surely algorithms used for decision mak- ing are always biased in the sense of being discriminatory: they are meant to discriminate between various possi- bilities. For example, in a recruiting process the screening of CVs is meant to be biased and discriminatory toward

those characteristics of the candidate that best fit the po- sition. The ethical and political question is if a particular discrimination is unjust and unfair. But, again: views on justice and fairness differ. This renders the bias issue not only technical but also connects it to political discussions about justice and fairness. For example, it is controver- sial whether positive discrimination or affirmative ac- tion, which tries to undo bias by creating a positive bias toward the disadvantaged individuals or groups, is just. Should justice be blind and impartial-and hence should algorithms be blind to race, for example-or does justice mean creating an advantage for those who are already disadvantaged, thus amounting to a (corrective kind of) partiality and discrimination? And should policy in a dem- ocratic context prioritize the protection of the interests of the majority or focus on promoting the interests of a minority, albeit a historically or currently disadvantaged minority?
   This brings us to the question about measures. Even if we agree there is bias, there are various ways of dealing with the problem. This includes technological ways but also societal and political measures and education. Which measures we should take is controversial, and depends again on our notion of justice and fairness. For example, the question regarding affirmative action raises the more general issue of whether we should accept the world as it is or actively shape the future world in a way that avoids










Should justice be blind and impartial- or does justice mean creating an advantage for those who are
already disadvantaged?

perpetuating injustices of the past. Some argue that one should use a data set that mirrors the real world. The data may represent prejudices in society and the algorithm may model existing biases people have, but this is not a prob- lem developers should be worried about. Others argue that such a data set exists only because centuries of bias, that this bias and discrimination is unjust and unfair, and that therefore one should change that data set or the algorithm in order to promote affirmative action. For ex- ample, in response to results from the Google search al- gorithm that seem biased against female math professors, one could say that this simply mirrors the world as it is (and that this mirroring is exactly what the search algo- rithm should do); or we could have the algorithm priori- tize images of female math professors in order to change perception and perhaps change the world (Fry 2018). One could also try to establish developer teams that are more diverse in terms of background, opinion, and experience, and that better represent the groups potentially affected by the algorithm (House of Commons 2018).
   The mirror view does not work if the training data do not mirror the world as it is and contain old data that do not reflect the current situation. Decisions based on these data then help to perpetuate the discriminatory past rather than prepare for the future. Moreover, another objection against the mirror view is that even if a model mirrors the world as it is, this can lead to discriminatory

action and other harms with regard to specific individu- als and groups. For example, based on profiles made by an AI, credit firms may deny loans to applicants on the basis of where they live, or online sites may charge some customers more than others based on customer profiles created by AIs. Profiles may also follow individuals across domains (Kelleher and Tierney 2018). And a seemingly simple autocomplete function may falsely link your name to crime (which may lead to terrible consequences), even if the search AI behind it correctly mirrors the world in the sense that most people intend to search for the name of the criminal instead of your name. Another, perhaps less obvious example of bias: a music retrieval system used by services such as Spotify, which makes recommendations on the basis of current behavior (the music tracks people click on), may discriminate against music and musicians that are less mainstream. Even if it mirrors the world as it is, this leads to a situation in which some musicians can- not make a living from their music and to communities that feel unrecognized and not respected.
    Again, while these are clear cases of problematic dis- crimination, one should always ask: is discrimination in a particular case just or not? And if it is deemed unjust, what can be done about it and by whom? For example, what can computer scientists do about it? Should they make the training data sets more diverse, perhaps even creating "idealized" data and data sets as Eric Horvitz (Microsoft)

has suggested (Surur 2017)? Or should the data sets mir- ror the world? Should developers build in positive discrim- ination in their algorithms, or create "blind" algorithms? How to deal with bias in AI is not a merely technical ques- tion; it is a political and philosophical one. The question is what kind of society and world we want, whether we should try to change it, and if so, what ways of changing it are acceptable and fair. It is also a question that is as much about humans as it is about machines: do we think human decision making is just and fair, and if not, what is the role of AI? Perhaps AI could teach us something about humans and human societies by revealing our biases. And discussing AI ethics may reveal social and institutional power imbalances.
   Discussions about AI ethics thus reach deep into sen- sitive societal and political issues, which are related to nor- mative philosophical questions about, for example, justice and fairness and into philosophical and scientific ques- tions about humans and human societies. One of these issues is the future of work.


The Future of Work and the Meaning of Life

Automation powered by AI is predicted to radically trans- form our economies and societies, raising questions about

not only the future and meaning of work but also the fu- ture and meaning of human life.
   First there is the worry that AI will destroy jobs, per- haps causing massive unemployment. There is also the question what kind of jobs will be taken over by AI: only so-called blue-collar jobs, or also others? A famous report by Benedikt Frey and Michael Osborne (2013) predicts that 47 percent of all jobs in the United States could be automated. Other reports have less dramatic figures, but most predict that job loss will be significant. Many authors agree the economy has been and will continue to be greatly transformed (Brynjolfsson and McAffee 2014), including substantial effects on employment now and in the future. And job loss due to AI is predicted to hit all kinds of work- ers, not only blue-collar ones, since AI is increasingly able to do complex cognitive tasks. If this is true, how can we prepare the new generations for this future? What should they learn? What should they do? And what if AI benefits some people more than others?
   With this last question, we touch again on issues of justice and fairness, which have occupied thinkers in political philosophy for ages. If AI would create a wider gap between rich and poor, for example, is this just? And if not, what can be done about it? One could also frame the problem in terms of inequality (will AI increase in- equality in societies and in the world?) or in terms of vul- nerability: will the employed, wealthy, and educated in






Automation powered by AI is predicted to radically transform our
economies and societies, raising questions about not only the future and meaning of work but also the future and meaning of human life.

technologically advanced countries enjoy the benefits of AI while the unemployed, poor, and less educated in devel- oping countries will be far more vulnerable to the negative impacts (Jansen et al. 2018)? And to take up a more recent ethical and political concern: What about environmental justice? What is the impact of AI on the environment and our relation to the environment? What does "sustainable AI" mean? There is also the question whether AI ethics and politics should be only about the values and interests of humans or not. (See chapter 12.)
   Another rather existential question concerns the meaning of work and human lives. The worry about job destruction assumes that work is the only value and the only source of income and meaning. But if jobs are the only thing of value, then we should probably create more men- tal illnesses, smoke more, and get more obese, since these problems tend to create jobs.1 We don't want that. Clearly, we think that other values are more important than job creation in itself. And why rely on jobs for income and meaning? We could organize our societies and economies in a different way. We could decouple work and income, or rather what we consider "work" and income. Today many people do unpaid work, for example in the household and in care for children and elderly. Why is this not "work"? Why would it be less meaningful to do that kind of work? And why do we not make it a source of income? Moreover, some people think that automation could give us more of

what is now called leisure. Maybe we can do more plea- surable and creative things, not necessarily in the form of a job. We can, in other words, question the idea that a meaningful life is only a life spent doing paid work that is prestructured by others or that takes place within a so- called self-employed frame. Maybe we can enforce mea- sures such as "basic income" in order to allow everyone to do what they think is meaningful. Thus, in response to the future of work problem, we can think about what makes work meaningful, what kind of work humans should (be allowed to) do, and how we can reorganize our societies and economies in such a way that income is not limited to jobs and employment.
   That being said, so far utopian ideas about leisure so- cieties and other postindustrial paradises have not been realized. We have already had several waves of automation from the nineteenth century until now, but to what extent have the machines liberated and emancipated us? Perhaps they have taken over some of the dirty and dangerous work, but they have also been used for exploitation and have not radically changed the hierarchical structure of so- ciety. Some have benefited enormously from automation, whereas others have not. Perhaps fantasies about not hav- ing jobs are a luxury reserved only for those on the win- ning side. And have the machines freed us up to have more meaningful lives? Or do they threaten the very possibility of such lives? This is a long-standing discussion and there

are no easy answers to these questions, but the concerns raised are good reasons to at least be skeptical about the brave new world painted by the AI prophets.
   Furthermore, perhaps work is not necessarily toil that needs to be avoided or exploitation that needs to be re- sisted; a different view is that work has value, that it gives the worker purpose and meaning, and that it has various benefits such as social connections with others, belonging to something larger, health, and opportunities to exercise responsibility (Boddington 2016). If this is the case, then perhaps we should reserve work for humans-or at least some kinds of work, meaningful work that offers oppor- tunities for these goods to be realized. Or at least some tasks. AI need not take over entire jobs, but it could take over some less meaningful tasks. We can collaborate with AIs. For example, we could choose not to delegate creative work to AIs (something Bostrom proposes) or we could choose to collaborate with AIs to do creative things. The concern here may be that if machines take over everything we do now in life, there would be nothing left for us to do and we would find our lives meaningless. However, this is a big "if": keeping in mind the skepticism about what AI can do (see chapter 3) and the fact that so many of our activities are not "work" but are very meaningful, we will probably have plenty left to do. The question is then not what humans will do when all their work and activities are done by machines, but rather which tasks we want to or

have to reserve for humans and what the roles of AI could be, if any, in supporting us in these tasks in ways that are ethically good and societally acceptable.
   To conclude, AI ethics makes us think about what a good and fair society is, what a meaningful human life is, and what the role of technology is and could be in rela- tion to these. Philosophy, including ancient philosophy, may well be a source of inspiration for thinking about to- day's technologies and their potential and actual ethical and societal problems. If AI raises these ancient questions about the good and meaningful life once again, we have resources in various philosophical and religious traditions that can help us in addressing these questions. For exam- ple, as Shannon Vallor (2016) has argued, the tradition of virtue ethics developed by Aristotle, Confucius, and other ancient thinkers may still help us today to think about what human flourishing is and should be in a technologi- cal age. In other words, it could be that we have already answers to these questions, but we need to do some work to think about what the good life means in the context of today's technologies, including AI.
   However, the idea of developing "an AI ethics of the good life" and an AI ethics for the real world in general face a number of problems. The first is speed. The model of virtue ethics Western philosophy has inherited from Aris- totle assumes a slowly changing society in which technol- ogy does not change so quickly and in which people have

time to learn practical wisdom; it is not clear how it can be used to cope with a fast-changing society (Boddington 2016) and the rapid development of technologies such as AI. Do we still have time to respond and to develop and communicate practical wisdom in relation to the use of technologies such as AI? Does ethics come too late? When philosophy's owl of Minerva finally spreads its wings, the world may already have been altered beyond recognition. What is and should be the role of such an ethics in the context of real-world developments?
   Second, given the diversity and plurality of views on this within societies and cultural differences between so- cieties, questions about the good and meaningful life with technology may well be answered differently in different places and contexts, and in practice they will be subject to all kinds of political processes that may or may not end in consensus. Acknowledging this diversity and plurality might lead to a more pluralist approach. It also might take the form of relativism. Twentieth-century philosophy and society theory, especially so-called postmodernism, have raised much skepticism with regard to answers that present themselves as universal while having emerged from a particular geographical, historical, and cultural context (e.g., "the West") and in relation to particular in- terests and power relations. It has also been questioned if politics should aim at consensus (see Chantal Mouffe's work, e.g., Mouffe 2013); is consensus always desirable,

or could agonistic struggle over the future of AI also have some benefits? Moreover, there is also a problem concern- ing power: thinking about ethics in the real world means thinking not only about what needs to be done with regard to AI but also about who will and should decide about the future of AI and hence the future of our society. Consider again the issues of totalitarianism and the power of large corporations. If we reject totalitarianism and plutocracy, what does democratic decision making with regard to AI mean? What kind of knowledge about AI is needed on the part of politicians and citizens? If there is too little under- standing of AI and its potential problems, we face a danger of technocracy or simply no AI policy at all.
   Yet, as the next chapter shows, at least one of the AI- relevant political processes that has emerged recently dis- plays the ambition to be timely. It is also proactive, aims at consensus, shows a surprising degree of convergence, seems to adhere to a kind of unashamed universalism, is based on expert knowledge, and pays at least lip service to the ideals of democracy, serving the public good and public interest, and involving all stakeholders: AI policymaking.

10



POLICY PROPOSALS 




What Needs to Be Done and Other Questions Policymakers Have to Answer

Given the ethical problems with AI, it is clear that some- thing should be done. Most AI policy initiatives therefore include ethics of AI. Today there are a lot of initiatives in this area and this should be applauded. However, it is not so clear what should be done, what precise course of action should be taken. For example, it is not so clear how to deal with transparency or bias, given the technologies as they are, existing bias in society, and divergent views on jus- tice and fairness. There are also many possible measures to choose from: policy can mean regulation by means of laws and directives, say, legal regulation, but there are also other strategies that may or may not be connected to le- gal regulation, such as technological measures, codes of

ethics, and education. And within regulation there are not only laws but also standards such as ISO norms. Moreover, other sorts of questions also need to be answered in policy proposals: not only what should be done, but also why it should be done, when it should be done, how much should be done, by whom it should be done, and what the nature, extent, and urgency of the problem are.
   First, it is important to justify the measures proposed. For example, a proposal may draw on principles of human rights to justify a proposal to reduce biased algorithmic decision making. Second, in response to technology devel- opment, policy often comes too late, when the technol- ogy is already embedded in society. Instead, one can try to make policy before the technology is fully developed and used. For AI this is still possible, to some extent, al- though a lot of AI is already out there. The time dimension is also relevant with regard to the temporal scope of the policy: is it only meant for the next five or ten years, or is it meant as a framework for the longer term? Here we need to make choices. For example, one could disregard long-term predictions and focus on the near future, as most proposals do, or one could offer a vision of the fu- ture of humanity. Third, not everyone agrees that solving the problems requires a lot of new measures. Some people and organizations have argued that current legislation is enough to deal with AI. If this is the case, then it seems that not much needs to be done by lawmakers, but more

needs to be done by those who interpret law and those who develop AI. Others think that we need to fundamen- tally rethink society and its institutions, including our le- gal systems, in order to deal with the underlying problems and to prepare future generations. Fourth, a policy pro- posal should be clear about who should take action. This may be not one actor but more than one. As we have seen, many hands are involved in any technological action. This raises the question of how to distribute responsibility for policy and change: is it mainly up to governments to take action, or should, for example, businesses and industry develop their own course of action to ensure ethical AI? When it comes to business, should one address only large corporations or also small and medium-sized businesses? And what is the role of individual (computer) scientists and engineers? What is the role of citizens?
   Fifth, answering what should be done, how much should be done, and other questions crucially depends on how one defines the nature, extent, and urgency of the problem itself. For example, there is a tendency in technol- ogy policy (and indeed in AI ethics) to see new problems everywhere. However, in the previous chapter we have seen that many problems may not be unique to a new tech- nology, but perhaps existed long before. Furthermore, as the discussion about bias has also shown, what we propose to do depends on how we define the problem: is it a prob- lem of justice, and if so, what kind of justice is threatened?


pOliCY prOp OSalS	147

The definition will shape the measures one proposes. For example, if one proposes measures of affirmative action, then this is rooted in a particular problem definition. Fi- nally, also playing a role is the very definition of AI, which is always contestable and which matters for the scope of the policy. For example, is it possible and desirable to distinguish clearly between AI and smart autonomous algorithms, or between AI and automation technologies? All these questions render AI policymaking a potentially controversial business. And indeed, we do find many dis- agreements and tensions, for example on how much new legislation is needed, on which principles exactly to draw for justifying one's measures, and on the question whether ethics should be balanced with other considerations (e.g., competitiveness of businesses and the economy). How- ever, if we consider the actual policy documents, we also find a remarkable degree of convergence.


Ethical Principles and Justifications

The widely shared intuition that there is an urgency and importance in dealing with the ethical and societal chal- lenges raised by AI has led to an avalanche of initiatives and policy documents that not only identify some ethi- cal problems with AI but also aim to provide normative guidance for policy. AI policy with an ethical component






The widely shared intuition that there is an urgency and importance in dealing with the ethical and societal challenges raised by AI has led to an avalanche of initiatives and policy documents.

has been proposed by a range of actors, including govern- ments and governmental bodies such as national ethics committees, tech companies such as Google, engineers and their professional organizations such as IEEE, inter- governmental organizations such as the EU, nongovern- mental nonprofit actors, and researchers.
   If we review some recent initiatives and proposals, it turns out that most documents start with the justification of policy by articulating principles and then make some recommendations with regard to the ethical problems identified. As we will see, these problems and principles are very similar. Initiatives often rely on general ethical prin- ciples and principles from professional ethical codes. Let me review some proposals.
   Most proposals reject the science fiction scenario in which superintelligent machines take over. For example, under the presidency of Obama, the US government pub- lished the report "Preparing for the Future of Artificial Intelligence," which explicitly claims that the long-term concerns about superintelligent general AI "should have little impact on current policy" (Executive Office of the President 2016, 8). Instead, the report discusses current and near future problems raised by machine learning, such as bias and the problem that even developers may not understand their system well enough to prevent such outcomes. The report emphasizes that AI is good for inno- vation and economic growth and stresses self-regulation,

but says that the US government can monitor the safety and fairness of applications and if necessary adapt regula- tory frameworks.
   Many countries in Europe have AI strategies now that include an ethical component. "Explainable AI" is a goal shared by many policymakers. The UK's House of Commons (2018) says that transparency and the right to explanation is key for algorithmic accountability, and that industries and regulators should tackle biased algo- rithmic decision making. The UK's House of Lords Select Committee on AI also examines the ethical implications of AI. In France, the Villani report proposes working to- ward "meaningful AI" that does not reinforce problems of exclusion, increase inequality, or lead to a society in which we are governed by black box algorithms: AI should be explainable and environmentally friendly (Villani 2018). Austria has recently set up a national advisory council dedicated to robotics and AI1 that has made policy recom- mendations based on human rights, justice and fairness, inclusiveness and solidarity, democracy and participation, nondiscrimination, responsibility, and similar values. Its white paper also recommends explainable AI and explic- itly says that responsibility remains with humans; AIs can- not be morally responsible (ACRAI 2018). International organizations and conferences are also very active. For example, the International Conference of Data Protection and Privacy Commissioners has published a declaration

on ethics and data protection in AI, including principles of fairness, accountability, transparency and intelligibility, responsible design and privacy by design (a concept that calls for taking privacy into account throughout the entire engineering process), empowerment of individuals, and the reduction and mitigation of biases or discrimination (ICDPPC 2018).
   Some policymakers frame their aim in terms of "trust- worthy AI." The European Commission, for example, un- doubtedly one of the main global players in the area of AI policymaking, puts a lot of weight on the term. In April 2018, it set up a new High-Level Expert Group on Arti- ficial Intelligence to create a new set of AI guidelines; in December 2018 the group released a draft working docu- ment with ethics guidelines that calls for a human-centric approach to AI and the development of trustworthy AI, which respects fundamental rights and ethical principles. The rights mentioned are human dignity, freedom of the individual, respect for democracy, justice and the rule of law, and citizens' rights. The ethical principles are benefi- cence (do good) and no harm, autonomy (preserve human agency), justice (be fair), and explicability (operate trans- parently). These principles are familiar from bioethics, but the document adds explicability and includes interpreta- tions that highlight the specific ethical problems raised by AI. For example, the no harm principle is interpreted as requiring that AI algorithms must avoid discrimination,

manipulation, and negative profiling, and must protect vulnerable groups such as children and immigrants. The justice principle is interpreted as including the demand that developers and implementers of AI need to ensure that individuals and minority groups maintain freedom from bias. The principle of explicability is seen as requir- ing that AI systems be auditable and "comprehensible and intelligible by human beings at varying levels of compre- hension and expertise" (European Commission AI HLEG 2018, 10). The final version, released in April 2019, speci- fies that explainability is not only about explaining the technical process but also about the related human deci- sions (European Commission AI HLEG 2019, 18).
   Previously another EU advisory body, the European Group on Ethics in Science and New Technologies (EGE), released a statement on AI, robotics, and autonomous sys- tems, proposing the principles of human dignity, auton- omy, responsibility, justice, equity, solidarity, democracy, rule of law and accountability, security and safety, data protection and privacy, and sustainability. The principle of human dignity is said to imply that people have to be made aware whether they are interacting with a machine or with another human being (EGE 2018). Note also that the EU already has existing regulations in place relevant to the development and use of AI. The General Data Protec- tion Regulation (GDPR), which was enacted in May 2018, aims to protect and empower all EU citizens with regard

to data privacy. It includes principles such as the right to be forgotten (the data subject can ask to erase his or her personal data and halt the further processing of the data) and privacy by design. It also gives data subjects the right to access "meaningful information about the logic involved" in automated decision making and information about the "envisaged consequences" of such processing (European Parliament and the Council of the EU 2016). The difference with the policy documents is that here these principles are legal requirements. It is legislation that is enforced: organizations in breach of GDPR can be fined. However, it has been questioned whether the provi- sions of the GDPR amount to a full right of explanation of the decision (Digital Europe 2018) and, more generally, whether it offers enough protection against the risks of automated decision making (Wachter, Mittelstadt, and Floridi 2017). The GDPR provides a right to be informed about automated decision making but does not seem to demand an explanation about the rationale for any indi- vidual decision. This is also a concern when it comes to decision making in the legal sphere. A Council of Europe study, based on work by a committee of human rights ex- perts, has demanded that individuals have the right to a fair trial and due process in terms comprehensible to them (Yeung 2018).
Legal discussions are of course highly relevant for
discussions about AI ethics and AI policy. Turner (2019)

has discussed comparisons with animals (how they are and have been treated by the law and whether they have rights) and has reviewed a number of legal instruments with regard to what they could mean for AI. For example, when harm has been inflicted, the question of negligence concerns whether a person was under a duty of care to pre- vent harm, even if said harm was not intended. This could be applied to the designer or trainer of the AI. But how easy is it to foresee the consequences of AI? Criminal law, by contrast, requires the intention to do harm. But this is often not the case with AI. Product liability, on the other hand, does not concern the fault of individuals but has the company who produced the technology pay for damages, regardless of fault. This could be one possible solution to legal responsibility for AI. Intellectual property laws are also relevant to AI, such as copyright and patents, and dis- cussions have emerged about "legal personality" for AI, a legal fiction but an instrument that is currently applied to companies and various organizations. Should it also be applied to AI? In a controversial resolution of 2017, the European Parliament suggested that giving the most so- phisticated autonomous robots the status of electronic persons is one possible legal solution to the issue of legal responsibility-an idea that has not been taken up by the European Commission in its 2018 strategy for AI.2 Others have aggressively opposed the very idea of giving rights and personhood to machines, arguing, for instance, that

it would then become difficult if not impossible to hold anyone accountable since people will seek to exploit the concept for selfish ends (Bryson, Diamantis, and Grant 2017). There was also the famous case of Sophia, a robot granted "citizenship" by Saudi Arabia in 2017. Such a case raises again the question regarding moral status of robots and AIs (see chapter 4).
   AI policy has also been proposed beyond North America and Europe. China, for instance, has a national AI strategy. Its development plan recognizes that AI is a disruptive technology that can affect social stability, im- pact law and social ethics, violate personal privacy, and create safety risks; the plan therefore recommends to strengthen forward-looking prevention and minimizing risk (State Council of China 2017). Some actors in the West tell a competition narrative: they fear that China will overtake us or even that we are approaching a new world war. Others try to learn from China's strategy. Re- searchers may also ask how different cultures deal with AI in different ways. AI research itself can contribute to tak- ing a more cross-cultural or comparative perspective on AI ethics, for example, when it reminds us of differences between individualist and collectivistic cultures when it comes to moral dilemmas (Awad et al. 2018). This could raise problems for AI ethics if it aims to be universal. One could also explore how narratives about AI in China or Japan, for example, differ from Western narratives. Yet

in spite of cultural differences, it turns out that AI ethics policies are remarkably similar. While China's plan places more emphasis on social stability and the collective good, the ethical risks identified and the principles mentioned are not that different from those proposed by Western countries.
   But, as mentioned before, AI ethics policy is also not at all restricted to governments and their committees and bodies. Academics have also taken initiatives. For ex- ample, the Montreal Declaration Responsible AI has been proposed by the University of Montreal and involved consultation of citizens, experts, and other stakeholders. It says that the development of AI should promote the well-being of all sentient creatures and the autonomy of human beings, eliminate all types of discrimination, re- spect personal privacy, protect us from propaganda and manipulation, promote democratic debate, and make vari- ous players responsible for working against the risks of AI (Université de Montréal 2017). Other researchers have proposed the principles of beneficence, non-maleficence, autonomy, justice, and explicability (Floridi et al. 2018). Universities such as Cambridge and Stanford work on the ethics of AI, often from an applied ethics perspective. Peo- ple working in professional ethics also do helpful work. For example, the Markkula Centre for Applied Ethics at Santa Clara University has offered a number of ethical theories as a toolkit for technology and engineering practice, which













In spite of cultural differences, it turns out that AI ethics policies are remarkably similar.

may also inform AI ethics.3 And philosophers of technol- ogy have recently shown a lot of interest in AI.
   We also find initiatives on AI ethics in the corporate world. For example, the Partnership on AI includes compa- nies such as DeepMind, IBM, Intel, Amazon, Apple, Sony, and Facebook.4 Many companies recognize the need for ethical AI. For example, Google has published ethics prin- ciples for AI: providing social benefit, avoiding creating or reinforcing unfair bias, enforcing safety, maintaining accountability, maintaining privacy design, promoting scientific excellence, and limiting potentially harmful or abusive applications such as weapons or technologies that violate principles of international law and human rights.5 Microsoft talks about "AI for Good" and proposes the principles of fairness, reliability and safety, privacy and security, inclusiveness, transparency, and accountability.6 Accenture has proposed universal principles of data ethics, including respecting the persons behind the data, privacy, inclusion, and transparency.7 And although in corporate documents the emphasis tends to be on self-regulation, some companies recognize the need for external regula- tion. Apple's CEO Tim Cook has said that tech regulation, for example, to ensure privacy, is inevitable because the free market is not working.8 Yet there is debate about whether this requires new regulation. Some support the path of regulation, including new laws. California has al- ready proposed a bill that requires the disclosure of bots: it

is unlawful to use a bot if it is done in a way that misleads another person about its artificial identity.9 Others take a more conservative position. Digital Europe (2018), which represents the digital industry in Europe, has argued that today's legal framework is equipped to address concerns related to AI, including bias and discrimination, but that in order to build trust, transparency, explainability, and interpretability are important: people and businesses should understand when and how algorithms are used in decision making, and we need to provide meaningful in- formation and facilitate the interpretation of algorithmic decisions.
   Nonprofit actors also play a role. For example, the international Campaign to Stop Killer Robots raises many ethical questions regarding military applications of AI.10 From a transhumanist side, there are the Asilomar AI prin- ciples, agreed on by academic and industry participants of a conference convened by the Future of Life Institute (Max Tegmark and others). The overall aim is to keep AI beneficial and to respect ethical principles and values such as safety, transparency, responsibility, value alignment, privacy, and human control.11 There are also professional organizations that work on AI policy. The Institute of Elec- trical and Electronics Engineers (IEEE), which claims to be the world's largest technical professional organization, has put forward a Global Initiative on Ethics of Autono- mous and Intelligent Systems. After discussions among

experts, the Initiative has produced a document with a vision for "ethically aligned design," proposing that the design, development, and implementation of these tech- nologies should be guided by the general principles of hu- man rights, well-being, accountability, transparency, and awareness of misuse. Implementing ethics in global tech- nical standards could be an effective way of contributing to the development of ethical AI.


Technological Solutions and the Question of Methods and Operationalization

The IEEE Global Initiative shows that, in terms of mea- sures, some policy documents focus on technological solu- tions. For example, as mentioned in the previous chapter, some researchers have called for explainable artificial intelligence, for opening the black box. There are good reasons for wanting to do this: explaining the rationale behind one's decision is not only ethically required but also an important aspect of human intelligence (Samek, Wiegand, and Müller 2017). The idea of explainable AI or transparent AI is then that the actions and decisions made by AIs should be easily understood. As we've seen, this idea is difficult to implement in the case of machine learning that uses neural networks (Goebel et al. 2018). But policy can of course support research in this direction.

    In general, it is an excellent idea to embed ethics in the design of new technologies. Ideas such as ethics by design or value-sensitive design, which have their own history,12 can help us to create AI in a way that leads to more ac- countability, responsibility, and transparency. For exam- ple, ethics by design could include the requirement that traceability is ensured at all stages (Dignum et al. 2018), thus contributing to the accountability of AI. The idea of traceability can be taken literally, in the sense of record- ing data about the behavior of the system. Winfield and Jirotka (2017) have called for implementing an "ethical black box" in robots and autonomous systems, which re- cords what the robot does (data from sensors and from the "internal" state of the system) in a way similar to the black box installed in airplanes. This idea could also be applied to autonomous AI: when something goes wrong, such data might help us to explain what exactly went wrong. This may assist ethical and legal analyses of the case. Moreover, as the researchers rightly remark, we can learn something from the aircraft industry, which is highly regulated and has tough safety certification processes and visible proc- esses of accident investigation. Could similar regulatory and safety infrastructures be installed for AI? To make a comparison with another transportation field, the auto- mobile industry has also proposed certification or a kind of "driver's license" for AI autonomous vehicles.13 Some researchers go further and aim at creating moral machines,







Ideas such as ethics by design or value- sensitive design can help to create AI in a way that leads to more accountability, responsibility, and transparency.

attempting "machine ethics" in the sense that machines themselves can make ethical decisions. Others argue that this is a dangerous idea and that this should be reserved for humans, that it is impossible to create full ethical agents, that there is no need for machines to be full ethi- cal agents and that it is enough for them to be safe and law abiding (Yampolskiy 2013), or that there could be forms of "functional morality" (Wallach and Allen 2009) that do not amount to full morality but still render the machine relatively ethical. This discussion, which connects again to the issue of moral status, is relevant, for example, in the case of self-driving cars: to what extent is it necessary, possible, and desirable to build ethics into these cars, and what kind of ethics should this be and how should it be technically implemented?
   Policymakers tend to endorse many of these direc- tions in AI research and innovation, such as explainable AI and, more generally, embedding ethics in design. For example, next to nontechnical methods such as regula- tion, standardization, education, stakeholder dialogue, and inclusive design teams, the High-Level Expert Group report mentions a number of technical methods including ethics and rule of law by design, architectures for trust- worthy AI, testing and validating, traceability and audit- ability, and explanation. For example, ethics by design can include privacy by design. The report also mentions some ways in which trustworthy AI can be operationalized, such

as traceability as a way to contribute to transparency: in the case of rule-based AI it should be clarified how the model has been built, and in the case of learning-based AI the method of training the algorithm should be clari- fied, including how the data were gathered and selected. This should ensure that the AI system is auditable, par- ticularly in critical situations (European Commission AI HLEG 2019).
   The question of methods and operationalization is crucial: it is one thing to name a number of ethical prin- ciples and quite another to figure out how to implement them in practice. Even concepts such as privacy by design, which are supposed to be closer to the process of develop- ment and engineering, are usually formulated in a rather abstract and general way: it remains unclear what exactly we should do. This takes us to the next chapter for a brief discussion of some challenges for AI ethics policy.















pOliCY prOp OSalS	165



11


CHALLENGES FOR POLICYMAKERS 




Proactive Ethics: Responsible Innovation and Embedding Values in Design

Perhaps unsurprisingly, AI ethics policy faces numer- ous challenges. We have seen that some policy proposals endorse a vision of AI ethics that is proactive: we need to take ethics into account at the early stage of the de- velopment of AI technology. The idea is to avoid ethical and societal problems created by AI that would be hard to deal with once they have arrived. This is in line with ideas about responsible innovation, embedding values in design, and similar ideas proposed over recent years. It shifts the problem from having to deal with the nega- tive effects of technologies that are already widely used to taking responsibility for technologies that are being developed today.

   However, it is not easy to foresee the unintended con- sequences of new technologies at design stage. One way to mitigate this problem is to construct scenarios about fu- ture ethical effects. There are various methods for practic- ing ethics in research and innovation (Reijers et al. 2018), one of which is not only to study and assess the impact of current AI narratives (Royal Society 2018) but also to create new, more concrete narratives about particular AI applications.


Practice Oriented and Bottom Up: How Can We Translate These to Practice?

Responsible innovation is not only about embedding eth- ics in design, but also requires taking into account the opinions and interests of various stakeholders. Inclusive governance entails broad stakeholder involvement, pub- lic debate, and early societal intervention in research and innovation (Von Schomberg 2011). This can mean, for example, organizing focus groups and using other tech- niques to see what people think about the technology.
   This more bottom-up responsible innovation ap- proach is somewhat in tension with the applied ethics approach of most policy documents, which are rather top down and remarkably abstract. First, policies are often created by experts, without input from a broad range of









Responsible innovation is not only about embedding ethics in design, but also requires taking into account the opinions and interests of various stakeholders.

stakeholders. Second, even if they endorse principles such as ethics by design, they tend to remain too vague about what applying these principles means in practice. To make AI policy work, it remains a huge challenge to build a bridge between, on the one hand, abstract, high-level ethical and legal principles and, on the other hand, the practices of technology development and use in particular contexts, the technologies, and the voices of those who are part of these practices and work in these contexts. This bridging work is left to the addressees of the proposals. Can and should more be done, at the earlier stage of policymak- ing? At the very least, more work on the "how" is required alongside the "what": the methods, procedures, and insti- tutions we need for making AI ethics work in practice. We need to pay more attention to process.
   With regard to the "who" question concerning AI eth- ics, we need more room for bottom-up next to top-down, in the sense of listening more to researchers and profes- sionals who work with AI in practice and indeed to people potentially disadvantaged by AI. If we endorse the ideal of democracy and if that concept includes inclusiveness and participation in decision making about the future of our societies, then hearing the voice of stakeholders is not optional but ethically and politically required. While some policymakers engage in some form of stakeholder consul- tation (for example, the European Commission has its AI Alliance),1 it remains questionable if such efforts really

reach the developers, the end users of the technology, and-most importantly-those who will have to carry most of the risks and live with its negative consequences. How democratic and participative is the decision making and policy about AI, really?
   The ideal of democracy is also endangered by the fact that power is concentrated in the hands of a relatively small number of large corporations. Paul Nemitz (2018) has argued that such accumulation of digital power in the hands of a few is problematic: if a handful of companies exercise power not only over individuals-by profiling us, they centralize power-but also over infrastructures for democracy, then in spite of their best intentions to con- tribute to ethical AI, such companies also put up barriers to it. It is then necessary to regulate and set boundaries to safeguard public interest, and to make sure that these companies do not shape the rules by themselves. Murrah Shanahan has also pointed to the "self-perpetuating ten- dency for power, wealth, and resources to concentrate in the hands of a few" (2015, 166), which makes it difficult to bring about a more equitable society. It also renders peo- ple vulnerable to all kinds of risks, including exploitation and violations of privacy, for example what a Council of Europe study calls "the chilling effect of data repurposing" (Yeung 2018, 33).
   If we compare the situation with environmental policy, we may also be pessimistic about the possibility

that countries will take effective and collaborative action concerning AI ethics. Consider, for example, the politi- cal processes around climate change in the United States, where sometimes even the very problem of global warming and climate change is denied and where powerful politi- cal forces work against taking action, or the rather limited success of international climate conferences to agree on a common and effective climate policy. Those seeking global action in light of the ethical and societal problems raised by AI may face similar difficulties. Often interests other than public good prevail, and there is far too little intergovernmental policy on new digital technologies, in- cluding AI. One exception is the global interest in banning automatic lethal weapons, which also have an AI aspect to it. But this remains an exception, and is also not sup- ported by all countries (it remains controversial in the US, for example).
   Moreover, albeit well-intended, ethics by design and responsible innovation have their own limitations. First, methods such as value-sensitive design presuppose that we can articulate our values, and efforts to build moral machines assume that we can fully articulate our ethics. But this is not necessarily the case; our everyday ethics may not be a matter of fully articulate reasoning at all. Sometimes we respond to ethical problems without being able to fully justify our response (Boddington 2017). To borrow a term from Wittgenstein: our ethics is not only

embodied but also embedded in a form of life. It is deeply connected to the way we do things as embodied and social beings, and as societies and cultures. This sets limits to the project of fully articulating ethics and moral reason- ing. It poses a problem for the project of developing moral machines and challenges the assumptions that ethics and democracy can be fully deliberative. It is also a problem for those policymakers who think that AI ethics can be dealt with fully by means of a list of principles or by means of specific legal and technical methods. We absolutely need methods, procedures, and operations. But these are not enough; ethics does not work like a machine, and neither do policy and responsible innovation.
Second, these approaches can also be a barrier to eth-
ics when it would be ethically required to stop the develop- ment of the technology. Often they function in practice as a kind of oil that helps to lubricate the machinery of innovation, to enhance profit making, and to ensure the acceptability of the technology. This may not be necessar- ily bad. But what if the ethical principles imply that the technology, or a particular application of the technology, should actually be halted or paused? Crawford and Calo (2016) have argued that value-sensitive design and re- sponsible innovation tools work on the assumption that the technology will be developed; they are less helpful when it comes to deciding whether it should be built at all. For example, in the case of advanced AI such as new

machine learning applications, it might be that the tech- nology is still unreliable or has serious ethical drawbacks, and that at least some applications should not be used (yet). Whether or not stopping is always the best solution, the point is that we should at least have the space to ask the question and to decide. If this critical space is lacking, responsible innovation remains a fig leaf for doing busi- ness as usual.


Toward a Positive Ethics

That being said, generally speaking AI ethics is not neces- sarily about banning things (Boddington 2017). Another barrier to getting AI ethics to work in practice is that many actors in the AI field such as companies and technical re- searchers still think of ethics as a constraint, as something negative. This idea is not totally misguided: often ethics has to constrain, has to limit, has to say that something is unacceptable. And if we take AI ethics seriously and imple- ment its recommendations, we might face some trade-offs, in particular in the short term. Ethics may have a cost: in terms of money, time, and energy. However, by reducing risks, ethics and responsible innovation support the long- term, sustainable development of businesses and of soci- ety. It is still a challenge to convince all the actors in the AI field, including policymakers, that this is indeed the case.









AI ethics is not necessarily about banning things; we also need a positive ethics: to develop a vision of the good life and the good society.

Note also that policy and regulation are not only about banning things or making things more difficult; they can also be supportive, offering incentives, for example.
   Furthermore, next to a negative ethics that sets limits, we also need to make explicit and elaborate a positive eth- ics: to develop a vision of the good life and the good society. While some of the ethical principles proposed above hint at such a vision, it remains a challenge to move the dis- cussion in that direction. As previously argued, the ethi- cal questions regarding AI are not just about technology; they are about human lives and human flourishing, about the future of society, and perhaps also about nonhumans, the environment, and the future of the planet (see the next chapter). Discussions about AI ethics and AI policy then lead us once more to the big questions we need to ask ourselves-as individuals, as societies, and perhaps as humanity. Philosophers can help our thinking about these questions. For policymakers, the challenge is to develop a broad vision of the technological future that includes ideas about what is important, meaningful, and valuable. While in general liberal democracies are set up to leave such questions to individuals and are supposed to be "thin" about matters such as the good life (a political innovation that has stopped at least some kinds of wars and has con- tributed to stability and prosperity), in light of the ethical and political challenges we face it would be irresponsible to neglect the more substantive, "thick" ethical questions

altogether. Policy, including AI policy, should also be about positive ethics.
   The way to do this for policymakers, however, is not by flying solo and taking the role of the Platonic philosopher- king, but to find the right balance between technocracy and participative democracy. The questions at hand are questions that concern us all; we all have a stake in them. Therefore, they cannot be left in the hands of the few, whether these are people in government or in large cor- porations. This leads us back to questions about how to make responsible innovation and participation in AI pol- icy work. The problem is not only about power; it is also about the good: the good for individuals and the good for society. Our current ideas about the good life and the good society, if we can articulate them at all, may well need a lot more critical discussion. Let me suggest that for the West it could be helpful to at least explore the option of trying to learn from other, non-Western political systems and other political cultures. An effective and well-justified AI policy should not avoid tapping into these kinds of ethical- philosophical and political-philosophical discussions.


Interdisciplinarity and Transdisciplinarity

There are further barriers that we need to overcome if we want to make AI ethics more effective and support the

 responsible development of the technology, avoiding what technical researchers call a new AI "winter": the slowing down of AI development and investment. One is the lack of sufficient interdisciplinarity and transdisciplinarity. We still face a significant gap in background and understand- ing between, on the one hand, people from the humani- ties and social sciences, and, on the other hand, people from the natural and engineering sciences, both within and outside academia. So far, institutional support for a significant and substantial bridging between these two "worlds" is lacking, in academia and in wider society. But if we really want to have ethical high tech such as ethical AI, we need to bring these people and these worlds closer together, sooner rather than later.
   This requires a change in how research and develop- ment is done-it should involve not only technical and business people but also people from the humanities, for example-but also a change in how we educate people, both young people and not so young people. We need to ensure that, on the one hand, people with a humanities background become aware of the importance of thinking about new technologies such as AI and can acquire some knowledge of these technologies and what they do. On the other hand, scientists and engineers need to be made more sensitive to the ethical and societal aspects of technol- ogy development and use. When they learn to use AI and later contribute to the development of new AI technology,

 ethics should be seen not as a marginal topic that has little to do with their technological practice but as constituting an essential part of it. Ideally, what it means to "do AI" or to "do data science" would then simply include ethics. More generally, we could consider the idea of a more diverse and holistic kind of Bildung or narrative that is more radically interdisciplinary and pluralistic with regard to methods and approaches, its topics, and also its media and tech- nologies. To put it bluntly: if engineers learn to do things with texts and humanities people learn to do things with computers, there is more hope for a technology ethics and policy that works in practice.


The Risk of an AI Winter and the Danger of the Mindless Use of AI

If these directions in policy and education do not get off the ground and, more generally, if the project of ethical AI fails, we face not only the risk of an "AI winter"; the ultimate and arguably more important risk is ethical, so- cial, and economic disaster and its related human, non- human, and environmental costs. This has nothing to do with the singularity, terminators, or other apocalyptic scenarios about the distant future, but with the slow but certain increase in the accumulation of technological risk and the resulting growth of human, social, economic, and

environmental vulnerabilities. This increase in risks and vulnerabilities is related to the ethical problems indicated here and in the previous chapters, including the ignorant and reckless use of advanced automation technologies such as AI. The gap in education is perhaps exacerbat- ing what AI risks do in general: even if it does not always directly cause new risks, it also and especially multiplies existing risks. So far there is no such thing as a "driver's license" for using AI, and there is no compulsory AI eth- ics education for technical researchers, business people, government administrators, and other people involved in AI innovation, use, and policy. There is a lot of untamed AI out there in the hands of people who don't know the risks and ethical problems, or who may have the wrong kind of expectations about the technology. The danger is, once again, the exercise of power without knowledge and (therefore) without responsibility-and, worse, others be- ing subjected to this. If there exists such a thing as evil at all, it lives where the twentieth-century philosopher Han- nah Arendt located it: in the mindlessness of banal every- day work and decisions. To assume that AI is neutral and to use it without understanding what one is doing contrib- utes to such mindlessness and, ultimately, to the ethical corruption of the world. Education policy can help to miti- gate this and thus contribute to good and meaningful AI.
   A number of nagging, perhaps slightly painful ques- tions remain, however, which are often neglected in

discussions about AI ethics and policy but that at least deserve mention, if not a lot more analysis. Is AI ethics all about the good for, and value of, humans, or should we also take into account nonhuman values, goods, and interests? And even if AI ethics is mainly about humans, could it be that the question regarding AI ethics is not the most important problem for humanity to address? This question brings us to the last chapter.



12



    IT'S THE CLIMATE, STUPID! ON PRIORITIES, THE ANTHROPOCENE, AND ELON MUSK'S CAR IN SPACE 



Should AI Ethics Be Human-Centric?

While many writings on AI ethics and policy mention the environment or sustainable development, they emphasize human values and are often explicitly human-centric. For example, the HLEG's Ethics Guidelines say that a human- centric approach to AI is needed "in which the human being enjoys a unique and inalienable moral status of primacy in the civil, political, economic and social fields" (European Commission AI HLEG 2019, 10) and universities such as Stanford and MIT have framed their research policies in terms of human-centered AI.1
   Usually this human-centricity is defined in relation to technology: the idea is that the good and dignity of humans take priority over whatever technology may re- quire or do. The point is that technology should benefit

humans and should serve humans rather than the other way around. Yet, as we have seen in the first chapters, the appropriateness of this focus on humans in AI ethics is not as obvious as it may at first seem, especially if we consider posthumanist approaches or critically question competi- tion narratives (humans versus technology). Philosophy of technology shows that there are more-and subtler and more sophisticated-ways of defining the relation between humans and technology. Furthermore, a human- centric approach is at least nonobvious, if not prob- lematic, in light of philosophical discussions about the environment and other living beings. In environmental philosophy and ethics there is a long-standing discussion about the value of nonhumans, especially living be- ings, about how to respect that value and these beings, and about the potential tensions that may arise with re- specting the value of humans. For AI ethics, this implies that we should at least ask the question regarding the im- pact of AI on other living beings and consider the problem that there might be tension between human and nonhu- man values and interests.


Getting Our Priorities Right

It could also be argued that there are other more serious problems than those caused by AI, and that it is important







A human-centric approach is at least nonobvious, if not problematic, in light of philosophical discussions about the
environment and other
living beings.

to get one's priorities right. This objection could arise from a consideration of global problems such as climate change, according to some the problem humanity needs to address and prioritize given its urgency and potential impact on the planet as a whole.
   Looking at the United Nations' 2015 agenda for sus- tainable development (the so-called Sustainable Develop- ment Goals)2 and its overview of global issues concerning what UN Secretary-General Ban Ki-moon called "people and planet," we see many global issues that demand ethi- cal and political attention: rising inequalities within and among countries, war and violent extremism, poverty and malnutrition, lack of access to fresh water, lack of effec- tive and democratic institutions, ageing populations, in- fectious and epidemic diseases, risks related to nuclear energy, lack of opportunities for children and young people, gender inequality and various forms of discrimi- nation and exclusion, humanitarian crises and all kinds of human rights violations, problems related to migra- tion and refugees, and climate change and environmental problems-sometimes related to climate change-such as more frequent and intense natural disasters and forms of environmental degradation such as drought and loss of biodiversity. In light of these massive problems, should AI be our top priority? Does AI distract from more important issues?

   One the one hand, a focus on AI and other problems with technology seems out of place when so many humans are suffering and the world is plagued by so many other problems. While people in one part of the world struggle to gain access to fresh water or to survive in violent envi- ronments, people in another part of the world worry about their privacy on the internet and fantasize about a future when AIs achieve superintelligence. Ethically speaking, something fishy seems to be going on, which is related to global inequalities and injustices. Ethics and policy should not remain blind to such problems, which are not neces- sarily about AI at all. For example, sometimes in devel- oping countries, low tech rather than high tech can help people to deal with problems since they can afford, install, and maintain it.
   On the other hand, AI could cause new problems and also aggravate existing problems in societies and with the environment. For example, some fear that AI will widen the gap between rich and poor and that, like many digi- tal technologies, it will increase energy consumption and create more waste. From this perspective, discuss- ing and dealing with AI ethics is not a distraction but one of the ways we can contribute to addressing the world's problems, including environmental ones. One could thus conclude that we also need to pay attention to AI: yes, poverty, war, and so on are serious problems, but AI may also cause or aggravate serious problems now and in the









While people in one part of the world struggle
to gain access to fresh water, people in another part of the world worry about their privacy on the internet.

future, and should be on our list of problems needing solu- tions. However, this does not answer the question about priorities-an important ethical and policy question. The point is not that there are easy answers to that question; the point is that the question is not even asked in most academic writings and policy documents on AI.


AI, Climate Change, and the Anthropocene

One of the most challenging ways to ask the question regarding priorities is to bring in the discussion about climate change and related topics such as the Anthropo- cene: "Why worry about AI if the urgent problem is cli- mate change and the future of the planet is at stake?" Or to adapt a phrase from US political culture: "It's the climate, stupid!" Let me unpack this challenge and discuss its im- plications for thinking about AI ethics.
    While some extremists reject the scientific findings, climate change is widely recognized by scientists and poli- cymakers to be not only a serious global problem but also "one of the greatest challenges of our time," as the UN's Sustainable Development Goals text puts it. It is not only a problem in the future: global temperature and sea levels are already rising, which is affecting lower-lying coastal areas and countries. Soon more people will have to deal with the consequences of climate change. Many people











"Why worry about AI if the urgent problem is climate change and the future of the planet is at stake?"

conclude from this that we have to act urgently, now, to mitigate the risks of climate change-"mitigate" because the process may well be beyond the tipping point. The idea is that it is not only high time to do something but pos- sibly already too late to avoid all consequences. Compared with transhumanist fears about superintelligence, this concern is much better supported by scientific evidence and has gained considerable support among the well- educated elites in the West who, understandably bored by postmodern skepticism and bureaucratized identity poli- tics, now see a reason to focus on a problem that seems so true, so real, and so universal: climate change is really happening and concerns everyone and everything on this planet. A recent wave of activism calls attention to the cli- mate crisis, for example Greta Thunberg's campaign and the climate strikes.
   Sometimes the concept of the Anthropocene is used to frame the problem. Coined by climate researcher Paul Crutzen and biologist Eugene Stoermer, this is the idea that we are living in a geological epoch in which humanity has dramatically increased its power over the Earth and its ecosystems, turning humans into a geological force. Consider the exponential growth of human and cattle populations, increasing urbanization, the exhaustion of fossil fuels, the massive use of fresh water, the extinction of species, the release of toxic substances, and so on. Some think the Anthropocene started with the agricultural

revolution; others think it took off with the industrial revolution (Crutzen 2006) or after the Second World War. In any case, a new story and new history have been created, perhaps even a new grand narrative. The concept is often used today to raise concern about global warming and climate change, and to bring together various disciplines (including the humanities) to think about the future of the planet.
    Not everyone adopts the term-it is controversial even among geologists-and some have questioned its an- thropocentrism. For example, Haraway (2015) has argued from a posthumanist perspective that other species and "abiotic" actors also play a role in the shifting environment. But even without a controversial concept such as the An- thropocene, climate change and (other) environmental problems are here to stay, and policy must deal with them, preferably sooner rather than later. What does this mean for AI policy?
   Many researchers think that AI and big data could also help us to deal with many of the world's problems, in- cluding climate change. Like digital information and com- munication technologies in general, AI can contribute to sustainable development and to dealing with many envi- ronmental problems. Sustainable AI is likely to become a successfuldirectioninresearchanddevelopment. However, AI could also make things worse for the environment- and hence for all of us. Consider again increased energy

consumption and waste. And seen from the perspective of the Anthropocene problem, the risk is that humans could use AI to tighten their grip on the Earth, thus worsening the problem instead of solving it.
This is especially problematic if AI is seen not only as a
solution but as the main solution. Consider a superintelli- gence scenario of an AI that knows better than us humans what is good for us: a "benign" AI that serves humanity by making humans act in their own interests and that of the planet-say, a technological equivalent of Plato's philosopher-king, a machine god. Homo deus (Harrari 2015) is replaced by AI deus, which manages our life sup- port system for us and manages us. To solve the problems of resource distribution, for example, the AI could act as a "server," managing the access humans have to resources. Its decisions would be based on its analysis of patterns in data. This diet scenario could be combined with Pro- methean technological solutions such as geo-engineering. It's not just humans who need to be managed; the planet needs to be reengineered. Technology would thus be used to "fix" our problems and those of the planet.
   Yet these scenarios would not only be authoritarian and violate human autonomy but would also centrally con- tribute to the problem of the Anthropocene itself: human hyper-agency, this time delegated by humans to machines, turns the entire planet into a resource and machine for humans. The problem of the Anthropocene is "solved" by

taking it to its technocratic extreme, leading to a world of machines in which humans are first treated as children to be cared for and perhaps later become obsolete. With this kind of Big Data Anthropocene and the all-too-familiar drama of humans being replaced by machines, we land back in the scenarios of dreams and nightmares.


The New Space Craze and the Platonic Temptation

Another answer to climate change and the Anthropocene, which is also a technophile vision and is sometimes linked with transhumanist narratives, is this: we might mess up this planet, but we can escape from the Earth and go to space.
   An iconic image of 2018 was Elon Musk's Tesla sports car floating in space.3 Musk also has plans to colonize Mars. He is not the only dreamer: there is a growing inter- est in going to space. And it is more than just a dream. A lot of money is being invested in space projects. In contrast to the twentieth-century space race, it is now propelled by private companies. And not only tech millionaires but also artists are very interested in space. Musk's company SpaceX plans to send artists to orbit the moon.4 Space tour- ism is another increasingly popular idea. Who wouldn't want to go to space? Space is hot.

   There is nothing wrong with going to space per se. It does have potential benefits. For example, research on how to survive in more extreme environments can help us to deal with problems on Earth, experiment with sus- tainable technologies, and take a planetary perspective. Consider also that the problem of the Anthropocene could be formulated only because many years earlier, space tech- nology made it possible for us to view the Earth from a distance. And considering the image of Musk's car again: some think the electric car is a solution to environmental problems, without questioning the assumption that cars are the best means of transportation and without consid- ering how the electricity is produced. Anyway, there are interesting ideas out there.
But the space dreams are problematic if the result is
that earthly problems are neglected, and if they are symp- tomatic of a tendency Hannah Arendt (1958) already diagnosed when she wrote about the human condition: too much abstraction and alienation. She suggested that science supports a desire to leave the Earth: literally, by means of space technology (in her time, Sputnik) but also by means mathematical methods that abstract and alien- ate from what I would call our messy earthly, embodied, and political life. From this perspective, transhuman- ist fantasies about superintelligence and about leaving the Earth can be interpreted as exponents of a problem- atic kind of alienation and escapism. It is Platonism and

transhumanism writ large: the idea is to overcome not only the limitations of the human body, but also those of this other "life support system": the Earth itself. Not only the body but the Earth is seen as a prison, as something we need to escape.
   One danger of AI, then, is that it enables this kind of thinking and becomes an alienation machine: an instru- ment to leave the Earth and deny our vulnerable, bodily, earthly, and dependent existential condition. In other words: a rocket. Again, there is nothing wrong with rock- ets per se. The problem is the mix of particular technolo- gies with particular narratives. While AI can potentially be a positive force for our personal lives, for society, and for humanity, a combination of the amplification of the abstracting and alienating tendencies in science and tech- nology with transhumanist and "trans-Earthist" fanta- sies may lead to a technological future that is not good for human beings and other living beings on Earth. If we escape rather than deal with our problems-for example, climate change-then we may win Mars (for now) but lose the Earth.
   And as always, there is a further political side to this: some have more opportunities, money, and power to es- cape than others. The problem is not only that space tech and AI have real costs for the Earth and that all the money invested in space projects is not spent on real earthly

problems such as war and poverty; the worry is also that the rich will be able to escape the Earth they destroy, whereas the rest of us have to stay on an increasingly un- livable planet (see, e.g., Zimmerman 2015). Like rockets and other tech, AI may become a tool for the "survival of the richest," as one commentator put it (Rushkoff 2018). Today some of this is already happening with other tech- nologies: in cities such as Delhi and Beijing, the majority of people are plagued by air pollution whereas the rich fly out, live in less polluted areas, or buy good air by using air purification technologies. Not everyone breathes the same air. Will AI contribute to such gaps between the rich and poor, leading to more stressful and unhealthy lives for some and better lives for others? Will AI alienate us from environmental problems? It seems an ethical requirement that AI should also make life on Earth better, preferably for all of us and taking into account that for human life we depend on the Earth. Some space narratives may hinder rather than help us to reach this goal.


Return to Earth: Toward Sustainable AI

Let me return to the very practical problem of priorities and the very real and present-day risks related to climate change. What should AI ethics and policy do in light of

these challenges? And when there are conflicts with the value of nonhuman lives, how can these be solved? Most people will agree that just handing over the control to an AI or escaping the Earth are not good solutions. But what is a good solution? Is there a solution? A more produc- tive answer to these questions leads us necessarily back to philosophical questions regarding how we as humans relate to technology and to our environment. It also leads us back to the technology chapter: what can AI and data science do for us, and what can we reasonably expect from AI?
   It is clear that AI can help us to tackle environmental problems. Consider climate change. AI would seem par- ticularly suited to helping us with such complex problems. AI can help us to study the problem, for example, by de- tecting patterns in environmental data that we cannot see, since the data are so abundant and so complex. It can also help us with solutions, for example, by helping us to deal with coordination complexity and implementing measures such as cuts in harmful emissions, as Floridi et al. (2018) have argued. More generally, AI could help by means of monitoring and modeling environmental systems and by enabling solutions such as smart energy grids and smart agriculture, as a World Economic Forum blog proposes (Herweijer 2018). Governments but also companies can take the lead here. For example, Google has already used AI to cut back on data center energy use.

   However, this does not necessarily "save the planet." AI can also cause problems and potentially make things worse. Consider again the negative environmental impact AI can have given the energy, infrastructures, and materi- als it relies on. We need to consider not only use but also production: the electricity may be produced in nonsus- tainable ways, and the production of AI devices uses en- ergy and raw materials and produces waste. Or consider the "self-nudging" proposed by Floridi et al: they suggest that AI may help us to behave in environmentally good ways by helping us to stick to our self-imposed choice. But this has its own ethical risks: it is not clear that it respects human autonomy and dignity, as the authors claim, and it might go in the direction of the benign AI that takes care of humans but destroys human freedom and contributes to the problem of the Anthropocene. There is at least the risk of new forms of paternalism and authoritarianism. Fur- thermore, using AI to tackle climate change may go hand in hand with a worldview that turns the world into a mere repository of data and a view of the human that reduces human intelligence to data processing-perhaps a rather inferior kind of data processing that requires enhance- ment by machines. Such views are unlikely to reshape our relation to the environment in a way that mitigates chal- lenges such as climate change and the problems indicated by the term Anthropocene.

   We also face a risk of techno-solutionism in the sense that proposals for using AI to tackle environmental prob- lems may assume that there can be a final solution to all problems, that technology alone can give the answer to our hardest questions, and that we can solve the problems entirely by use of human or artificial intelligence. But envi- ronmental problems cannot be solved entirely by means of technological-scientific intelligence; they are also linked to political and social problems that cannot be dealt with solely by means of technology. Environmental problems are always also human problems. And mathematics and its technological offspring are very helpful tools, but are limited when it comes understanding human problems and dealing with them. For example, values may conflict. AI doesn't necessarily help us answer the question about priorities, which is an important ethical and political ques- tion we should leave to humans to answer. And the hu- manities and social sciences teach us to be very cautious about "final" solutions.
   Furthermore, humans are not the only ones with prob- lems; nonhumans face difficulties as well, which are often neglected in discussions about the future of AI. Finally, the view that we should escape the Earth, or the worldview that everything is data that we humans can manipulate with the help of machines, could lead to a widening of the gap between the rich and poor and to large-scale forms of exploitation and violations of human dignity, as well as

threatening the lives of future generations by risking to destroy the conditions for life on our planet. We need to reflect more deeply on how to build sustainable societies and environments-we need human thinking.


Wanted: Intelligence and Wisdom

Yet the way humans think also has various sides. AI is related to one kind of human thinking and intelligence: the more abstract, cognitive kind. This kind of thinking has proved very successful, but it has its limitations and is not the only kind of thinking we can or should do. An- swering ethical and political questions concerning how to live, how to deal with our environment, and how we best relate to nonhuman living beings requires more than abstract human intelligence (e.g., arguments, theories, models) or AI pattern recognition. We need smart people and intelligent machines, but we also need intuitions and know-how that cannot be made entirely explicit, and we need to develop practical wisdom and virtue in response to concrete problems and situations and in order to decide our priorities. Such wisdom may well be informed by ab- stract cognitive processes and data analysis, but it is also based on embodied, relational, and situational experi- ences in the world, on dealing with other people, with ma- teriality, and with our natural environment. Our success

in tackling the big problems of our time will most likely depend on combinations of abstract intelligence-human and artificial-and concrete practical wisdom developed on the basis of concrete and situational human experience and practice-including our experience with technology. In whatever direction the further development of AI goes, the challenge to develop the latter kind of knowledge and learning is ours. Humans have to do it. AI is good at rec- ognizing patterns, but wisdom cannot be delegated to machines.

GLOSSARY 


Anthropocene
The alleged current geological epoch in which humanity has dramatically in- creased its power and effect on the Earth and its ecosystems, turning humans into a geological force.

Artificial intelligence (AI)
Intelligence displayed or simulated by technological means. Often it is as- sumed that "intelligence" in this definition means: considered intelligent by the standard of human intelligence, the sort of intelligent capacities and be- havior that humans display. The term can refer to the science or to the tech- nologies, for example, learning algorithms.

Bias
Discrimination against or in favor of particular individuals or groups. In the context of ethics and politics, the question arises whether a particular bias is unjust or unfair.

Data science
Interdisciplinary science that uses statistics, algorithms, and other methods to extract meaningful and useful patterns from data sets-sometimes known as "big data." Today, machine learning is often used in this field. Next to analysis of data, data science is also concerned with the capturing, preparation, and interpretation of data.

Deep learning
 A form of machine learning that uses neural networks with several layers of "neurons": simple interconnected processing units that interact.

Ethics by design
An approach to technology ethics and a key component of responsible innova- tion that aims to integrate ethics in the design and development stage of the technology. Sometimes formulated as "embedding values in design." Similar terms are "value-sensitive design" and "ethically aligned design."

Explainability
The ability to explain or be explained. In the context of ethics, it refers to the ability to explain to others why you have done something or why you have made a decision; this is part of what it means to be responsible.

Explainable AI
AI that can explain to humans its actions, decisions, or recommendations, or can provide sufficient information about how it came to its result.

General AI
 Human-like intelligence, which can be applied widely as opposed to narrow AI, which can only be applied to one particular problem or task. Also called "strong" AI as opposed to "weak" AI.

Machine learning
A machine or software that can automatically learn: not in the way humans learn, but based on a computational and statistical process. Feeding on data, learning algorithms can detect patterns or rules in the data and make predic- tions for future data.

Moral agency
The capacity for moral action, reasoning, judgment, and decision making, as opposed to merely having moral consequences.

Moral patiency
The moral standing of an entity in the sense of how that entity should be treated.

Moral responsibility
Can be used as a synonym for "being moral" and will then refer to produc- ing morally good consequences, adhering to moral principles, being virtu- ous, deserving praise, and so on-the emphasis depending on the normative theory assumed. One can also ask under what conditions one can be held responsible. Conditions for attributing moral responsibility are moral agency and knowledge. Relational approaches stress that one is always answerable to others.

Positive ethics
Ethics concerned with how we should live (together), based on a vision of the good life and the good society. Contrasts with negative ethics, which sets limits and says what we should not do.

Posthumanism
A range of beliefs that questions humanism, especially the central position of the human being, and expands the circle of ethical concern to nonhumans.

Responsible innovation
Approach to rendering innovation more ethically and societally responsible, which typically entails embedding ethics in design and taking into account the opinions and interests of stakeholders.

Superintelligence
The idea that machines will surpass human intelligence. Sometimes connected with the idea of an "intelligence explosion" caused by intelligent machines designing even more intelligent machines.

Sustainable AI
AI that enables and contributes to a sustainable way of living for humanity and does not destroy the ecosystems on Earth on which humans (and also many nonhumans) depend.

Symbolic AI
AI that relies on symbolic representations of higher cognitive tasks such as abstract reasoning and decision making. It may use a decision tree and take the form of an expert system that requires input from domain experts.

Technological singularity
The idea that there will be a moment in human history when an explosion of machine intelligence will bring such a dramatic change to our civilization that we will no longer understand what is happening.

Transhumanism
The belief that humans should enhance themselves by means of advanced technologies and in this way transform the human condition: humanity should move to a next stage. It is also an international movement.



GlOSSarY	205

Trustworthy AI
AI that can be trusted by humans. Conditions for such trust can refer to (other) ethical principles such as human dignity, respect for human rights, and so on, and/or to social and technical factors that influence whether people will want to use the technology. The use of the term "trust" with regard to technologies is controversial.

NOTES 


Chapter 1
1. See https://www.youtube.com/watch?v=D5VN56jQMWM.
2. See the case of Paul Zilly as told by Fry (2018, 71-72). More details in Julia Angwin, Jeff Larson, Surya Mattu and Lauren Kirchner, "Machine Bias," ProPublica, May 23, 2016, https://www.propublica.org/article/machine
-bias-risk-assessments-in-criminal-sentencing.
3. For example, in 2016 a local police zone in Belgium started using predic- tive policing software to predict burglaries and vehicle theft (AlgorithmWatch 2019, 44).
4. BuzzFeedVideo, "You Won't Believe What Obama Says in This Video!" https://www.youtube.com/watch?v=cQ54GDm1eL0&fbclid=IwAR1oD0Alop EZa00XHo3WNcey_qNnNqTsvHN_aZsNb0d2t9cmsDbm9oCfX8A.

Chapter 2
1. Some talk of taming or domesticating AI, although the analogy with wild animals is problematic, if only because in contrast to the "wild" AI some imag- ine, animals are limited by their natural faculties and can be trained and devel- oped only up to some point (Turner 2019).
2. It is often suggested that Mary Shelley must have been influenced by her parents, who discussed politics, philosophy, and literature, but also science, and by her partner Percy Bysshe Shelley, who was an amateur scientist espe- cially interested in electricity.

Chapter 3
1. Dreyfus was influenced by Edmund Husserl, Martin Heidegger, and Maurice Merleau-Ponty.

Chapter 4
1. A real-world case of this was the robot dog Spot who was kicked by its devel- opers to test it, something that met with surprisingly empathetic responses: https://www.youtube.com/watch?v=aR5Z6AoMh6U.

Chapter 5
1. See https://www.humanbrainproject.eu/en/.

2. See, for example, the European Commission's AI High Level Expert Group's (2018) definition of AI.

Chapter 6
1. See http://tylervigen.com/spurious-correlations.
2. Concrete examples such as Facebook, Walmart, American Express, Hello Barbie, and BMW are drawn from Marr (2018).

Chapter 8
1. One could ask, however, if decisions made by AIs really count as decisions, and if so, if there is a difference in the kind of decisions we delegate or should delegate to AIs. In this sense, the problem regarding responsibility of or for AI raises the very question of what a decision is. The problem also connects with issues about delegation: we delegate decisions to machines. But what does this delegation entail in terms of responsibility?
2. Indeed, this case is more complicated since one could argue that the dele- gate is then still responsible for that particular task-at least to some extent- and it may not be clear how the responsibility is distributed in such cases.
3. Note that this was and is not always the case; as Turner (2019) reminds us, there are cases of animals being punished.

Chapter 9
1. Thanks to Bill Price for the thought experiment.

Chapter 10
1. See https://www.acrai.at/en/.
2. The resolution can be found here: http://www.europarl.europa.eu/doceo/ document/TA-8-2017-0051_EN.html?redirect#title1.
3. See   https://www.scu.edu/ethics-in-technology-practice/conceptual
-frameworks/.
4. See https://www.partnershiponai.org/.
5. See https://www.blog.google/technology/ai/ai-principles/.
6. See https://www.microsoft.com/en-us/ai/our-approach-to-ai.
7. See https://www.accenture.com/t20160629T012639Z  w  /us-en/
_acnmedia/PDF-24/Accenture-Universal-Principles-Data-Ethics.pdf.
8. See   https://www.businessinsider.de/apple-ceo-tim-cook-on-privacy-the
-free-market-is-not-working-regulations-2018-11?r=US&IR=T.
9. See https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id
=201720180SB1001.

10. See https://www.stopkillerrobots.org/.
11. See https://futureoflife.org/ai-principles/.
12. Consider people such as Batya Friedman and Helen Nissenbaum in the United States, and later Jeroen van den Hoven and others in the Netherlands, who have been championing the ethical design of technology for some time.
13. See   https://www.tuev-sued.de/company/press/press-archive/tuv-sud
-and-dfki-to-develop-tuv-for-artificial-intelligence.

Chapter 11
1. See  https://ec.europa.eu/digital-single-market/en/european-ai-alliance.

Chapter 12
1. See https://hai.stanford.edu/ and https://hcai.mit.edu.
2. See https://sustainabledevelopment.un.org/post2015/transforming ourworld.
3. See   https://www.theguardian.com/science/2018/feb/07/space-oddity
-elon-musk-spacex-car-mars-falcon-heavy.
4. See  https://cosmosmagazine.com/space/why-we-need-to-send-artists
-into-space.




















NOTES	209



REFERENCES 


Accessnow. 2018. "Mapping Regulatory Proposals for Artificial Intelligence in Europe." https://www.accessnow.org/cms/assets/uploads/2018/11/ mapping_regulatory_proposals_for_AI_in_EU.pdf.
ACRAI (Austria Council on Robotics and Artificial Intelligence). 2018. "Die Zukunft Österreichs mit Robotik und Künstlicher Intelligenz positive ge- stalten: White paper des Österreichischen Rats für Robotik und Künstliche Intelligenz."
"Algorithm and Blues." 2016. Nature 537:449.
AlgorithmWatch. 2019. "Automating Society: Taking Stock of Automated Decision Making in the EU." A report by AlgorithmWatch in cooperation with Bertelsmann Stiftung. January 2019. Berlin: AW AlgorithmWatch GmbH. http://www.algorithmwatch.org/automating-society.
Alpaydin, Ethem. 2016. Machine Learning. Cambridge, MA: MIT Press.
Anderson, Michael and Susan Anderson. 2011. "General Introduction." In Ma- chine Ethics, edited by Michael Anderson and Susan Anderson, 1-4. Cambridge: Cambridge University Press.
Arendt, Hannah. 1958. The Human Condition. Chicago: Chicago University Press.
Arkoudas, Konstantine, and Selmer Bringsjord. 2014. "Philosophical Foun- dations." In The Cambridge Handbook of Artificial Intelligence, edited by Keith Frankish and William M. Ramsey. Cambridge: Cambridge University Press.
Armstrong, Stuart. 2014. Smarter Than Us: The Rise of Machine Intelligence. Berkeley: Machine Intelligence Research Institute.
Awad, Edmond, Sohan Dsouza, Richard Kim, Jonathan Schulz, Joseph Hen- rich, Azim Shariff, Jean-François Bonnefon, and Iyad Rahwan. 2018. "The Moral Machine Experiment." Nature 563:59-64.
Bacon, Francis. 1964. "The Refutation of Philosophies." In The Philosophy of Francis Bacon, edited by Benjamin Farrington, 103-132. Chicago: University of Chicago Press.

Boddington, Paula. 2016. "The Distinctiveness of AI Ethics, and Implications for Ethical Codes." Paper presented at the workshop Ethics for Artificial In- telligence, July 9, 2016, IJCAI-16, New York. https://www.cs.ox.ac.uk/efai/ 2016/11/02/the-distinctiveness-of-ai-ethics-and-implications-for-ethical
-codes/.
Boddington, Paula. 2017. Towards a Code of Ethics for Artificial Intelligence. Cham: Springer.
Boden, Margaret A. 2016. AI: Its Nature and Future. Oxford: Oxford University Press.
Borowiec, Steven. 2016. "AlphaGo Seals 4-1 Victory Over Go Grandmaster Lee Sedol." Guardian, March 15. https://www.theguardian.com/technology/ 2016/mar/15/googles-alphago-seals-4-1-victory-over-grandmaster-lee
-sedol.
Bostrom, Nick. 2014. Superintelligence. Oxford: Oxford University Press.
Brynjolfsson, Erik, and Andrew McAfee. 2014. The Second Machine Age. New York: W. W. Norton.
Bryson, Joanna. 2010. "Robots Should Be Slaves." In Close Engagements with Artificial Companions: Key Social, Psychological, Ethical and Design Issues, edited by Yorick Wilks, 63-74. Amsterdam: John Benjamins.
Bryson, Joanna. 2018. "AI & Global Governance: No One Should Trust AI." United Nations University Centre for Policy Research. AI & Global Governance, November 13, 2018. https://cpr.unu.edu/ai-global-governance-no-one-should
-trust-ai.html.
Bryson, Joanna, Mihailis E. Diamantis, and Thomas D. Grant. 2017. "Of, For, and By the People: The Legal Lacuna of Synthetic Persons." Artificial Intelligence & Law 25, no. 3: 273-291.
Caliskan, Aylin, Joanna J. Bryson, and Arvind Narayanan. 2017. "Semantics Derived Automatically from Language Corpora Contain Human-like Biases." Science 356:183-186.
Castelvecchi, Davide. 2016. "Can We Open the Black Box of AI?" Nature 538, no. 7623: 21-23.
CDT (Centre for Democracy & Technology) 2018. "Digital Decisions." https:// cdt.org/issue/privacy-data/digital-decisions/.

Coeckelbergh, Mark. 2010. "Moral Appearances: Emotions, Robots, and Hu- man Morality." Ethics and Information Technology 12, no. 3: 235-241.
Coeckelbergh, Mark. 2011. "You, Robot: On the Linguistic Construction of Artificial Others." AI & Society 26, no. 1: 61-69.
Coeckelbergh, Mark. 2012. Growing Moral Relations: Critique of Moral Status Ascription. New York: Palgrave Macmillan.
Coeckelbergh, Mark. 2013. Human Being @ Risk: Enhancement, Technology, and the Evaluation of Vulnerability Transformations. Cham: Springer.
Coeckelbergh, Mark. 2017. New Romantic Cyborgs. Cambridge, MA: MIT Press. Crawford, Kate, and Ryan Calo. 2016. "There Is a Blind Spot in AI Research."
Nature 538:311-313.
Crutzen, Paul J. 2006. "The 'Anthropocene.'" In Earth System Science in the Anthropocene edited by Eckart Ehlers and Thomas Krafft, 13-18. Cham: Springer.
Darling, Kate, Palash Nandy, and Cynthia Breazeal. 2015. "Empathic Concern and the Effect of Stories in Human-Robot Interaction." In 2015 24th IEEE Inter- national Symposium on Robot and Human Interactive Communication (RO-MAN), 770-775. New York: IEEE.
Dennett, Daniel C. 1997. "Consciousness in Human and Robot Minds. In Cog- nition, Computation, and Consciousness, edited by Masao Ito, Yasushi Miyashita, and Edmund T. Rolls, 17-29. New York: Oxford University Press.
Digital Europe. 2018. "Recommendations on AI Policy: Towards a Sustainable and Innovation-friendly Approach." Digitaleurope.org, November 7, 2018.
Dignum, Virginia, Matteo Baldoni, Cristina Baroglio, Maruiyio Caon, Raja Chatila, Louise Dennis, Gonzalo Génova, et al. 2018. "Ethics by Design: Ne- cessity or Curse?" Association for the Advancement of Artificial Intelligence. http://www.aies-conference.com/2018/contents/papers/main/AIES_2018
_paper_68.pdf.
Dowd, Maureen. 2017. "Elon Musk's Billion-Dollar Crusade to Stop the
A.I. Apocalypse." Vanity Fair, March 26, 2017. https://www.vanityfair.com/ news/2017/03/elon-musk-billion-dollar-crusade-to-stop-ai-space-x.
Dreyfus, Hubert L. 1972. What Computers Can't Do. New York: HarperCollins.

Druga, Stefania and Randi Williams. 2017. "Kids, AI Devices, and Intelligent Toys." MIT Media Lab, June 6, 2017. https://www.media.mit.edu/posts/kids
-ai-devices/f.
European Commission. 2018. "Ethics and Data Protection." http://ec.europa
.eu/research/participants/data/ref/h2020/grants_manual/hi/ethics/h2020
_hi_ethics-data-protection_en.pdf.
European Commission Directorate-General of Employment, Social Affairs and Inclusion. 2018. "Employment and Social Developments in Europe 2018." Luxembourg: Publications Office of the European Union. http://ec.europa.eu/ social/main.jsp?catId=738&langId=en&pubId=8110.
European Commission AI HLEG (High-Level Expert Group on Artificial Intel- ligence). 2018. "Draft Ethics Guidelines for Trustworthy AI: Working Docu- ment for Stakeholders." Working document, December 18, 2018. Brussels: European Commission. https://ec.europa.eu/digital-single-market/en/news/ draft-ethics-guidelines-trustworthy-ai.
European Commission AI HLEG (High-Level Expert Group on Artificial Intelligence). 2019. "Ethics Guidelines for Trustworthy AI." April 8, 2019. Brussels: European Commission. https://ec.europa.eu/futurium/en/ai-alliance
-consultation/guidelines#Top.
 EGE (European Group on Ethics in Science and New Technologies). 2018. "Statement on Artificial Intelligence, Robotics and 'Autonomous' Systems." Brussels: European Commission.
European Parliament and the Council of the European Union. 2016. "General Data Protection Regulation (GDPR)." https://eur-lex.europa.eu/legal-content/ EN/TXT/?uri=celex%3A32016R0679.
Executive Office of the President, National Science and Technology Coun- cil Committee on Technology. 2016. "Preparing for the Future of Artificial Intelligence." Washington, DC: Office of Science and Technology Policy (OSTP).
 Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena. 2018. "AI4People-An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations." Minds and Machines 28, no. 4: 689-707.

Floridi, Luciano, and J. W. Sanders. 2004. "On the Morality of Artificial Agents."
Minds and Machines 14, no. 3: 349-379.
Ford, Martin. 2015. Rise of the Robots: Technology and the Threat of a Jobless Future. New York: Basic Books.
Frankish, Keith, and William M. Ramsey. 2014. "Introduction." In The Cam- bridge Handbook of Artificial Intelligence, edited by Keith Frankish and William
M. Ramsey, 1-14. Cambridge: Cambridge University Press.
Frey, Carl Benedikt, and Michael A. Osborne. 2013. "The Future of Employ- ment: How Susceptible Are Jobs to Computerisation?" Working paper, Oxford Martin Programme on Technology and Employment, University of Oxford.
Fry, Hannah. 2018. Hello World: Being Human in the Age of Algorithms. New York: W. W. Norton.
Fuchs, Christian. 2014. Digital Labour and Karl Marx. New York: Routledge.
Goebel, Randy, Ajay Chander, Katharina Holzinger, Freddy Lecue, Zeynep Akata, Simone Stumpf, Peter Kieseberg, and Andreas Holzinger. 2018. "Ex- plainable AI: The New 42?" Paper presented at the CD-MAKE 2018, Hamburg, Germany, August 2018.
Gunkel, David. 2012. The Machine Question. Cambridge, MA: MIT Press.
Gunkel, David. 2018. "The Other Question: Can and Should Robots Have Rights?" Ethics and Information Technology 20:87-99.
Harari, Yuval Noah. 2015. Homo Deus: A Brief History of Tomorrow. London: Hervill Secker.
Haraway, Donna. 1991. "A Cyborg Manifesto: Science, Technology, and Socialist-Feminism in the Late Twentieth Century." In Simians, Cyborgs and Women: The Reinvention of Nature, 149-181. New York: Routledge.
Haraway, Donna. 2015. "Anthropocene, Capitalocene, Plantationocene, Ch- thulucene: Making Kin." Environmental Humanities 6:159-165.
Herweijer, Celine. 2018. "8 Ways AI Can Help Save the Planet." World Eco- nomic Forum, January 24, 2018. https://www.weforum.org/agenda/2018/01/ 8-ways-ai-can-help-save-the-planet/.
House of Commons. 2018. "Algorithms in Decision-Making." Fourth Report of Session 2017-19, HC351. May 23, 2018.

ICDPPC (International Conference of Data Protection and Privacy Com- missioners). 2018. "Declaration on Ethics and Data Protection in Artificial Intelligence." https://icdppc.org/wp-content/uploads/2018/10/20180922
_ICDPPC-40th_AI-Declaration_ADOPTED.pdf.
IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. 2017. "Ethically Aligned Design: A Vision for Prioritizing Human Well-Being with Autonomous and Intelligent Systems," Version 2. IEEE. http://standards
.Ieee.org/develop/indconn/ec/autonomous_systems.html.
Ihde, Don. 1990. Technology and the Lifeworld: From Garden to Earth. Blooming- ton: Indiana University Press.
Jansen, Philip, Stearns Broadhead, Rowena Rodrigues, David Wright, Philp Brey, Alice Fox, and Ning Wang. 2018. "State-of-the-Art Review." Draft of the D4.1 deliverable submitted to the European Commission on April 13, 2018. A report for The SIENNA Project, an EU H2020 research and innovation program under grant agreement no. 741716.
Johnson, Deborah G. 2006. "Computer Systems: Moral Entities but not Moral Agents." Ethics and Information Technology 8, no. 4: 195-204.
Kant, Immanuel. 1997. Lectures on Ethics. Edited by Peter Heath and J. B. Schneewind. Translated by Peter Heath. Cambridge: Cambridge University Press.
Kelleher, John D., and Brendan Tierney. 2018. Data Science. Cambridge, MA: MIT Press.
Kharpal, Arjun. 2017. "Stephen Hawking Says A.I. Could Be 'Worst Event in the History of Our Civilization.'" CNBC. November 6, 2017. https://www.cnbc
.com/2017/11/06/stephen-hawking-ai-could-be-worst-event-in-civilization
.html.
Kubrick, Stanley, dir. 1968. 2001: A Space Odyssey. Beverly Hills, CA: Metro-Goldwyn-Mayer.
Kurzweil, Ray. 2005. The Singularity Is Near. New York: Viking.
Leta Jones, Meg. 2018. "Silencing Bad Bots: Global, Legal and Political Ques- tions for Mean Machine Communication." Communication Law and Policy 23, no. 2: 159-195.

Lin, Patrick, Keith Abney, and George Bekey. 2011. "Robot Ethics: Mapping the Issues for a Mechanized World." Artificial Intelligence 175:942-949.
MacIntyre, Lee C. 2018. Post-Truth. Cambridge, MA: MIT Press. Marcuse, Herbert. 1991. One-Dimensional Man. Boston: Beacon Press.
Marr, Bernard. 2018. "27 Incredible Examples of AI and Machine Learning in Practice." Forbes, April 30. https://www.forbes.com/sites/bernardmarr/ 2018/04/30/27-incredible-examples-of-ai-and-machine-learning-in-practice/ #6b37edf27502.
McAfee, Andrew, and Erik Brynjolfsson. 2017. Machine, Platform, Crowd: Harnessing Our Digital Future. New York: W. W. Norton.
Miller, Tim. 2018. "Explanation in Artificial Intelligence: Insights from the Social Sciences." arXiv, August 15. https://arxiv.org/pdf/1706.07269.pdf.
Mouffe, Chantal. 2013. Agonistics: Thinking the World Politically. London: Verso.
Nemitz, Paul Friedrich, 2018. "Constitutional Democracy and Technology in the Age of Artificial Intelligence." Philosophical Transactions of the Royal Society A 376, no. 2133. https://doi.org/10.1098/rsta.2018.0089.
Noble, David F. 1997. The Religion of Technology. New York: Penguin Books.
Reijers, Wessel, David Wright, Philip Brey, Karsten Weber, Rowena Rodrigues, Declan O' Sullivan, and Bert Gordijn. 2018. "Methods for Practising Ethics in Research and Innovation: A Literature Review, Critical Analysis and Recom- mendation." Science and Engineering Ethics 24, no. 5: 1437-1481.
Royal Society, the. 2018. "Portrayals and Perceptions of AI and Why They Matter." December 11, 2018. https://royalsociety.org/topics-policy/projects/ ai-narratives/.
Rushkoff, Douglas. 2018. "Survival of the Richest." Medium, July 5. https:// medium.com/s/futurehuman/survival-of-the-richest-9ef6cddd0cc1.
Samek, Wojciech, Thomas Wiegand, and Klaus-Robert Müller. 2017. "Explain- able Artificial Intelligence: Understanding, Visualizing and Interpreting Deep Learning Models." https://arxiv.org/pdf/1708.08296.pdf.
Schwab, Katharine. 2018. "The Exploitation, Injustice, and Waste Powering Our AI." Fast Company. September 18, 2018. https://www.fastcompany.com/ 90237802/the-exploitation-injustice-and-waste-powering-our-ai.

Seseri, Rudina. 2018. "The Problem with 'Explainable AI.'" Tech Crunch. June 14, 2018. https://techcrunch.com/2018/06/14/the-problem-with-explainable
-ai/?guccounter=1
Searle, John. R. 1980. "Minds, Brains, and Programs." Behavioral and Brain Sciences 3, no. 3: 417-457.
Shanahan, Murray. 2015. The Technological Singularity. Cambridge, MA: The MIT Press.
Siau, Keng, and Weiyu Wang. 2018. "Building Trust in Artificial Intelligence, Machine Learning, and Robotics." Cutter Business Technology Journal 32, no. 2: 46-53.
State Council of China. 2017. "New Generation Artificial Intelligence Develop- ment Plan." Translated by Flora Sapio, Weiming Chen, and Adrian Lo. https:// flia.org/notice-state-council-issuing-new-generation-artificial-intelligence
-development-plan/.
Stoica, Ion. 2017. "A Berkeley View of Systems Challenges for AI." Techni- cal Report No. UCB/EECS-2017-159. http://www2.eecs.berkeley.edu/Pubs/ TechRpts/2017/EECS-2017.
Sullins, John. 2006. "When Is a Robot a Moral Agent?" International Review of Information Ethics 6: 23-30.
Surur. 2017. "Microsoft Aims to Lie to Their AI to Reduce Sexist Bias." Au- gust 25, 2017. https://mspoweruser.com/microsoft-aims-lie-ai-reduce-sexist
-bias/.
Suzuki, Yutaka, Lisa Galli, Ayaka Ikeda, Shoji Itakura, and Michiteru Kitazaki. 2015. "Measuring Empathy for Human and Robot Hand Pain Using Electro- encephalography." Scientific Reports 5, article number 15924. https://www
.nature.com/articles/srep15924
Tegmark, Max. 2017. Life 3.0: Being Human in the Age of Artificial Intelligence. Allen Lane/Penguin Books.
Turkle, Sherry. 2011. Alone Together: Why We Expect More from Technology and Less from Each Other. New York: Basic Books.
Turner, Jacob. 2019. Robot Rules: Regulating Artificial Intelligence. Cham: Pal- grave Macmillan.

Université de Montréal. 2017. "Montréal Declaration Responsible AI." https://www.montrealdeclaration-responsibleai.com/the-declaration.
Vallor, Shannon. 2016. Technology and the Virtues. New York: Oxford University Press.
Vigen, Tyler. 2015. Spurious Correlations. New York: Hachette Books.
Villani, Cédric. 2018. For a Meaningful Artificial Intelligence: Towards a French and European Strategy. Composition of a parliamentary mission from Septem- ber 8, 2017, to March 8, 2018, and assigned by the Prime Minister of France, Èdouard Philippe.
Von Schomberg, René, ed. 2011. "Towards Responsible Research and In- novation in the Information and Communication Technologies and Secu- rity Technologies Fields." A report from the European Commission Services. Luxembourg: Publications Office of the European Union.
Vu, Mai-Anh T., Tülay Adali, Demba Ba, György Buzsáki, David Carlson, Katherine Heller, et al. 2018. "A Shared Vision for Machine Learning in Neuro- science. " Journal of Neuroscience 38, no. 7: 1601-607.
Wachter, Sandra, Brent Mittelstadt, and Luciano Floridi. 2017. "Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation." International Data Privacy Law, 2017. http:// dx.doi.org/10.2139/ssrn.2903469.
Wallach, Wendell and Colin Allen. 2009. Moral Machines: Teaching Robots Right from Wrong. Oxford: Oxford University Press.
Weld, Daniel S. and Gagan Bansal. 2018. "The Challenge of Crafting Intelligible Intelligence." https://arxiv.org/pdf/1803.04263.pdf.
Winfield, Alan F.T. and Marina Jirotka. 2017. "The Case for an Ethical Black Box." In Towards Autonomous Robotic Systems, edited by Yang Gao, Saber Fallah, Yaochu Jin, and Constantina Lekakou (proceedings of TAROS 2017, Guildford, UK, July 2017), 262-273. Cham: Springer.
Winikoff, Michael. 2018. "Towards Trusting Autonomous Systems." In Engi- neering Multi-Agent Systems, edited by Amal El Fallah Seghrouchni, Alessandro Ricci, and Son Trao, 3-20. Cham: Springer.

Yampolskiy, Roman V. 2013. "Artificial Intelligence Safety Engineering: Why Machine Ethics Is a Wrong Approach." In Philosophy and Theory of Artificial Intelligence edited by Vincent C. Müller, 289-296. Cham: Springer.
Yeung, Karen. 2018. "A Study of the Implications of Advanced Digital Technol- ogies (Including AI Systems) for the Concept of Responsibility within a Human Rights Framework." A study commissioned for the Council of Europe Commit- tee of experts on human rights dimensions of automated data processing and different forms of artificial intelligence. MSI-AUT (2018)05.
Zimmerman, Jess. 2015. "What If the Mega-Rich Just Want Rocket Ships to Escape the Earth They Destroy?" Guardian, September 16, 2015. https:// www.theguardian.com/commentisfree/2015/sep/16/mega-rich-rocket-ships
-escape-earth.
Zou, James, and Londa Schiebinger. 2018. "Design AI So That It's Fair." Nature
559:324-326.

FURTHER READING 


Alpaydin, Ethem. 2016. Machine Learning. Cambridge, MA: MIT Press.
Arendt, Hannah. 1958. The Human Condition. Chicago: Chicago University Press.
Aristotle. 2002. Nichomachean Ethics. Translated by Christopher Rowe, with commentary by Sarah Broadie. Oxford: Oxford University Press.
Boddington, Paula. 2017. Towards a Code of Ethics for Artificial Intelligence. Cham: Springer.
Boden, Margaret A. 2016. AI: Its Nature and Future. Oxford: Oxford University Press.
Bostrom, Nick. 2014. Superintelligence. Oxford: Oxford University Press.
Brynjolfsson, Erik, and Andrew McAfee. 2014. The Second Machine Age. New York: W.W. Norton.
Coeckelbergh, Mark. 2012. Growing Moral Relations: Critique of Moral Status Ascription. New York: Palgrave Macmillan.
Crutzen, Paul J. 2006. "The 'Anthropocene.'" In Earth System Science in the Anthropocene, edited by Eckart Ehlers and Thomas Krafft, 13-18. Cham: Springer.
Dignum, Virginia, Matteo Baldoni, Cristina Baroglio, Maruiyio Caon, Raja Chatila, Louise Dennis, Gonzalo Génova, et al. 2018. "Ethics by Design: Ne- cessity or Curse?" Association for the Advancement of Artificial Intelligence. http://www.aies-conference.com/2018/contents/papers/main/AIES_2018
_paper_68.pdf.
Dreyfus, Hubert L. 1972. What Computers Can't Do. New York: Harper & Row.
Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chaz- erand, Virginia Dignum, Christoph Luetge, Robert Madelin, Ugo Pagallo, Francesca Rossi, Burkhard Schafer, Peggy Valcke, and Effy Vayena. 2018.

"AI4People-An Ethical Framework for a Good AI Society: Opportunities, Risks, Principles, and Recommendations." Minds and Machines 28, no. 4: 689- 707.
Frankish, Keith, and William M. Ramsey, eds. 2014. The Cambridge Handbook of Artificial Intelligence. Cambridge: Cambridge University Press.
European Commission AI HLEG (High-Level Expert Group on Artificial Intelligence). 2019. "Ethics Guidelines for Trustworthy AI." April 8, 2019. Brussels: European Commission. https://ec.europa.eu/futurium/en/ai-alliance
-consultation/guidelines#Top.
Fry, Hannah. 2018. Hello World: Being Human in the Age of Algorithms. New York and London: W. W. Norton.
Fuchs, Christian. 2014. Digital Labour and Karl Marx. New York: Routledge. Gunkel, David. 2012. The Machine Question. Cambridge, MA: MIT Press.
Harari, Yuval Noah. 2015. Homo Deus: A Brief History of Tomorrow. London: Hervill Secker.
Haraway, Donna. 1991. "A Cyborg Manifesto: Science, Technology, and Socialist-Feminism in the Late Twentieth Century." In Simians, Cyborgs and Women: The Reinvention of Nature, 149-181. New York: Routledge.
IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems. 2017. "Ethically Aligned Design: A Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems," Version 2. IEEE, 2017. http:// standards.Ieee.org/develop/indconn/ec/autonomous_systems.html.
Kelleher, John D. and Brendan Tierney. 2018. Data Science. Cambridge, MA: MIT Press.
Nemitz, Paul Friedrich, 2018. "Constitutional Democracy and Technology in the Age of Artificial Intelligence." Philosophical Transactions of the Royal Society A 376, no. 2133. https://doi.org/10.1098/rsta.2018.0089
Noble, David F. 1997. The Religion of Technology. New York: Penguin Books.
Reijers, Wessel, David Wright, Philip Brey, Karsten Weber, Rowena Rodrigues, Declan O'Sullivan, and Bert Gordijn. 2018. "Methods for Practising Ethics in Research and Innovation: A Literature Review, Critical Analysis and Recom- mendation." Science and Engineering Ethics 24, no. 5: 1437-1481.

Shelley, Mary. 2017. Frankenstein. Annotated edition. Edited by David H. Gus- ton, Ed Finn, and Jason Scott Robert. Cambridge, MA: MIT Press.
Turkle, Sherry. 2011. Alone Together: Why We Expect More from Technology and Less from Each Other. New York: Basic Books.
Wallach, Wendell, and Colin Allen. 2009. Moral Machines: Teaching Robots Right from Wrong. Oxford: Oxford University Press.






























FUrThEr rEa DiNG	223



INDEX 




Abstracting, 60, 196
Algorithms, 2-3, 6-7, 12, 50, 64, 68,
70, 73, 83, 85, 89, 100, 112, 120,
130-132, 136, 148, 151-152,
160
Alienation, 195-196
AlphaGo, 1, 72, 78
Animals, 42, 47, 59, 65, 115, 155
Answerability, 115
Anthropocene, 183, 189, 191-195,
199
Antikythera mechanism, 18
Arendt, Hannah, 180, 195
Aristotle, 111, 115, 142
Artificial intelligence, 1, 3, 10, 16,
200
definition, 63-64, 68, 147
fear of, 2, 19, 21, 26, 150, 187,
191
ethical issues, 5, 60, 62, 94
expert systems, 71, 73-74,
84
explainable, 109, 119-120, 122,
151, 161, 164
general, 15, 31, 34, 36, 38-39, 52,
61, 63, 66, 94, 150, 192
Good Old-Fashioned (GOFAI), 72
history, 17, 21-22, 25, 32, 65
human vs. artificial, 2 invisible, 78
meaningful, 151, 180 policy (see Policy)




psychopath, 52 responsibility for (see
Responsibility)
sustainable, 139, 192, 197-201
symbolic, 32, 35, 45, 60, 71,
73
trustworthy, 119, 152, 164
winter, 179
Artificial life, 73

Basic income, 140
Bias, 6-7, 10, 61, 75, 91, 119,
125-147, 150-153, 159-160
Big data, 3, 68, 73-75, 85-86, 92,
94, 97-98, 192, 194
Boden, Margaret, 15, 36, 38-39,
64-65, 67, 83, 90
Bostrom, Nick, 12-16, 38, 141
Brooks, Rodney, 73
Bryson, Joanna, 55, 119, 130,
159

Cartesian, 35
Chatbot, 7, 69, 102-103
China, 65, 129, 156-157
Chinese room argument, 36 Churchland, Paul, 34
Climate change, 172, 186, 189-192,
194, 196-199
COMPAS, 5-6, 127
Complexity, 198
Criminal law, 155
Crutzen, Paul, 191

Dartmouth workshop, 66
Data, 1, 3, 6, 12, 24, 45, 63, 67-68,
71, 73-75, 83-95, 98-102, 114,
128-131, 134-136, 159, 162,
165, 193, 198-201. See also Big data
Data protection, 97-98, 151-153
Data repurposing, 99, 171
Data science, 63, 67, 83, 87, 89, 92,
94, 114, 125, 129, 179, 198
Decision tree, 71-72, 116, 125
Deep Blue, 1
Deep learning, 72, 74, 93, 117
DeepMind, 1, 72, 93, 159
Dennett, Daniel, 34-35, 38-39
Discrimination, 6-7, 75, 125,
132-136, 151-152, 157, 160,
186
Doomsday, 26
Dreyfus, Hubert, 32, 34-36, 43,
60, 71

Education, 3, 74, 94, 121, 132, 146,
164, 179-180
Empathy, 55
Energy, 2, 76, 106, 174, 186-187,
192, 198-199
Engineering, 46, 64, 68, 152, 157,
165, 178, 193
Enlightenment, 38-40, 42-43
Environment, 67, 70, 73, 75-76, 80,
87, 98, 103, 105, 139, 151, 171,
176, 179-180, 183-187, 192,
195, 198-201
Escapism, 195
Eschatology, 25
Ethics, 5, 7, 10, 17, 19, 31, 40, 42,
45, 48, 50, 56, 60-62, 66, 70, 80,
97-98, 105, 111, 120, 136, 139,

142-148, 150, 152-154,
156-165, 167-170, 172-177,
179-181, 183-184, 187, 189,
197
bottom-up, 168, 170 ethical issues (see Artificial
 intelligence: ethical issues) ethically aligned design, 161 ethics by design, 162-165, 170,
172
human-centric, 152, 183-184
non-Western, 177
of the good life, 142, 176-177
participatory, 151, 170-171, 177
positive, 174-177
practice-oriented, 168, 170,
173-174, 179, 202
proactive, 167
European Union (EU), 150, 153-154
Ex Machina (film), 21 Existential risks, 13
Expert systems, 71, 73-74, 84, 125
Explainability, 115-116, 118-121,
123, 153, 160
Exploitation, 46, 99-100, 102,
140-141, 171, 200

Facebook, 3, 78, 93, 100, 159
Fairness, 131-132, 136-137, 145,
152, 159
Fake news, 103-104
Fake videos, 75
Floridi, Luciano, 53, 154, 157, 198,
199
Frankenstein, 17-22, 28, 61

General Data Protection Regulation (GDPR), 153

Gilgamesh epic, 25
Gnosticism, 24
Golem, 18-19, 21
Google, 1, 3, 16, 134, 150, 159,
198
Google Duplex, 3

Harari, Yuval, 12, 14, 193
Haraway, Donna, 42, 192
Hawking, Stephen, 21
Hello Barbie, 70, 93
High-Level Expert Group (HLEG) on AI, 153, 165, 183
Homo deus, 14, 22, 193
Human dignity, 40, 152-153,
200
Humanism, 38, 40, 42, 46
Humanities, 38, 40-42, 75,
178-179, 192, 200

Ihde, Don, 43
Immanence, 22
Immortality, 14, 24-25
Infrastructures, 45, 69, 106, 162,
171, 199
Institute of Electrical and Electronics Engineers (IEEE), 150, 160-161
Intelligence, 11-13, 15-16, 29,
31-32, 38, 43, 64-68, 70, 161,
199, 200-202
artificial (see Artificial intelligence) kinds of, 43, 64
Intelligence explosion, 11-12
Interdisciplinarity, 177-178
Interpretation, 89, 130, 152, 160

Jobs, 8, 137-141
Johnson, Deborah, 51

Justice, 94, 120, 128, 130-132,
136-7, 139, 145, 147, 151-153,
157
blind vs. positive discrimination, 132, 136
mirror view, 134

Kant, Immanuel, 56
Know-how, 32, 201
Knowledge, 8, 32, 36, 38, 60-61, 68,
71, 86, 89, 115, 118, 121-122,
144, 178, 180, 202
kinds of, 122, 144, 202
Kurzweil, Ray, 11, 13, 16

Machine learning, 1, 3, 34, 61, 63,
68, 72-73, 83-94, 97, 114,
117-119, 125, 130-131, 150,
161, 174
types of (supervised vs. unsupervised, reinforcement), 85-86
Manipulation, 32, 34, 41, 45,
99-103, 119, 153, 157
McCarthy, John, 66
Meaning of life, 137-139, 141
Mindlessness, 180
Models and modeling, 24, 64, 68,
71, 73, 84, 89, 91-92, 120, 134,
142, 198
Modernity, 19, 38-39, 41, 61
Monotheism, 22, 28
Morality, 51-54, 61, 164
functional, 52, 164
Moral agency, 48, 50-54
Moral intuitions, 56
Moral machines, 51, 162,
172-173
Moral patiency, 48, 54-55

Moral status, 29, 47-48, 55-56,
58-60, 156, 164, 183
moral agency vs. moral patiency, 48, 50-55
relational approach, 59, 115-116,
201
Musk, Elon, 11, 16, 21, 194-195

Negligence, 114, 155
Neural networks, 72, 93, 117, 129,
161
Nonhumans, 42-43, 47, 60-61, 176,
184, 200
nonhuman values and interests, 184

Pattern recognition, 83, 201
Planet, 12, 176, 186, 189, 191-195,
197, 199, 201
Platonic, 24, 28, 177, 195
Pluralism, 143, 179
Policy, 8, 10, 29, 61-62, 66, 114,
119, 144-148, 150-152, 154,
156-157, 160-161, 164-165,
168, 170-174, 176-177,
179-181, 183, 187, 189, 192,
197
challenges, 167, 176,
education, 164, 179-180
methods, 164, 170, 172
principles, 150,
priorities, 132,
type of questions (what, who, etc.), 61, 66, 176-177, 189
Positive discrimination, 132,
136
Posthumanism, 38, 42-43,
46,
Postphenomenology, 43, 45-46

Power, 3, 7, 18, 87, 111, 136,
143-144, 171, 177, 180, 191,
196
Practical wisdom, 143, 201-202
Predictive policing, 6, 75, 94, 128
Product liability, 155
Privacy, 6, 75, 78, 97-99, 102,
151-154, 156-157, 159-160,
164-165, 171, 187
Prometheus myth, 18
Pygmalion, 18

Relational thinking, 59, 115-116,
201
Religion, 15, 22, 26, 28
Responsibility, 10, 19, 54, 61, 105,
108-118, 141, 147, 151, 153,
155, 160, 162, 167, 180
a-responsible machines, 111-112
attribution problem, 109,
113-114
control condition, 111
distribution problem, 112-114,
147
knowledge condition, 111,
115-118
legal, 112, 114, 155
many hands problem, 113, 147 many things problem, 113
Responsible research and innovation, 168
Rich and poor, gap between, 75, 137, 187, 197, 200
Romanticism, 38-39, 42
R.U.R. (play), 21

Safety, 104-105, 151, 153, 156,
159-160, 162
Searle, John, 35-36

Security, 3, 75, 85-86, 94, 105-106,
127, 153, 159
Self-driving cars, 3, 5, 48, 50, 52, 54,
61, 74-75, 106, 112-113, 164
Shelley, Mary, 18-19
Singularity, 12-13, 22, 25-26, 179
Social media, 3, 7, 74, 76, 78, 87, 89,
98-100, 103-104, 118
Society, 7-8, 40, 61, 106, 119, 123,
125, 128-129, 131, 134, 136,
140, 142-147, 151, 168, 171,
174, 176-178, 196
Spurious correlations, 90, 120, 128
Stakeholders, 76, 127, 144, 157,
168, 170
Statistics, 34, 68, 83, 87, 90, 94
Superintelligence, 11-17, 26, 31, 38,
61, 187, 191, 193, 195
Surveillance, 6, 74-75, 97, 100, 128 Sustainable Development Goals,
186, 189

Tay (chatbot), 7, 103
Tegmark, Max, 12-13, 160
Terminator, 21, 119, 179
Tesla, 16, 194
Theology, 22, 25
Totalitarianism, 100, 103, 144
Transcendence, 22, 24-25
Transdisciplinarity, 177-178
Trans-Earthism, 196
Transparency, 61, 98, 116, 118-121,
145, 151-152, 159-162, 165
Trust, 12, 99, 104, 118-119, 128,
130, 152, 160, 164
Trolley dilemmas, 52-53, 60
Turing, Alan, 65
Turing test, 65
2001: A Space Odyssey (film), 2, 21

United Nations (UN), 186
United States, 5-6, 128-129, 137,
172

Value-sensitive design, 162,
172-173
Virtue ethics, 56, 142
Vulnerability, 76, 137
existential, 108, 196
Vulnerable users, 102, 106

Waste, 102, 187, 192, 199
Watson, 93-94
Wisdom, 90, 201-202
practical, 143, 201-202
Wittgensteinian, 35
Work, the future of, 8, 136, 140









The MIT Press Essential Knowledge Series

AI Ethics, Mark Coeckelbergh
Auctions, Timothy P. Hubbard and Harry J. Paarsch
The Book, Amaranth Borsuk Carbon Capture, Howard J. Herzog Citizenship, Dimitry Kochenov
Cloud Computing, Nayan B. Ruparelia
Collaborative Society, Dariusz Jemielniak and Aleksandra Przegalinska
Computational Thinking, Peter J. Denning and Matti Tedre
Computing: A Concise History, Paul E. Ceruzzi The Conscious Mind, Zoltan E. Torey Crowdsourcing, Daren C. Brabham
Cynicism, Ansgar Allen
Data Science, John D. Kelleher and Brendan Tierney
Deep Learning, John D. Kelleher
Extremism, J. M. Berger Fake Photos, Hany Farid fMRI, Peter A. Bandettini Food, Fabio Parasecoli Free Will, Mark Balaguer The Future, Nick Montfort GPS, Paul E. Ceruzzi Haptics, Lynette A. Jones
Information and Society, Michael Buckland
Information and the Modern Corporation, James W. Cortada
Intellectual Property Strategy, John Palfrey The Internet of Things, Samuel Greengard Irony and Sarcasm, Roger Kreuz
Machine Learning: The New AI, Ethem Alpaydin
Machine Translation, Thierry Poibeau Memes in Digital Culture, Limor Shifman Metadata, Jeffrey Pomerantz
The Mind-Body Problem, Jonathan Westphal
MOOCs, Jonathan Haber Neuroplasticity, Moheb Costandi Nihilism, Nolen Gertz
Open Access, Peter Suber
Paradox, Margaret Cuonzo

Post-Truth, Lee McIntyre Recycling, Finn Arne Jørgensen Robots, John Jordan
School Choice, David R. Garcia
Self-Tracking, Gina Neff and Dawn Nafus
Sexual Consent, Milena Popova Smart Cities, Germaine R. Halegoua Spaceflight, Michael J. Neufeld
Spatial Computing, Shashi Shekhar and Pamela Vold
Sustainability, Kent E. Portney
Synesthesia, Richard E. Cytowic
The Technological Singularity, Murray Shanahan
3D Printing, John Jordan Understanding Beliefs, Nils J. Nilsson Virtual Reality, Samuel Greengard Waves, Frederic Raichlen

























Mark COECkElbErGh is a full Professor of Philosophy of Media and Tech- nology at the Philosophy of Department of the University of Vienna and the former President of the Society for Philosophy and Technology (SPT). His expertise focuses on ethics of new and emerging technologies, in particular robotics and artificial intelligence. He is currently a member of various entities that support policy building in the area of robotics and artificial intelligence, such as the European Commission's High Level Expert Group on Artificial Intelligence and the Austrian Council on Robotics and Artificial Intelligence. He is the author of twelve philosophy books and numerous articles, and is involved in several European research projects.


















































































































































































































































































































